{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b19a0988-ad85-46b9-8f8c-6c2bb6a263e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kaggel = {\"username\":\"vishal5050\",\"key\":\"44a082b603938e8f31058ab2854f49d5\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db4726-6402-4a5c-a9d6-4e3b3fafadc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094fe4f8-b0bb-4c3e-aa14-ebdb205c35d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CREATING 10K APPLE SUBSET\n",
      "======================================================================\n",
      "\n",
      "Source: /Users/vishal/Desktop/ML study/project/CNN type/first/fruit360_apples_only\n",
      "Target: /Users/vishal/Desktop/ML study/project/CNN type/first/fruit360_apples_10k_subset\n",
      "Goal: 8000 train + 2000 test = 10,000 total images\n",
      "======================================================================\n",
      "\n",
      "STEP 1: Analyzing Original Dataset\n",
      "----------------------------------------------------------------------\n",
      "Found 30 apple varieties:\n",
      "  Apple 12: 466 train, 154 test\n",
      "  Apple 5: 440 train, 146 test\n",
      "  Apple Granny Smith 1: 492 train, 164 test\n",
      "  Apple 13: 699 train, 235 test\n",
      "  Apple 14: 466 train, 154 test\n",
      "  Apple Braeburn 1: 492 train, 164 test\n",
      "  Apple Golden 2: 492 train, 164 test\n",
      "  Apple Red 1: 492 train, 164 test\n",
      "  Apple Golden 3: 481 train, 161 test\n",
      "  Apple Rotten 1: 488 train, 162 test\n",
      "  Apple Red Yellow 1: 492 train, 164 test\n",
      "  Apple Pink Lady 1: 456 train, 152 test\n",
      "  Apple Core 1: 235 train, 78 test\n",
      "  Apple 11: 430 train, 142 test\n",
      "  Apple 6: 473 train, 157 test\n",
      "  Apple 8: 687 train, 228 test\n",
      "  Apple 18: 484 train, 160 test\n",
      "  Apple 19: 729 train, 241 test\n",
      "  Apple hit 1: 702 train, 234 test\n",
      "  Apple 9: 694 train, 231 test\n",
      "  Apple 17: 610 train, 201 test\n",
      "  Apple 7: 694 train, 229 test\n",
      "  Apple 10: 699 train, 231 test\n",
      "  Apple Crimson Snow 1: 444 train, 148 test\n",
      "  Apple Golden 1: 480 train, 160 test\n",
      "  Apple Red 2: 492 train, 164 test\n",
      "  Apple Red Yellow 2: 672 train, 219 test\n",
      "  Apple worm 1: 696 train, 231 test\n",
      "  Apple Red 3: 429 train, 144 test\n",
      "  Apple Red Delicious 1: 490 train, 166 test\n",
      "\n",
      "Original totals: 16096 train, 5348 test = 21444 total\n",
      "\n",
      "STEP 2: Calculating Balanced Sample Size per Variety\n",
      "----------------------------------------------------------------------\n",
      "  Apple 12: 231 train, 57 test\n",
      "  Apple 5: 218 train, 54 test\n",
      "  Apple Granny Smith 1: 244 train, 61 test\n",
      "  Apple 13: 348 train, 87 test\n",
      "  Apple 14: 231 train, 57 test\n",
      "  Apple Braeburn 1: 244 train, 61 test\n",
      "  Apple Golden 2: 244 train, 61 test\n",
      "  Apple Red 1: 244 train, 61 test\n",
      "  Apple Golden 3: 239 train, 59 test\n",
      "  Apple Rotten 1: 242 train, 60 test\n",
      "  Apple Red Yellow 1: 244 train, 61 test\n",
      "  Apple Pink Lady 1: 226 train, 56 test\n",
      "  Apple Core 1: 116 train, 29 test\n",
      "  Apple 11: 213 train, 53 test\n",
      "  Apple 6: 235 train, 58 test\n",
      "  Apple 8: 341 train, 85 test\n",
      "  Apple 18: 240 train, 60 test\n",
      "  Apple 19: 361 train, 90 test\n",
      "  Apple hit 1: 349 train, 87 test\n",
      "  Apple 9: 345 train, 86 test\n",
      "  Apple 17: 302 train, 75 test\n",
      "  Apple 7: 344 train, 86 test\n",
      "  Apple 10: 346 train, 86 test\n",
      "  Apple Crimson Snow 1: 220 train, 55 test\n",
      "  Apple Golden 1: 238 train, 59 test\n",
      "  Apple Red 2: 244 train, 61 test\n",
      "  Apple Red Yellow 2: 332 train, 83 test\n",
      "  Apple worm 1: 345 train, 86 test\n",
      "  Apple Red 3: 213 train, 53 test\n",
      "  Apple Red Delicious 1: 244 train, 61 test\n",
      "\n",
      "Actual subset size: 7983 train + 1988 test = 9971 total\n",
      "\n",
      "STEP 3: Creating 10K Subset\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Copying sampled images...\n",
      "  âœ“ Apple 12: 231 train, 57 test copied\n",
      "  âœ“ Apple 5: 218 train, 54 test copied\n",
      "  âœ“ Apple Granny Smith 1: 244 train, 61 test copied\n",
      "  âœ“ Apple 13: 348 train, 87 test copied\n",
      "  âœ“ Apple 14: 231 train, 57 test copied\n",
      "  âœ“ Apple Braeburn 1: 244 train, 61 test copied\n",
      "  âœ“ Apple Golden 2: 244 train, 61 test copied\n",
      "  âœ“ Apple Red 1: 244 train, 61 test copied\n",
      "  âœ“ Apple Golden 3: 239 train, 59 test copied\n",
      "  âœ“ Apple Rotten 1: 242 train, 60 test copied\n",
      "  âœ“ Apple Red Yellow 1: 244 train, 61 test copied\n",
      "  âœ“ Apple Pink Lady 1: 226 train, 56 test copied\n",
      "  âœ“ Apple Core 1: 116 train, 29 test copied\n",
      "  âœ“ Apple 11: 213 train, 53 test copied\n",
      "  âœ“ Apple 6: 235 train, 58 test copied\n",
      "  âœ“ Apple 8: 341 train, 85 test copied\n",
      "  âœ“ Apple 18: 240 train, 60 test copied\n",
      "  âœ“ Apple 19: 361 train, 90 test copied\n",
      "  âœ“ Apple hit 1: 349 train, 87 test copied\n",
      "  âœ“ Apple 9: 345 train, 86 test copied\n",
      "  âœ“ Apple 17: 302 train, 75 test copied\n",
      "  âœ“ Apple 7: 344 train, 86 test copied\n",
      "  âœ“ Apple 10: 346 train, 86 test copied\n",
      "  âœ“ Apple Crimson Snow 1: 220 train, 55 test copied\n",
      "  âœ“ Apple Golden 1: 238 train, 59 test copied\n",
      "  âœ“ Apple Red 2: 244 train, 61 test copied\n",
      "  âœ“ Apple Red Yellow 2: 332 train, 83 test copied\n",
      "  âœ“ Apple worm 1: 345 train, 86 test copied\n",
      "  âœ“ Apple Red 3: 213 train, 53 test copied\n",
      "  âœ“ Apple Red Delicious 1: 244 train, 61 test copied\n",
      "\n",
      "======================================================================\n",
      "VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "Final subset:\n",
      "  Training:   7983 images\n",
      "  Test:       1988 images\n",
      "  Total:      9971 images\n",
      "\n",
      "Dataset reduced by 53.5%\n",
      "\n",
      "======================================================================\n",
      "SUBSET CREATION COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "âœ“ 10K subset saved to:\n",
      "  ğŸ“ /Users/vishal/Desktop/ML study/project/CNN type/first/fruit360_apples_10k_subset\n",
      "\n",
      "âœ“ Structure:\n",
      "  Training/  (7983 images)\n",
      "  Test/      (1988 images)\n",
      "\n",
      "âœ“ All 30 apple varieties included\n",
      "âœ“ Balanced proportional sampling maintained\n",
      "\n",
      "======================================================================\n",
      "READY FOR PHASE 1 TRAINING!\n",
      "======================================================================\n",
      "\n",
      "Expected training time reduction:\n",
      "  Original (21K images):  33-52 hours\n",
      "  Subset (10K images):    15-25 hours\n",
      "  Time saved:             ~18-27 hours\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create a balanced 10K subset from Fruit-360 apples (21,444 images).\n",
    "This maintains variety distribution while reducing training time.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "PROJECT_ROOT = '/Users/vishal/Desktop/ML study/project/CNN type/first'\n",
    "FULL_APPLES = os.path.join(PROJECT_ROOT, 'fruit360_apples_only')\n",
    "SUBSET_10K = os.path.join(PROJECT_ROOT, 'fruit360_apples_10k_subset')\n",
    "\n",
    "TARGET_TRAIN_SIZE = 8000  # 8000 training images\n",
    "TARGET_TEST_SIZE = 2000   # 2000 test images\n",
    "\n",
    "random.seed(42)  # For reproducibility\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREATING 10K APPLE SUBSET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nSource: {FULL_APPLES}\")\n",
    "print(f\"Target: {SUBSET_10K}\")\n",
    "print(f\"Goal: 8000 train + 2000 test = 10,000 total images\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# STEP 1: GET ALL VARIETIES\n",
    "# ============================================\n",
    "print(\"\\nSTEP 1: Analyzing Original Dataset\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "train_source = os.path.join(FULL_APPLES, 'Training')\n",
    "test_source = os.path.join(FULL_APPLES, 'Test')\n",
    "\n",
    "varieties = [d for d in os.listdir(train_source) \n",
    "             if os.path.isdir(os.path.join(train_source, d))]\n",
    "\n",
    "print(f\"Found {len(varieties)} apple varieties:\")\n",
    "\n",
    "# Count images per variety\n",
    "variety_counts = {}\n",
    "for variety in varieties:\n",
    "    train_path = os.path.join(train_source, variety)\n",
    "    test_path = os.path.join(test_source, variety)\n",
    "    \n",
    "    train_imgs = [f for f in os.listdir(train_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    test_imgs = [f for f in os.listdir(test_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    variety_counts[variety] = {\n",
    "        'train': len(train_imgs),\n",
    "        'test': len(test_imgs),\n",
    "        'total': len(train_imgs) + len(test_imgs)\n",
    "    }\n",
    "    \n",
    "    print(f\"  {variety}: {len(train_imgs)} train, {len(test_imgs)} test\")\n",
    "\n",
    "total_train = sum(v['train'] for v in variety_counts.values())\n",
    "total_test = sum(v['test'] for v in variety_counts.values())\n",
    "\n",
    "print(f\"\\nOriginal totals: {total_train} train, {total_test} test = {total_train + total_test} total\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 2: CALCULATE SAMPLING RATIO\n",
    "# ============================================\n",
    "print(\"\\nSTEP 2: Calculating Balanced Sample Size per Variety\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Calculate how many images per variety (proportional sampling)\n",
    "train_samples_per_variety = {}\n",
    "test_samples_per_variety = {}\n",
    "\n",
    "for variety in varieties:\n",
    "    # Proportional to original distribution\n",
    "    proportion = variety_counts[variety]['total'] / (total_train + total_test)\n",
    "    \n",
    "    train_target = int(TARGET_TRAIN_SIZE * proportion)\n",
    "    test_target = int(TARGET_TEST_SIZE * proportion)\n",
    "    \n",
    "    # Ensure we don't sample more than available\n",
    "    train_available = variety_counts[variety]['train']\n",
    "    test_available = variety_counts[variety]['test']\n",
    "    \n",
    "    train_samples_per_variety[variety] = min(train_target, train_available)\n",
    "    test_samples_per_variety[variety] = min(test_target, test_available)\n",
    "    \n",
    "    print(f\"  {variety}: {train_samples_per_variety[variety]} train, {test_samples_per_variety[variety]} test\")\n",
    "\n",
    "actual_train_total = sum(train_samples_per_variety.values())\n",
    "actual_test_total = sum(test_samples_per_variety.values())\n",
    "\n",
    "print(f\"\\nActual subset size: {actual_train_total} train + {actual_test_total} test = {actual_train_total + actual_test_total} total\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 3: CREATE SUBSET\n",
    "# ============================================\n",
    "print(\"\\nSTEP 3: Creating 10K Subset\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Remove old subset if exists\n",
    "if os.path.exists(SUBSET_10K):\n",
    "    print(\"Removing old subset...\")\n",
    "    shutil.rmtree(SUBSET_10K)\n",
    "\n",
    "# Create directories\n",
    "train_output = os.path.join(SUBSET_10K, 'Training')\n",
    "test_output = os.path.join(SUBSET_10K, 'Test')\n",
    "\n",
    "os.makedirs(train_output, exist_ok=True)\n",
    "os.makedirs(test_output, exist_ok=True)\n",
    "\n",
    "# Sample and copy images\n",
    "print(\"\\nCopying sampled images...\")\n",
    "\n",
    "for variety in varieties:\n",
    "    # Training images\n",
    "    train_src = os.path.join(train_source, variety)\n",
    "    train_dst = os.path.join(train_output, variety)\n",
    "    os.makedirs(train_dst, exist_ok=True)\n",
    "    \n",
    "    all_train_imgs = [f for f in os.listdir(train_src) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    sampled_train = random.sample(all_train_imgs, train_samples_per_variety[variety])\n",
    "    \n",
    "    for img in sampled_train:\n",
    "        shutil.copy2(os.path.join(train_src, img), os.path.join(train_dst, img))\n",
    "    \n",
    "    # Test images\n",
    "    test_src = os.path.join(test_source, variety)\n",
    "    test_dst = os.path.join(test_output, variety)\n",
    "    os.makedirs(test_dst, exist_ok=True)\n",
    "    \n",
    "    all_test_imgs = [f for f in os.listdir(test_src) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    sampled_test = random.sample(all_test_imgs, test_samples_per_variety[variety])\n",
    "    \n",
    "    for img in sampled_test:\n",
    "        shutil.copy2(os.path.join(test_src, img), os.path.join(test_dst, img))\n",
    "    \n",
    "    print(f\"  âœ“ {variety}: {len(sampled_train)} train, {len(sampled_test)} test copied\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 4: VERIFY SUBSET\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Count final images\n",
    "final_train = 0\n",
    "final_test = 0\n",
    "\n",
    "for variety in varieties:\n",
    "    train_path = os.path.join(train_output, variety)\n",
    "    test_path = os.path.join(test_output, variety)\n",
    "    \n",
    "    train_count = len([f for f in os.listdir(train_path) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    test_count = len([f for f in os.listdir(test_path) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    \n",
    "    final_train += train_count\n",
    "    final_test += test_count\n",
    "\n",
    "print(f\"\\nFinal subset:\")\n",
    "print(f\"  Training:   {final_train} images\")\n",
    "print(f\"  Test:       {final_test} images\")\n",
    "print(f\"  Total:      {final_train + final_test} images\")\n",
    "\n",
    "reduction_percent = (1 - (final_train + final_test) / (total_train + total_test)) * 100\n",
    "print(f\"\\nDataset reduced by {reduction_percent:.1f}%\")\n",
    "\n",
    "# ============================================\n",
    "# STEP 5: SUMMARY\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUBSET CREATION COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nâœ“ 10K subset saved to:\")\n",
    "print(f\"  ğŸ“ {SUBSET_10K}\")\n",
    "\n",
    "print(f\"\\nâœ“ Structure:\")\n",
    "print(f\"  Training/  ({final_train} images)\")\n",
    "print(f\"  Test/      ({final_test} images)\")\n",
    "\n",
    "print(f\"\\nâœ“ All {len(varieties)} apple varieties included\")\n",
    "print(f\"âœ“ Balanced proportional sampling maintained\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"READY FOR PHASE 1 TRAINING!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nExpected training time reduction:\")\n",
    "print(f\"  Original (21K images):  33-52 hours\")\n",
    "print(f\"  Subset (10K images):    15-25 hours\")\n",
    "print(f\"  Time saved:             ~18-27 hours\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709018a6-b0ef-4677-96c9-d7903eec716f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95914e2e-cb01-4cea-ba71-15401db94cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42023069-25af-4d3a-9bcb-77c4d350bbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c516c6-1603-4ded-a6dc-1e674dd2fc2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                    PYRAMIDNET-18 ARCHITECTURE\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ Building PyramidNet-18 (alpha=48)...\n",
      "\n",
      "Channel progression through stages:\n",
      "----------------------------------------------------------------------\n",
      "Stage 1 (Conv2_x):\n",
      "  Block 1: 16 â†’ 22 filters\n",
      "  Block 2: 22 â†’ 28 filters\n",
      "\n",
      "Stage 2 (Conv3_x):\n",
      "  Block 1: 28 â†’ 34 filters (stride=2)\n",
      "  Block 2: 34 â†’ 40 filters (stride=1)\n",
      "\n",
      "Stage 3 (Conv4_x):\n",
      "  Block 1: 40 â†’ 46 filters (stride=2)\n",
      "  Block 2: 46 â†’ 52 filters (stride=1)\n",
      "\n",
      "Stage 4 (Conv5_x):\n",
      "  Block 1: 52 â†’ 58 filters (stride=2)\n",
      "  Block 2: 58 â†’ 64 filters (stride=1)\n",
      "\n",
      "======================================================================\n",
      "MODEL SUMMARY\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"PyramidNet18\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"PyramidNet18\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,352</span> â”‚ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> â”‚ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>,  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ re_lu[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> â”‚ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,168</span> â”‚ re_lu_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">88</span> â”‚ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,356</span> â”‚ re_lu_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">352</span> â”‚ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)               â”‚            â”‚ conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">88</span> â”‚ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,544</span> â”‚ re_lu_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span> â”‚ conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,056</span> â”‚ re_lu_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">616</span> â”‚ add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)               â”‚            â”‚ conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span> â”‚ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,568</span> â”‚ re_lu_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> â”‚ conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">10,404</span> â”‚ re_lu_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">952</span> â”‚ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ conv2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)               â”‚            â”‚ conv2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> â”‚ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,240</span> â”‚ re_lu_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> â”‚ conv2d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">14,400</span> â”‚ re_lu_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,360</span> â”‚ add_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ conv2d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)               â”‚            â”‚ conv2d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span> â”‚ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,560</span> â”‚ re_lu_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">184</span> â”‚ conv2d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,044</span> â”‚ re_lu_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,840</span> â”‚ add_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ conv2d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>)               â”‚            â”‚ conv2d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">184</span> â”‚ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">21,528</span> â”‚ re_lu_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> â”‚ conv2d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,336</span> â”‚ re_lu_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,392</span> â”‚ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ conv2d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)               â”‚            â”‚ conv2d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span> â”‚ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,    â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)  â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">27,144</span> â”‚ re_lu_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)  â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span> â”‚ conv2d_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)  â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">30,276</span> â”‚ re_lu_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)  â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,016</span> â”‚ add_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ conv2d_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ conv2d_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)  â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">232</span> â”‚ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,408</span> â”‚ re_lu_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚ conv2d_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> â”‚ re_lu_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,712</span> â”‚ add_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ conv2d_23[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ conv2d_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ re_lu_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePoolâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ global_average_pâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> â”‚ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ \u001b[38;5;34m3\u001b[0m)                â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  â”‚      \u001b[38;5;34m2,352\u001b[0m â”‚ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  â”‚         \u001b[38;5;34m64\u001b[0m â”‚ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu (\u001b[38;5;33mReLU\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m112\u001b[0m,  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ re_lu[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”‚ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚         \u001b[38;5;34m64\u001b[0m â”‚ max_pooling2d[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_1 (\u001b[38;5;33mReLU\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m16\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚      \u001b[38;5;34m3,168\u001b[0m â”‚ re_lu_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m22\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚         \u001b[38;5;34m88\u001b[0m â”‚ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m22\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_2 (\u001b[38;5;33mReLU\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m22\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚      \u001b[38;5;34m4,356\u001b[0m â”‚ re_lu_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m22\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚        \u001b[38;5;34m352\u001b[0m â”‚ max_pooling2d[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m22\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add (\u001b[38;5;33mAdd\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m22\u001b[0m)               â”‚            â”‚ conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚         \u001b[38;5;34m88\u001b[0m â”‚ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m22\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_3 (\u001b[38;5;33mReLU\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m22\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚      \u001b[38;5;34m5,544\u001b[0m â”‚ re_lu_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m28\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚        \u001b[38;5;34m112\u001b[0m â”‚ conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m28\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_4 (\u001b[38;5;33mReLU\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m28\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚      \u001b[38;5;34m7,056\u001b[0m â”‚ re_lu_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m28\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚        \u001b[38;5;34m616\u001b[0m â”‚ add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m28\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_1 (\u001b[38;5;33mAdd\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ conv2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m28\u001b[0m)               â”‚            â”‚ conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚        \u001b[38;5;34m112\u001b[0m â”‚ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m28\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_5 (\u001b[38;5;33mReLU\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m56\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m28\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    â”‚      \u001b[38;5;34m8,568\u001b[0m â”‚ re_lu_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m34\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    â”‚        \u001b[38;5;34m136\u001b[0m â”‚ conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m34\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_6 (\u001b[38;5;33mReLU\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m34\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    â”‚     \u001b[38;5;34m10,404\u001b[0m â”‚ re_lu_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m34\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    â”‚        \u001b[38;5;34m952\u001b[0m â”‚ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m34\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_2 (\u001b[38;5;33mAdd\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ conv2d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m34\u001b[0m)               â”‚            â”‚ conv2d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    â”‚        \u001b[38;5;34m136\u001b[0m â”‚ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m34\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_7 (\u001b[38;5;33mReLU\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m34\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    â”‚     \u001b[38;5;34m12,240\u001b[0m â”‚ re_lu_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m40\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    â”‚        \u001b[38;5;34m160\u001b[0m â”‚ conv2d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m40\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_8 (\u001b[38;5;33mReLU\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m40\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    â”‚     \u001b[38;5;34m14,400\u001b[0m â”‚ re_lu_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m40\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    â”‚      \u001b[38;5;34m1,360\u001b[0m â”‚ add_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m40\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_3 (\u001b[38;5;33mAdd\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ conv2d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m40\u001b[0m)               â”‚            â”‚ conv2d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    â”‚        \u001b[38;5;34m160\u001b[0m â”‚ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m40\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_9 (\u001b[38;5;33mReLU\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m40\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    â”‚     \u001b[38;5;34m16,560\u001b[0m â”‚ re_lu_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m46\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    â”‚        \u001b[38;5;34m184\u001b[0m â”‚ conv2d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m46\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_10 (\u001b[38;5;33mReLU\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m46\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    â”‚     \u001b[38;5;34m19,044\u001b[0m â”‚ re_lu_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m46\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    â”‚      \u001b[38;5;34m1,840\u001b[0m â”‚ add_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m46\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_4 (\u001b[38;5;33mAdd\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ conv2d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m46\u001b[0m)               â”‚            â”‚ conv2d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    â”‚        \u001b[38;5;34m184\u001b[0m â”‚ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m46\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_11 (\u001b[38;5;33mReLU\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m46\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    â”‚     \u001b[38;5;34m21,528\u001b[0m â”‚ re_lu_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m52\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    â”‚        \u001b[38;5;34m208\u001b[0m â”‚ conv2d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m52\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_12 (\u001b[38;5;33mReLU\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m52\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    â”‚     \u001b[38;5;34m24,336\u001b[0m â”‚ re_lu_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m52\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    â”‚      \u001b[38;5;34m2,392\u001b[0m â”‚ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m52\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_5 (\u001b[38;5;33mAdd\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ conv2d_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m52\u001b[0m)               â”‚            â”‚ conv2d_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    â”‚        \u001b[38;5;34m208\u001b[0m â”‚ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚ \u001b[38;5;34m52\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_13 (\u001b[38;5;33mReLU\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m,    â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”‚                     â”‚ \u001b[38;5;34m52\u001b[0m)               â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m58\u001b[0m)  â”‚     \u001b[38;5;34m27,144\u001b[0m â”‚ re_lu_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m58\u001b[0m)  â”‚        \u001b[38;5;34m232\u001b[0m â”‚ conv2d_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_14 (\u001b[38;5;33mReLU\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m58\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_20 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m58\u001b[0m)  â”‚     \u001b[38;5;34m30,276\u001b[0m â”‚ re_lu_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_21 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m58\u001b[0m)  â”‚      \u001b[38;5;34m3,016\u001b[0m â”‚ add_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_6 (\u001b[38;5;33mAdd\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m58\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ conv2d_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ conv2d_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m58\u001b[0m)  â”‚        \u001b[38;5;34m232\u001b[0m â”‚ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_15 (\u001b[38;5;33mReLU\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m58\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_22 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚     \u001b[38;5;34m33,408\u001b[0m â”‚ re_lu_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚        \u001b[38;5;34m256\u001b[0m â”‚ conv2d_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_16 (\u001b[38;5;33mReLU\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_23 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚     \u001b[38;5;34m36,864\u001b[0m â”‚ re_lu_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_24 (\u001b[38;5;33mConv2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚      \u001b[38;5;34m3,712\u001b[0m â”‚ add_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ add_7 (\u001b[38;5;33mAdd\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ conv2d_23[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ conv2d_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚        \u001b[38;5;34m256\u001b[0m â”‚ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ re_lu_17 (\u001b[38;5;33mReLU\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m64\u001b[0m)  â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ re_lu_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePoolâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        â”‚          \u001b[38;5;34m0\u001b[0m â”‚ global_average_pâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        â”‚        \u001b[38;5;34m650\u001b[0m â”‚ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">295,018</span> (1.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m295,018\u001b[0m (1.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">293,578</span> (1.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m293,578\u001b[0m (1.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,440</span> (5.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,440\u001b[0m (5.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "KEY SPECIFICATIONS\n",
      "======================================================================\n",
      "Total parameters:     295,018\n",
      "Architecture:         PyramidNet-18\n",
      "Widening factor (Î±):  48\n",
      "Starting filters:     16\n",
      "Final filters:        ~64\n",
      "Depth:                18 layers\n",
      "Residual blocks:      8 (2 per stage)\n",
      "\n",
      "======================================================================\n",
      "ARCHITECTURE COMPARISON: ResNet vs PyramidNet\n",
      "======================================================================\n",
      "\n",
      "ResNet18 channel progression:\n",
      "  Conv2_x: 64  â†’ 64   (constant)\n",
      "  Conv3_x: 64  â†’ 128  (2x jump)\n",
      "  Conv4_x: 128 â†’ 256  (2x jump)\n",
      "  Conv5_x: 256 â†’ 512  (2x jump)\n",
      "\n",
      "PyramidNet18 channel progression (Î±=48):\n",
      "  Conv2_x: 16  â†’ 22 â†’ 28   (gradual +6)\n",
      "  Conv3_x: 28  â†’ 34 â†’ 40   (gradual +6)\n",
      "  Conv4_x: 40  â†’ 46 â†’ 52   (gradual +6)\n",
      "  Conv5_x: 52  â†’ 58 â†’ 64   (gradual +6)\n",
      "\n",
      "âœ¨ Key Difference:\n",
      "  ResNet:     Abrupt channel increases (step function)\n",
      "  PyramidNet: Smooth channel increases (pyramid/linear)\n",
      "\n",
      "======================================================================\n",
      "BENEFITS OF PYRAMIDNET\n",
      "======================================================================\n",
      "âœ“ Smoother gradient flow (better optimization)\n",
      "âœ“ More efficient parameter usage\n",
      "âœ“ Potentially higher accuracy with same parameters\n",
      "âœ“ Better feature representation\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Generating architecture comparison visualization...\n",
      "\n",
      "âœ“ Architecture comparison saved: pyramidnet_vs_resnet_architecture.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "âœ… PyramidNet-18 architecture is ready!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PyramidNet-18 Complete Implementation\n",
    "Based on: \"Deep Pyramidal Residual Networks\" (2017)\n",
    "https://arxiv.org/abs/1610.02915\n",
    "\n",
    "Architecture: Gradually increasing feature maps in a pyramid shape\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================\n",
    "# PYRAMIDNET-18 ARCHITECTURE\n",
    "# ============================================\n",
    "\n",
    "def pyramidnet_basicblock(x, in_filters, out_filters, stride=1, shortcut_type='projection'):\n",
    "    \"\"\"\n",
    "    PyramidNet Basic Block (similar to ResNet BasicBlock but with gradual channel increase)\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor\n",
    "        in_filters: Input channels\n",
    "        out_filters: Output channels\n",
    "        stride: Stride for downsampling\n",
    "        shortcut_type: 'identity' or 'projection'\n",
    "    \"\"\"\n",
    "    \n",
    "    shortcut = x\n",
    "    \n",
    "    # First conv\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D( out_filters , 3, strides = stride , padding = 'same', use_bias = False )(x)\n",
    "    \n",
    "    # Second conv\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(out_filters, 3, strides=1, padding='same', use_bias=False)(x)\n",
    "    \n",
    "    # Shortcut connection\n",
    "    if in_filters != out_filters or stride != 1:\n",
    "        # Need to adjust shortcut dimensions\n",
    "        if shortcut_type == 'projection':\n",
    "            shortcut = layers.Conv2D(out_filters, 1, strides=stride, use_bias=False)(shortcut)\n",
    "        else:\n",
    "            # Zero-padding shortcut (save parameters)\n",
    "            shortcut = layers.AveragePooling2D(stride, stride)(shortcut) if stride > 1 else shortcut\n",
    "            # Pad channels with zeros\n",
    "            pad_filters = out_filters - in_filters\n",
    "            if pad_filters > 0:\n",
    "                shortcut = layers.Lambda(\n",
    "                    lambda x: tf.pad(x, [[0, 0], [0, 0], [0, 0], [0, pad_filters]])\n",
    "                )(shortcut)\n",
    "    \n",
    "    # Add residual\n",
    "    x = layers.Add()([x, shortcut])\n",
    "    \n",
    "    return x\n",
    "\n",
    "def build_pyramidnet18(num_classes, alpha=48, input_shape=(224, 224, 3)):\n",
    "    \"\"\"\n",
    "    Build PyramidNet-18 architecture\n",
    "    \n",
    "    Args:\n",
    "        num_classes: Number of output classes\n",
    "        alpha: Widening factor (determines how much channels increase)\n",
    "        input_shape: Input image shape\n",
    "    \n",
    "    Architecture:\n",
    "        Layer       | Blocks | Output Size | Filters (start â†’ end)\n",
    "        ------------|--------|-------------|----------------------\n",
    "        Conv1       | 1      | 112Ã—112     | 16\n",
    "        Conv2_x     | 2      | 56Ã—56       | 16 â†’ 32\n",
    "        Conv3_x     | 2      | 28Ã—28       | 32 â†’ 64\n",
    "        Conv4_x     | 2      | 14Ã—14       | 64 â†’ 96\n",
    "        Conv5_x     | 2      | 7Ã—7         | 96 â†’ 128\n",
    "        GAP + FC    | 1      | 1Ã—1         | num_classes\n",
    "        \n",
    "    Total: ~11M parameters (similar to ResNet18)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate channel increases\n",
    "    # With 4 stages and 2 blocks per stage = 8 blocks total\n",
    "    num_blocks = 8\n",
    "    add_channels_per_block = alpha / num_blocks  # Gradual increase\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Initial Conv\n",
    "    start_filters = 16\n",
    "    x = layers.Conv2D(start_filters, 7, strides=2, padding='same', use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "    \n",
    "    current_filters = start_filters\n",
    "    \n",
    "    # Stage 1: Conv2_x (2 blocks, 56Ã—56)\n",
    "    print(\"Stage 1 (Conv2_x):\")\n",
    "    for i in range(2):\n",
    "        out_filters = int(start_filters + add_channels_per_block * (i + 1))\n",
    "        print(f\"  Block {i+1}: {int(current_filters)} â†’ {out_filters} filters\")\n",
    "        x = pyramidnet_basicblock(x, int(current_filters), out_filters, stride=1)\n",
    "        current_filters = out_filters\n",
    "    \n",
    "    # Stage 2: Conv3_x (2 blocks, 28Ã—28, downsample at first block)\n",
    "    print(\"\\nStage 2 (Conv3_x):\")\n",
    "    for i in range(2):\n",
    "        out_filters = int(start_filters + add_channels_per_block * (2 + i + 1))\n",
    "        stride = 2 if i == 0 else 1\n",
    "        print(f\"  Block {i+1}: {int(current_filters)} â†’ {out_filters} filters (stride={stride})\")\n",
    "        x = pyramidnet_basicblock(x, int(current_filters), out_filters, stride=stride)\n",
    "        current_filters = out_filters\n",
    "    \n",
    "    # Stage 3: Conv4_x (2 blocks, 14Ã—14, downsample at first block)\n",
    "    print(\"\\nStage 3 (Conv4_x):\")\n",
    "    for i in range(2):\n",
    "        out_filters = int(start_filters + add_channels_per_block * (4 + i + 1))\n",
    "        stride = 2 if i == 0 else 1\n",
    "        print(f\"  Block {i+1}: {int(current_filters)} â†’ {out_filters} filters (stride={stride})\")\n",
    "        x = pyramidnet_basicblock(x, int(current_filters), out_filters, stride=stride)\n",
    "        current_filters = out_filters\n",
    "    \n",
    "    # Stage 4: Conv5_x (2 blocks, 7Ã—7, downsample at first block)\n",
    "    print(\"\\nStage 4 (Conv5_x):\")\n",
    "    for i in range(2):\n",
    "        out_filters = int(start_filters + add_channels_per_block * (6 + i + 1))\n",
    "        stride = 2 if i == 0 else 1\n",
    "        print(f\"  Block {i+1}: {int(current_filters)} â†’ {out_filters} filters (stride={stride})\")\n",
    "        x = pyramidnet_basicblock(x, int(current_filters), out_filters, stride=stride)\n",
    "        current_filters = out_filters\n",
    "    \n",
    "    # Final layers\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='PyramidNet18')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZATION: Architecture Comparison\n",
    "# ============================================\n",
    "\n",
    "def visualize_architecture_comparison():\n",
    "    \"\"\"\n",
    "    Visualize channel progression: ResNet vs PyramidNet\n",
    "    \"\"\"\n",
    "    \n",
    "    # ResNet18 channel progression\n",
    "    resnet_stages = ['Conv1', 'Conv2_x', 'Conv3_x', 'Conv4_x', 'Conv5_x']\n",
    "    resnet_channels = [64, 64, 128, 256, 512]\n",
    "    \n",
    "    # PyramidNet18 channel progression (alpha=48)\n",
    "    pyramid_stages = ['Conv1', 'Conv2_x\\nBlock1', 'Conv2_x\\nBlock2', \n",
    "                      'Conv3_x\\nBlock1', 'Conv3_x\\nBlock2',\n",
    "                      'Conv4_x\\nBlock1', 'Conv4_x\\nBlock2',\n",
    "                      'Conv5_x\\nBlock1', 'Conv5_x\\nBlock2']\n",
    "    pyramid_channels = [16, 22, 28, 34, 40, 46, 52, 58, 64]\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # ResNet progression\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(resnet_stages, resnet_channels, 'o-', linewidth=3, markersize=10, color='#FF6B6B')\n",
    "    plt.title('ResNet18: Step Increases', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Stage')\n",
    "    plt.ylabel('Number of Channels')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(0, 550)\n",
    "    for i, (stage, ch) in enumerate(zip(resnet_stages, resnet_channels)):\n",
    "        plt.text(i, ch + 20, str(ch), ha='center', fontweight='bold')\n",
    "    \n",
    "    # PyramidNet progression\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(len(pyramid_channels)), pyramid_channels, 'o-', \n",
    "             linewidth=3, markersize=8, color='#4ECDC4')\n",
    "    plt.title('PyramidNet18: Gradual Increase', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Block Number')\n",
    "    plt.ylabel('Number of Channels')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(range(len(pyramid_channels)), range(1, len(pyramid_channels)+1))\n",
    "    for i, ch in enumerate(pyramid_channels):\n",
    "        plt.text(i, ch + 2, str(ch), ha='center', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pyramidnet_vs_resnet_architecture.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nâœ“ Architecture comparison saved: pyramidnet_vs_resnet_architecture.png\")\n",
    "    plt.show()\n",
    "\n",
    "# ============================================\n",
    "# BUILD AND DISPLAY MODEL\n",
    "# ============================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\" \"*20 + \"PYRAMIDNET-18 ARCHITECTURE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nğŸ“ Building PyramidNet-18 (alpha=48)...\")\n",
    "    print(\"\\nChannel progression through stages:\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Build model\n",
    "    model = build_pyramidnet18(num_classes=10, alpha=48)  # Example with 10 classes\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MODEL SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"KEY SPECIFICATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total parameters:     {model.count_params():,}\")\n",
    "    print(f\"Architecture:         PyramidNet-18\")\n",
    "    print(f\"Widening factor (Î±):  48\")\n",
    "    print(f\"Starting filters:     16\")\n",
    "    print(f\"Final filters:        ~64\")\n",
    "    print(f\"Depth:                18 layers\")\n",
    "    print(f\"Residual blocks:      8 (2 per stage)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ARCHITECTURE COMPARISON: ResNet vs PyramidNet\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(\"\\nResNet18 channel progression:\")\n",
    "    print(\"  Conv2_x: 64  â†’ 64   (constant)\")\n",
    "    print(\"  Conv3_x: 64  â†’ 128  (2x jump)\")\n",
    "    print(\"  Conv4_x: 128 â†’ 256  (2x jump)\")\n",
    "    print(\"  Conv5_x: 256 â†’ 512  (2x jump)\")\n",
    "    \n",
    "    print(\"\\nPyramidNet18 channel progression (Î±=48):\")\n",
    "    print(\"  Conv2_x: 16  â†’ 22 â†’ 28   (gradual +6)\")\n",
    "    print(\"  Conv3_x: 28  â†’ 34 â†’ 40   (gradual +6)\")\n",
    "    print(\"  Conv4_x: 40  â†’ 46 â†’ 52   (gradual +6)\")\n",
    "    print(\"  Conv5_x: 52  â†’ 58 â†’ 64   (gradual +6)\")\n",
    "    \n",
    "    print(\"\\nâœ¨ Key Difference:\")\n",
    "    print(\"  ResNet:     Abrupt channel increases (step function)\")\n",
    "    print(\"  PyramidNet: Smooth channel increases (pyramid/linear)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BENEFITS OF PYRAMIDNET\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"âœ“ Smoother gradient flow (better optimization)\")\n",
    "    print(\"âœ“ More efficient parameter usage\")\n",
    "    print(\"âœ“ Potentially higher accuracy with same parameters\")\n",
    "    print(\"âœ“ Better feature representation\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    # Visualize\n",
    "    print(\"\\nGenerating architecture comparison visualization...\")\n",
    "    visualize_architecture_comparison()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âœ… PyramidNet-18 architecture is ready!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38802dd3-1008-4a0b-89f3-942076673682",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9847078-b57b-4855-b7dd-48156768d2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "            PYRAMIDNET-18 COMPLETE TRAINING PIPELINE\n",
      "======================================================================\n",
      "\n",
      "Phase 1: Pre-train on Fruit-360 (10K images)\n",
      "Phase 2: Fine-tune on your curated data (800 images)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PHASE 1: PRE-TRAINING ON FRUIT-360\n",
      "======================================================================\n",
      "\n",
      "Loading Fruit-360 data (10K subset)...\n",
      "Found 6400 images belonging to 30 classes.\n",
      "Found 1583 images belonging to 30 classes.\n",
      "Found 1988 images belonging to 30 classes.\n",
      "\n",
      "âœ“ Data loaded:\n",
      "  Training:   6400 images\n",
      "  Validation: 1583 images\n",
      "  Test:       1988 images\n",
      "  Classes:    30\n",
      "\n",
      "Building PyramidNet-18 from scratch...\n",
      "âœ“ Model built: 296,318 parameters\n",
      "\n",
      "Starting pre-training at 07:08:03\n",
      "Expected time: 6-10 hours\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.1837 - loss: 2.9035\n",
      "Epoch 1: val_accuracy improved from -inf to 0.02653, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_pretrained.keras\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 261ms/step - accuracy: 0.1842 - loss: 2.9012 - val_accuracy: 0.0265 - val_loss: 9.2804 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.5080 - loss: 1.6702\n",
      "Epoch 2: val_accuracy improved from 0.02653 to 0.27543, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_pretrained.keras\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 272ms/step - accuracy: 0.5082 - loss: 1.6695 - val_accuracy: 0.2754 - val_loss: 3.0115 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.6445 - loss: 1.1985\n",
      "Epoch 3: val_accuracy improved from 0.27543 to 0.48958, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_pretrained.keras\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 264ms/step - accuracy: 0.6445 - loss: 1.1982 - val_accuracy: 0.4896 - val_loss: 1.6373 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.7195 - loss: 0.9346\n",
      "Epoch 4: val_accuracy improved from 0.48958 to 0.70689, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_pretrained.keras\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 264ms/step - accuracy: 0.7196 - loss: 0.9343 - val_accuracy: 0.7069 - val_loss: 0.8929 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.7863 - loss: 0.7247\n",
      "Epoch 5: val_accuracy did not improve from 0.70689\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 254ms/step - accuracy: 0.7863 - loss: 0.7246 - val_accuracy: 0.5584 - val_loss: 1.4436 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.8072 - loss: 0.6333\n",
      "Epoch 6: val_accuracy improved from 0.70689 to 0.79912, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_pretrained.keras\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 269ms/step - accuracy: 0.8073 - loss: 0.6331 - val_accuracy: 0.7991 - val_loss: 0.6021 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.8631 - loss: 0.4846\n",
      "Epoch 7: val_accuracy did not improve from 0.79912\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 280ms/step - accuracy: 0.8631 - loss: 0.4845 - val_accuracy: 0.5919 - val_loss: 1.7903 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.8828 - loss: 0.3923\n",
      "Epoch 8: val_accuracy did not improve from 0.79912\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 262ms/step - accuracy: 0.8828 - loss: 0.3924 - val_accuracy: 0.5357 - val_loss: 1.9065 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.9100 - loss: 0.3309\n",
      "Epoch 9: val_accuracy did not improve from 0.79912\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 258ms/step - accuracy: 0.9099 - loss: 0.3312 - val_accuracy: 0.6702 - val_loss: 1.1913 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.8986 - loss: 0.3472\n",
      "Epoch 10: val_accuracy improved from 0.79912 to 0.82186, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_pretrained.keras\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 256ms/step - accuracy: 0.8987 - loss: 0.3471 - val_accuracy: 0.8219 - val_loss: 0.4835 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.9033 - loss: 0.3309\n",
      "Epoch 11: val_accuracy did not improve from 0.82186\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 266ms/step - accuracy: 0.9033 - loss: 0.3309 - val_accuracy: 0.7303 - val_loss: 1.0248 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.9025 - loss: 0.3317\n",
      "Epoch 12: val_accuracy did not improve from 0.82186\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 263ms/step - accuracy: 0.9025 - loss: 0.3317 - val_accuracy: 0.5565 - val_loss: 2.0803 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.9343 - loss: 0.2320\n",
      "Epoch 13: val_accuracy improved from 0.82186 to 0.86545, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_pretrained.keras\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 260ms/step - accuracy: 0.9343 - loss: 0.2320 - val_accuracy: 0.8654 - val_loss: 0.3898 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.9117 - loss: 0.3005\n",
      "Epoch 14: val_accuracy did not improve from 0.86545\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 261ms/step - accuracy: 0.9117 - loss: 0.3004 - val_accuracy: 0.6140 - val_loss: 1.8255 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.9275 - loss: 0.2542\n",
      "Epoch 15: val_accuracy did not improve from 0.86545\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 265ms/step - accuracy: 0.9275 - loss: 0.2542 - val_accuracy: 0.8560 - val_loss: 0.4260 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.9391 - loss: 0.2066\n",
      "Epoch 16: val_accuracy did not improve from 0.86545\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 257ms/step - accuracy: 0.9391 - loss: 0.2066 - val_accuracy: 0.8023 - val_loss: 0.7068 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.9244 - loss: 0.2531\n",
      "Epoch 17: val_accuracy improved from 0.86545 to 0.89071, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_pretrained.keras\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 261ms/step - accuracy: 0.9244 - loss: 0.2531 - val_accuracy: 0.8907 - val_loss: 0.4307 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.9327 - loss: 0.2214\n",
      "Epoch 18: val_accuracy did not improve from 0.89071\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 268ms/step - accuracy: 0.9328 - loss: 0.2214 - val_accuracy: 0.8238 - val_loss: 0.6387 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.9457 - loss: 0.1805\n",
      "Epoch 19: val_accuracy did not improve from 0.89071\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 259ms/step - accuracy: 0.9457 - loss: 0.1805 - val_accuracy: 0.8610 - val_loss: 0.4365 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.9469 - loss: 0.1902\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.89071\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 268ms/step - accuracy: 0.9469 - loss: 0.1901 - val_accuracy: 0.8560 - val_loss: 0.4073 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.9611 - loss: 0.1464\n",
      "Epoch 21: val_accuracy improved from 0.89071 to 0.92862, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_pretrained.keras\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 261ms/step - accuracy: 0.9611 - loss: 0.1463 - val_accuracy: 0.9286 - val_loss: 0.2140 - learning_rate: 5.0000e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.9758 - loss: 0.0937\n",
      "Epoch 22: val_accuracy did not improve from 0.92862\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 263ms/step - accuracy: 0.9758 - loss: 0.0937 - val_accuracy: 0.9267 - val_loss: 0.2493 - learning_rate: 5.0000e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.9768 - loss: 0.0864\n",
      "Epoch 23: val_accuracy improved from 0.92862 to 0.94441, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_pretrained.keras\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 261ms/step - accuracy: 0.9768 - loss: 0.0865 - val_accuracy: 0.9444 - val_loss: 0.1686 - learning_rate: 5.0000e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.9735 - loss: 0.0973\n",
      "Epoch 24: val_accuracy did not improve from 0.94441\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 266ms/step - accuracy: 0.9735 - loss: 0.0973 - val_accuracy: 0.8522 - val_loss: 0.4593 - learning_rate: 5.0000e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.9741 - loss: 0.0952\n",
      "Epoch 25: val_accuracy did not improve from 0.94441\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 263ms/step - accuracy: 0.9741 - loss: 0.0952 - val_accuracy: 0.8604 - val_loss: 0.5408 - learning_rate: 5.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.9737 - loss: 0.0951\n",
      "Epoch 26: val_accuracy did not improve from 0.94441\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 259ms/step - accuracy: 0.9737 - loss: 0.0951 - val_accuracy: 0.9204 - val_loss: 0.2122 - learning_rate: 5.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.9695 - loss: 0.0995\n",
      "Epoch 27: val_accuracy did not improve from 0.94441\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 263ms/step - accuracy: 0.9695 - loss: 0.0995 - val_accuracy: 0.9413 - val_loss: 0.1957 - learning_rate: 5.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.9733 - loss: 0.0951\n",
      "Epoch 28: val_accuracy did not improve from 0.94441\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 262ms/step - accuracy: 0.9733 - loss: 0.0951 - val_accuracy: 0.9431 - val_loss: 0.1475 - learning_rate: 5.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.9829 - loss: 0.0748\n",
      "Epoch 29: val_accuracy did not improve from 0.94441\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 267ms/step - accuracy: 0.9829 - loss: 0.0748 - val_accuracy: 0.8648 - val_loss: 0.5777 - learning_rate: 5.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.9791 - loss: 0.0766\n",
      "Epoch 30: val_accuracy did not improve from 0.94441\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 260ms/step - accuracy: 0.9791 - loss: 0.0766 - val_accuracy: 0.9280 - val_loss: 0.2425 - learning_rate: 5.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.9696 - loss: 0.1055\n",
      "Epoch 31: val_accuracy did not improve from 0.94441\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 270ms/step - accuracy: 0.9696 - loss: 0.1055 - val_accuracy: 0.9097 - val_loss: 0.3544 - learning_rate: 5.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.9777 - loss: 0.0838\n",
      "Epoch 32: val_accuracy did not improve from 0.94441\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 270ms/step - accuracy: 0.9777 - loss: 0.0838 - val_accuracy: 0.9122 - val_loss: 0.3705 - learning_rate: 5.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.9827 - loss: 0.0684\n",
      "Epoch 33: val_accuracy did not improve from 0.94441\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 268ms/step - accuracy: 0.9827 - loss: 0.0685 - val_accuracy: 0.9160 - val_loss: 0.2350 - learning_rate: 5.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.9749 - loss: 0.0866\n",
      "Epoch 34: val_accuracy did not improve from 0.94441\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 268ms/step - accuracy: 0.9749 - loss: 0.0866 - val_accuracy: 0.9084 - val_loss: 0.4000 - learning_rate: 5.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.9747 - loss: 0.0876\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.94441\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 264ms/step - accuracy: 0.9747 - loss: 0.0875 - val_accuracy: 0.9368 - val_loss: 0.1816 - learning_rate: 5.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.9889 - loss: 0.0471\n",
      "Epoch 36: val_accuracy improved from 0.94441 to 0.95325, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_pretrained.keras\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 274ms/step - accuracy: 0.9889 - loss: 0.0471 - val_accuracy: 0.9533 - val_loss: 0.1586 - learning_rate: 2.5000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - accuracy: 0.9863 - loss: 0.0486\n",
      "Epoch 37: val_accuracy did not improve from 0.95325\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 273ms/step - accuracy: 0.9863 - loss: 0.0486 - val_accuracy: 0.9533 - val_loss: 0.1477 - learning_rate: 2.5000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.9911 - loss: 0.0396\n",
      "Epoch 38: val_accuracy did not improve from 0.95325\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 264ms/step - accuracy: 0.9911 - loss: 0.0396 - val_accuracy: 0.9078 - val_loss: 0.3330 - learning_rate: 2.5000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.9898 - loss: 0.0463\n",
      "Epoch 39: val_accuracy did not improve from 0.95325\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 267ms/step - accuracy: 0.9898 - loss: 0.0463 - val_accuracy: 0.9362 - val_loss: 0.1903 - learning_rate: 2.5000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.9895 - loss: 0.0447\n",
      "Epoch 40: val_accuracy improved from 0.95325 to 0.95704, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_pretrained.keras\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 268ms/step - accuracy: 0.9895 - loss: 0.0447 - val_accuracy: 0.9570 - val_loss: 0.1527 - learning_rate: 2.5000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.9918 - loss: 0.0383\n",
      "Epoch 41: val_accuracy did not improve from 0.95704\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 259ms/step - accuracy: 0.9918 - loss: 0.0383 - val_accuracy: 0.9438 - val_loss: 0.1852 - learning_rate: 2.5000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.9882 - loss: 0.0451\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.95704\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 261ms/step - accuracy: 0.9882 - loss: 0.0451 - val_accuracy: 0.9330 - val_loss: 0.3051 - learning_rate: 2.5000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.9935 - loss: 0.0357\n",
      "Epoch 43: val_accuracy did not improve from 0.95704\n",
      "\u001b[1m200/200\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 262ms/step - accuracy: 0.9935 - loss: 0.0357 - val_accuracy: 0.9551 - val_loss: 0.1535 - learning_rate: 1.2500e-04\n",
      "Epoch 43: early stopping\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "\n",
      "âœ“ Pre-training complete in 0.63 hours\n",
      "\n",
      "âœ“ Pre-training results:\n",
      "  Test Accuracy: 97.74%\n",
      "  Test Loss: 0.0687\n",
      "\n",
      "======================================================================\n",
      "PHASE 2: FINE-TUNING ON YOUR CURATED DATA\n",
      "======================================================================\n",
      "\n",
      "Loading your curated data...\n",
      "Found 519 images belonging to 8 classes.\n",
      "Found 168 images belonging to 8 classes.\n",
      "\n",
      "âœ“ Data loaded:\n",
      "  Training:   519 images\n",
      "  Validation: 168 images\n",
      "  Classes:    8\n",
      "\n",
      "Adapting model for fine-tuning...\n",
      "âœ“ Pre-trained weights loaded\n",
      "âœ“ Frozen: 44/74, Trainable: 30/74\n",
      "\n",
      "Starting fine-tuning at 07:46:06\n",
      "Expected time: 30-60 minutes\n",
      "======================================================================\n",
      "Epoch 1/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.1707 - loss: 4.0119"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.11905, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 266ms/step - accuracy: 0.1687 - loss: 4.0181 - val_accuracy: 0.1190 - val_loss: 3.6095 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - accuracy: 0.1496 - loss: 3.8576\n",
      "Epoch 2: val_accuracy improved from 0.11905 to 0.13095, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 225ms/step - accuracy: 0.1493 - loss: 3.8553 - val_accuracy: 0.1310 - val_loss: 2.9442 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.1510 - loss: 3.5840\n",
      "Epoch 3: val_accuracy improved from 0.13095 to 0.14286, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 225ms/step - accuracy: 0.1511 - loss: 3.5797 - val_accuracy: 0.1429 - val_loss: 2.5957 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.1403 - loss: 3.3352\n",
      "Epoch 4: val_accuracy improved from 0.14286 to 0.22619, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 232ms/step - accuracy: 0.1413 - loss: 3.3289 - val_accuracy: 0.2262 - val_loss: 2.3345 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.2029 - loss: 2.9354\n",
      "Epoch 5: val_accuracy improved from 0.22619 to 0.26190, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 229ms/step - accuracy: 0.2026 - loss: 2.9390 - val_accuracy: 0.2619 - val_loss: 2.2628 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.2135 - loss: 2.7715\n",
      "Epoch 6: val_accuracy improved from 0.26190 to 0.28571, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 229ms/step - accuracy: 0.2149 - loss: 2.7707 - val_accuracy: 0.2857 - val_loss: 2.2512 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.2135 - loss: 2.8039\n",
      "Epoch 7: val_accuracy improved from 0.28571 to 0.32143, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 241ms/step - accuracy: 0.2141 - loss: 2.8028 - val_accuracy: 0.3214 - val_loss: 2.0925 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.2665 - loss: 2.4194\n",
      "Epoch 8: val_accuracy improved from 0.32143 to 0.35714, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 230ms/step - accuracy: 0.2665 - loss: 2.4214 - val_accuracy: 0.3571 - val_loss: 2.0748 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step - accuracy: 0.2534 - loss: 2.5644\n",
      "Epoch 9: val_accuracy did not improve from 0.35714\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 240ms/step - accuracy: 0.2538 - loss: 2.5613 - val_accuracy: 0.3333 - val_loss: 2.0870 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.3581 - loss: 2.2621\n",
      "Epoch 10: val_accuracy improved from 0.35714 to 0.36310, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 229ms/step - accuracy: 0.3556 - loss: 2.2647 - val_accuracy: 0.3631 - val_loss: 1.9886 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.2607 - loss: 2.5012\n",
      "Epoch 11: val_accuracy improved from 0.36310 to 0.39881, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 230ms/step - accuracy: 0.2636 - loss: 2.4918 - val_accuracy: 0.3988 - val_loss: 1.9346 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step - accuracy: 0.3408 - loss: 2.1375\n",
      "Epoch 12: val_accuracy improved from 0.39881 to 0.42262, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 241ms/step - accuracy: 0.3406 - loss: 2.1388 - val_accuracy: 0.4226 - val_loss: 1.8967 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - accuracy: 0.3236 - loss: 1.9704\n",
      "Epoch 13: val_accuracy improved from 0.42262 to 0.45238, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 223ms/step - accuracy: 0.3241 - loss: 1.9743 - val_accuracy: 0.4524 - val_loss: 1.8457 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.3463 - loss: 1.9921\n",
      "Epoch 14: val_accuracy did not improve from 0.45238\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 229ms/step - accuracy: 0.3454 - loss: 1.9955 - val_accuracy: 0.4048 - val_loss: 1.7814 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.3208 - loss: 2.1889\n",
      "Epoch 15: val_accuracy improved from 0.45238 to 0.47619, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 227ms/step - accuracy: 0.3219 - loss: 2.1797 - val_accuracy: 0.4762 - val_loss: 1.7539 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.3332 - loss: 2.0831\n",
      "Epoch 16: val_accuracy did not improve from 0.47619\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 228ms/step - accuracy: 0.3347 - loss: 2.0830 - val_accuracy: 0.4702 - val_loss: 1.7897 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.3541 - loss: 1.9779\n",
      "Epoch 17: val_accuracy improved from 0.47619 to 0.48810, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 228ms/step - accuracy: 0.3542 - loss: 1.9818 - val_accuracy: 0.4881 - val_loss: 1.7520 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.4054 - loss: 1.8918\n",
      "Epoch 18: val_accuracy did not improve from 0.48810\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 223ms/step - accuracy: 0.4043 - loss: 1.8963 - val_accuracy: 0.4643 - val_loss: 1.6405 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.3767 - loss: 1.9265\n",
      "Epoch 19: val_accuracy did not improve from 0.48810\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 222ms/step - accuracy: 0.3768 - loss: 1.9286 - val_accuracy: 0.4881 - val_loss: 1.6157 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.3614 - loss: 1.9546\n",
      "Epoch 20: val_accuracy did not improve from 0.48810\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 225ms/step - accuracy: 0.3617 - loss: 1.9533 - val_accuracy: 0.4821 - val_loss: 1.6262 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.3715 - loss: 1.9131\n",
      "Epoch 21: val_accuracy did not improve from 0.48810\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 219ms/step - accuracy: 0.3724 - loss: 1.9122 - val_accuracy: 0.4702 - val_loss: 1.6196 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.3586 - loss: 1.9869\n",
      "Epoch 22: val_accuracy did not improve from 0.48810\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 222ms/step - accuracy: 0.3594 - loss: 1.9799 - val_accuracy: 0.4762 - val_loss: 1.5439 - learning_rate: 1.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.3471 - loss: 1.9449\n",
      "Epoch 23: val_accuracy improved from 0.48810 to 0.50595, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 227ms/step - accuracy: 0.3473 - loss: 1.9439 - val_accuracy: 0.5060 - val_loss: 1.5835 - learning_rate: 1.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.3807 - loss: 1.8597\n",
      "Epoch 24: val_accuracy improved from 0.50595 to 0.52976, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 231ms/step - accuracy: 0.3826 - loss: 1.8554 - val_accuracy: 0.5298 - val_loss: 1.5288 - learning_rate: 1.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 182ms/step - accuracy: 0.4210 - loss: 1.8103\n",
      "Epoch 25: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 239ms/step - accuracy: 0.4211 - loss: 1.8074 - val_accuracy: 0.5119 - val_loss: 1.5775 - learning_rate: 1.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.4271 - loss: 1.7781\n",
      "Epoch 26: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 226ms/step - accuracy: 0.4275 - loss: 1.7774 - val_accuracy: 0.4881 - val_loss: 1.5381 - learning_rate: 1.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m16/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”\u001b[0m \u001b[1m0s\u001b[0m 177ms/step - accuracy: 0.4134 - loss: 1.7605\n",
      "Epoch 27: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 222ms/step - accuracy: 0.4174 - loss: 1.7500 - val_accuracy: 0.5119 - val_loss: 1.4215 - learning_rate: 1.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.3913 - loss: 1.8650\n",
      "Epoch 28: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 224ms/step - accuracy: 0.3930 - loss: 1.8595 - val_accuracy: 0.5000 - val_loss: 1.5019 - learning_rate: 1.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - accuracy: 0.4024 - loss: 1.6480\n",
      "Epoch 29: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 235ms/step - accuracy: 0.4031 - loss: 1.6496 - val_accuracy: 0.5060 - val_loss: 1.4894 - learning_rate: 1.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.4690 - loss: 1.5929\n",
      "Epoch 30: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 225ms/step - accuracy: 0.4682 - loss: 1.5942 - val_accuracy: 0.5238 - val_loss: 1.4142 - learning_rate: 1.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.4490 - loss: 1.7680\n",
      "Epoch 31: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 224ms/step - accuracy: 0.4487 - loss: 1.7690 - val_accuracy: 0.5179 - val_loss: 1.4819 - learning_rate: 1.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.4627 - loss: 1.5597\n",
      "Epoch 32: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 226ms/step - accuracy: 0.4623 - loss: 1.5648 - val_accuracy: 0.5119 - val_loss: 1.4666 - learning_rate: 1.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.4521 - loss: 1.6609\n",
      "Epoch 33: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 224ms/step - accuracy: 0.4527 - loss: 1.6564 - val_accuracy: 0.5238 - val_loss: 1.4228 - learning_rate: 1.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.4793 - loss: 1.6279\n",
      "Epoch 34: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 224ms/step - accuracy: 0.4774 - loss: 1.6329 - val_accuracy: 0.5298 - val_loss: 1.3747 - learning_rate: 1.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 181ms/step - accuracy: 0.4457 - loss: 1.5219\n",
      "Epoch 35: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 237ms/step - accuracy: 0.4471 - loss: 1.5248 - val_accuracy: 0.5119 - val_loss: 1.3999 - learning_rate: 1.0000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.4421 - loss: 1.5872\n",
      "Epoch 36: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 219ms/step - accuracy: 0.4430 - loss: 1.5850 - val_accuracy: 0.5060 - val_loss: 1.3822 - learning_rate: 1.0000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.4627 - loss: 1.5927\n",
      "Epoch 37: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 223ms/step - accuracy: 0.4636 - loss: 1.5921 - val_accuracy: 0.5298 - val_loss: 1.3273 - learning_rate: 1.0000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.4375 - loss: 1.6176\n",
      "Epoch 38: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 221ms/step - accuracy: 0.4380 - loss: 1.6178 - val_accuracy: 0.5238 - val_loss: 1.3841 - learning_rate: 1.0000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.4186 - loss: 1.6353\n",
      "Epoch 39: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 224ms/step - accuracy: 0.4208 - loss: 1.6308 - val_accuracy: 0.5238 - val_loss: 1.3436 - learning_rate: 1.0000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.4962 - loss: 1.4686\n",
      "Epoch 40: val_accuracy improved from 0.52976 to 0.54167, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 223ms/step - accuracy: 0.4942 - loss: 1.4722 - val_accuracy: 0.5417 - val_loss: 1.3317 - learning_rate: 1.0000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.4539 - loss: 1.5473\n",
      "Epoch 41: val_accuracy did not improve from 0.54167\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 228ms/step - accuracy: 0.4542 - loss: 1.5495 - val_accuracy: 0.4940 - val_loss: 1.4249 - learning_rate: 1.0000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.5038 - loss: 1.5519\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.54167\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 224ms/step - accuracy: 0.5039 - loss: 1.5491 - val_accuracy: 0.5298 - val_loss: 1.3618 - learning_rate: 1.0000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.4121 - loss: 1.6225\n",
      "Epoch 43: val_accuracy did not improve from 0.54167\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 223ms/step - accuracy: 0.4154 - loss: 1.6164 - val_accuracy: 0.5119 - val_loss: 1.3250 - learning_rate: 5.0000e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.4358 - loss: 1.6388\n",
      "Epoch 44: val_accuracy did not improve from 0.54167\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 225ms/step - accuracy: 0.4378 - loss: 1.6319 - val_accuracy: 0.5357 - val_loss: 1.2765 - learning_rate: 5.0000e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step - accuracy: 0.4788 - loss: 1.5091\n",
      "Epoch 45: val_accuracy improved from 0.54167 to 0.55357, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 232ms/step - accuracy: 0.4795 - loss: 1.5091 - val_accuracy: 0.5536 - val_loss: 1.3055 - learning_rate: 5.0000e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - accuracy: 0.4818 - loss: 1.5218\n",
      "Epoch 46: val_accuracy did not improve from 0.55357\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 223ms/step - accuracy: 0.4828 - loss: 1.5192 - val_accuracy: 0.5179 - val_loss: 1.3195 - learning_rate: 5.0000e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step - accuracy: 0.5178 - loss: 1.3560\n",
      "Epoch 47: val_accuracy did not improve from 0.55357\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 224ms/step - accuracy: 0.5174 - loss: 1.3586 - val_accuracy: 0.5476 - val_loss: 1.3378 - learning_rate: 5.0000e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step - accuracy: 0.5045 - loss: 1.3952\n",
      "Epoch 48: val_accuracy improved from 0.55357 to 0.55952, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 233ms/step - accuracy: 0.5035 - loss: 1.3985 - val_accuracy: 0.5595 - val_loss: 1.2992 - learning_rate: 5.0000e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - accuracy: 0.4496 - loss: 1.5365\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "\n",
      "Epoch 49: val_accuracy improved from 0.55952 to 0.56548, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 232ms/step - accuracy: 0.4518 - loss: 1.5310 - val_accuracy: 0.5655 - val_loss: 1.2886 - learning_rate: 5.0000e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - accuracy: 0.4921 - loss: 1.5513\n",
      "Epoch 50: val_accuracy did not improve from 0.56548\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 227ms/step - accuracy: 0.4924 - loss: 1.5472 - val_accuracy: 0.5298 - val_loss: 1.3143 - learning_rate: 2.5000e-05\n",
      "Restoring model weights from the end of the best epoch: 44.\n",
      "\n",
      "âœ“ Fine-tuning complete in 3.3 minutes\n",
      "\n",
      "âœ“ Fine-tuning results:\n",
      "  Validation Accuracy: 55.36%\n",
      "  Validation Loss: 1.3346\n",
      "\n",
      "======================================================================\n",
      "PYRAMIDNET-18 PIPELINE COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š FINAL RESULTS:\n",
      "\n",
      "Phase 1 - Pre-training:\n",
      "  Time:     0.63 hours\n",
      "  Accuracy: 97.74%\n",
      "  Model:    /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_pretrained.keras\n",
      "\n",
      "Phase 2 - Fine-tuning:\n",
      "  Time:     3.3 minutes\n",
      "  Accuracy: 55.36%\n",
      "  Model:    /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_finetuned.keras\n",
      "\n",
      "ğŸ“ Results: /Users/vishal/Desktop/ML study/project/CNN type/first/pyramidnet18_results/\n",
      "  - pretrain_history.csv\n",
      "  - pretrain_curves.png\n",
      "  - finetune_history.csv\n",
      "  - finetune_curves.png\n",
      "\n",
      "======================================================================\n",
      "âœ… PyramidNet-18 trained and ready!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PyramidNet-18 Complete Training Pipeline:\n",
    "1. Pre-train from scratch on Fruit-360 apples (10K subset)\n",
    "2. Fine-tune on your curated handpicked data (800 images)\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "PROJECT_ROOT = '/Users/vishal/Desktop/ML study/project/CNN type/first'\n",
    "\n",
    "FRUIT360_10K = os.path.join(PROJECT_ROOT, 'fruit360_apples_10k_subset')\n",
    "CURATED_DATA = os.path.join(PROJECT_ROOT, 'data set fine tune')\n",
    "\n",
    "PRETRAIN_MODEL_PATH = os.path.join(PROJECT_ROOT, 'pyramidnet18_pretrained.keras')\n",
    "FINETUNE_MODEL_PATH = os.path.join(PROJECT_ROOT, 'pyramidnet18_finetuned.keras')\n",
    "RESULTS_PATH = os.path.join(PROJECT_ROOT, 'pyramidnet18_results')\n",
    "\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "PRETRAIN_EPOCHS = 100\n",
    "FINETUNE_EPOCHS = 50\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" \"*12 + \"PYRAMIDNET-18 COMPLETE TRAINING PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nPhase 1: Pre-train on Fruit-360 (10K images)\")\n",
    "print(\"Phase 2: Fine-tune on your curated data (800 images)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# PYRAMIDNET-18 ARCHITECTURE\n",
    "# ============================================\n",
    "\n",
    "def pyramidnet_basicblock(x, in_filters, out_filters, stride=1):\n",
    "    \"\"\"PyramidNet Basic Block\"\"\"\n",
    "    shortcut = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(out_filters, 3, strides=stride, padding='same', use_bias=False)(x)\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(out_filters, 3, strides=1, padding='same', use_bias=False)(x)\n",
    "    \n",
    "    if in_filters != out_filters or stride != 1:\n",
    "        shortcut = layers.Conv2D(out_filters, 1, strides=stride, use_bias=False)(shortcut)\n",
    "    \n",
    "    x = layers.Add()([x, shortcut])\n",
    "    return x\n",
    "\n",
    "def build_pyramidnet18(num_classes, alpha=48, input_shape=(224, 224, 3)):\n",
    "    \"\"\"Build PyramidNet-18\"\"\"\n",
    "    \n",
    "    num_blocks = 8\n",
    "    add_channels_per_block = alpha / num_blocks\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    start_filters = 16\n",
    "    x = layers.Conv2D(start_filters, 7, strides=2, padding='same', use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "    \n",
    "    current_filters = start_filters\n",
    "    \n",
    "    # Stage 1: Conv2_x (2 blocks)\n",
    "    for i in range(2):\n",
    "        out_filters = int(start_filters + add_channels_per_block * (i + 1))\n",
    "        x = pyramidnet_basicblock(x, int(current_filters), out_filters, stride=1)\n",
    "        current_filters = out_filters\n",
    "    \n",
    "    # Stage 2: Conv3_x (2 blocks)\n",
    "    for i in range(2):\n",
    "        out_filters = int(start_filters + add_channels_per_block * (2 + i + 1))\n",
    "        stride = 2 if i == 0 else 1\n",
    "        x = pyramidnet_basicblock(x, int(current_filters), out_filters, stride=stride)\n",
    "        current_filters = out_filters\n",
    "    \n",
    "    # Stage 3: Conv4_x (2 blocks)\n",
    "    for i in range(2):\n",
    "        out_filters = int(start_filters + add_channels_per_block * (4 + i + 1))\n",
    "        stride = 2 if i == 0 else 1\n",
    "        x = pyramidnet_basicblock(x, int(current_filters), out_filters, stride=stride)\n",
    "        current_filters = out_filters\n",
    "    \n",
    "    # Stage 4: Conv5_x (2 blocks)\n",
    "    for i in range(2):\n",
    "        out_filters = int(start_filters + add_channels_per_block * (6 + i + 1))\n",
    "        stride = 2 if i == 0 else 1\n",
    "        x = pyramidnet_basicblock(x, int(current_filters), out_filters, stride=stride)\n",
    "        current_filters = out_filters\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='PyramidNet18')\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# PHASE 1: PRE-TRAINING\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 1: PRE-TRAINING ON FRUIT-360\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nLoading Fruit-360 data (10K subset)...\")\n",
    "\n",
    "pretrain_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "pretrain_train = pretrain_datagen.flow_from_directory(\n",
    "    os.path.join(FRUIT360_10K, 'Training'),\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "pretrain_val = pretrain_datagen.flow_from_directory(\n",
    "    os.path.join(FRUIT360_10K, 'Training'),\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "pretrain_test = test_datagen.flow_from_directory(\n",
    "    os.path.join(FRUIT360_10K, 'Test'),\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_pretrain_classes = pretrain_train.num_classes\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded:\")\n",
    "print(f\"  Training:   {pretrain_train.samples} images\")\n",
    "print(f\"  Validation: {pretrain_val.samples} images\")\n",
    "print(f\"  Test:       {pretrain_test.samples} images\")\n",
    "print(f\"  Classes:    {num_pretrain_classes}\")\n",
    "\n",
    "print(\"\\nBuilding PyramidNet-18 from scratch...\")\n",
    "pretrain_model = build_pyramidnet18(num_classes=num_pretrain_classes, alpha=48)\n",
    "\n",
    "print(f\"âœ“ Model built: {pretrain_model.count_params():,} parameters\")\n",
    "\n",
    "pretrain_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "pretrain_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-7, verbose=1),\n",
    "    ModelCheckpoint(PRETRAIN_MODEL_PATH, monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "print(f\"\\nStarting pre-training at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"Expected time: 6-10 hours\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "pretrain_start = time.time()\n",
    "\n",
    "pretrain_history = pretrain_model.fit(\n",
    "    pretrain_train,\n",
    "    epochs=PRETRAIN_EPOCHS,\n",
    "    validation_data=pretrain_val,\n",
    "    callbacks=pretrain_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "pretrain_time = time.time() - pretrain_start\n",
    "\n",
    "print(f\"\\nâœ“ Pre-training complete in {pretrain_time/3600:.2f} hours\")\n",
    "\n",
    "pretrain_test_loss, pretrain_test_acc = pretrain_model.evaluate(pretrain_test, verbose=0)\n",
    "\n",
    "print(f\"\\nâœ“ Pre-training results:\")\n",
    "print(f\"  Test Accuracy: {pretrain_test_acc*100:.2f}%\")\n",
    "print(f\"  Test Loss: {pretrain_test_loss:.4f}\")\n",
    "\n",
    "pd.DataFrame(pretrain_history.history).to_csv(\n",
    "    os.path.join(RESULTS_PATH, 'pretrain_history.csv'), index=False)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(pretrain_history.history['accuracy'], label='Train', linewidth=2)\n",
    "plt.plot(pretrain_history.history['val_accuracy'], label='Val', linewidth=2)\n",
    "plt.title('Pre-training: Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(pretrain_history.history['loss'], label='Train', linewidth=2)\n",
    "plt.plot(pretrain_history.history['val_loss'], label='Val', linewidth=2)\n",
    "plt.title('Pre-training: Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_PATH, 'pretrain_curves.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# ============================================\n",
    "# PHASE 2: FINE-TUNING\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PHASE 2: FINE-TUNING ON YOUR CURATED DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nLoading your curated data...\")\n",
    "\n",
    "finetune_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.25\n",
    ")\n",
    "\n",
    "finetune_train = finetune_datagen.flow_from_directory(\n",
    "    CURATED_DATA,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "finetune_val = finetune_datagen.flow_from_directory(\n",
    "    CURATED_DATA,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_finetune_classes = finetune_train.num_classes\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded:\")\n",
    "print(f\"  Training:   {finetune_train.samples} images\")\n",
    "print(f\"  Validation: {finetune_val.samples} images\")\n",
    "print(f\"  Classes:    {num_finetune_classes}\")\n",
    "\n",
    "print(\"\\nAdapting model for fine-tuning...\")\n",
    "\n",
    "pretrained_model = keras.models.load_model(PRETRAIN_MODEL_PATH)\n",
    "\n",
    "finetune_model = build_pyramidnet18(num_classes=num_finetune_classes, alpha=48)\n",
    "\n",
    "for pretrained_layer, finetune_layer in zip(pretrained_model.layers[:-1], finetune_model.layers[:-1]):\n",
    "    if len(pretrained_layer.get_weights()) > 0:\n",
    "        finetune_layer.set_weights(pretrained_layer.get_weights())\n",
    "\n",
    "print(\"âœ“ Pre-trained weights loaded\")\n",
    "\n",
    "total_layers = len(finetune_model.layers)\n",
    "freeze_until = int(total_layers * 0.6)\n",
    "\n",
    "for i, layer in enumerate(finetune_model.layers):\n",
    "    layer.trainable = (i >= freeze_until)\n",
    "\n",
    "trainable = sum([1 for layer in finetune_model.layers if layer.trainable])\n",
    "print(f\"âœ“ Frozen: {freeze_until}/{total_layers}, Trainable: {trainable}/{total_layers}\")\n",
    "\n",
    "finetune_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "finetune_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-8, verbose=1),\n",
    "    ModelCheckpoint(FINETUNE_MODEL_PATH, monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "print(f\"\\nStarting fine-tuning at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"Expected time: 30-60 minutes\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "finetune_start = time.time()\n",
    "\n",
    "finetune_history = finetune_model.fit(\n",
    "    finetune_train,\n",
    "    epochs=FINETUNE_EPOCHS,\n",
    "    validation_data=finetune_val,\n",
    "    callbacks=finetune_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "finetune_time = time.time() - finetune_start\n",
    "\n",
    "print(f\"\\nâœ“ Fine-tuning complete in {finetune_time/60:.1f} minutes\")\n",
    "\n",
    "finetune_val_loss, finetune_val_acc = finetune_model.evaluate(finetune_val, verbose=0)\n",
    "\n",
    "print(f\"\\nâœ“ Fine-tuning results:\")\n",
    "print(f\"  Validation Accuracy: {finetune_val_acc*100:.2f}%\")\n",
    "print(f\"  Validation Loss: {finetune_val_loss:.4f}\")\n",
    "\n",
    "pd.DataFrame(finetune_history.history).to_csv(\n",
    "    os.path.join(RESULTS_PATH, 'finetune_history.csv'), index=False)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(finetune_history.history['accuracy'], label='Train', linewidth=2)\n",
    "plt.plot(finetune_history.history['val_accuracy'], label='Val', linewidth=2)\n",
    "plt.title('Fine-tuning: Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(finetune_history.history['loss'], label='Train', linewidth=2)\n",
    "plt.plot(finetune_history.history['val_loss'], label='Val', linewidth=2)\n",
    "plt.title('Fine-tuning: Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(RESULTS_PATH, 'finetune_curves.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# ============================================\n",
    "# SUMMARY\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PYRAMIDNET-18 PIPELINE COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nğŸ“Š FINAL RESULTS:\")\n",
    "print(f\"\\nPhase 1 - Pre-training:\")\n",
    "print(f\"  Time:     {pretrain_time/3600:.2f} hours\")\n",
    "print(f\"  Accuracy: {pretrain_test_acc*100:.2f}%\")\n",
    "print(f\"  Model:    {PRETRAIN_MODEL_PATH}\")\n",
    "\n",
    "print(f\"\\nPhase 2 - Fine-tuning:\")\n",
    "print(f\"  Time:     {finetune_time/60:.1f} minutes\")\n",
    "print(f\"  Accuracy: {finetune_val_acc*100:.2f}%\")\n",
    "print(f\"  Model:    {FINETUNE_MODEL_PATH}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Results: {RESULTS_PATH}/\")\n",
    "print(f\"  - pretrain_history.csv\")\n",
    "print(f\"  - pretrain_curves.png\")\n",
    "print(f\"  - finetune_history.csv\")\n",
    "print(f\"  - finetune_curves.png\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… PyramidNet-18 trained and ready!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6def8b28-e8ab-4965-8856-2954dc608a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5770c8a7-6039-4e44-9b8b-a48b60c9e124",
   "metadata": {},
   "source": [
    "# EXPERIMENT 1: Unfreeze All Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4caab34a-7529-463e-9700-b759a3066cc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "          EXPERIMENT 1: UNFREEZE ALL LAYERS\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š BASELINE (Previous Run):\n",
      "  Frozen layers: 60%\n",
      "  Learning rate: 0.0001\n",
      "  Max epochs: 50\n",
      "  Result: 55.36% val accuracy\n",
      "\n",
      "ğŸ§ª EXPERIMENT 1:\n",
      "  Frozen layers: 0% (ALL TRAINABLE)\n",
      "  Learning rate: 0.0001 (unchanged)\n",
      "  Max epochs: 50 (unchanged)\n",
      "  Hypothesis: Unfreezing will improve accuracy to 70-85%\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ Loading your curated data...\n",
      "Found 519 images belonging to 8 classes.\n",
      "Found 168 images belonging to 8 classes.\n",
      "\n",
      "âœ“ Data loaded:\n",
      "  Training:   519 images\n",
      "  Validation: 168 images\n",
      "  Classes:    8\n",
      "\n",
      "ğŸ”§ Loading pre-trained model...\n",
      "âœ“ Pre-trained model loaded\n",
      "ğŸ”§ Building model for 8 classes...\n",
      "ğŸ“¦ Transferring pre-trained weights...\n",
      "âœ“ Pre-trained weights transferred\n",
      "\n",
      "======================================================================\n",
      "ğŸ”“ UNFREEZING ALL LAYERS\n",
      "======================================================================\n",
      "\n",
      "âœ“ Layer status:\n",
      "  Total layers: 74\n",
      "  Trainable before: 74\n",
      "  Trainable after: 74\n",
      "  Frozen: 0 layers (0%)\n",
      "  Training: 74 layers (100%)\n",
      "\n",
      "âœ“ Trainable parameters: 293,448\n",
      "\n",
      "âš™ï¸  Compiling model...\n",
      "âœ“ Optimizer: Adam (lr=0.0001)\n",
      "âœ“ Loss: Categorical Crossentropy\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ STARTING EXPERIMENT 1 TRAINING\n",
      "======================================================================\n",
      "Started at: 05:22:05\n",
      "Max epochs: 50\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.1539 - loss: 3.2894"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.18452, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 309ms/step - accuracy: 0.1547 - loss: 3.2840 - val_accuracy: 0.1845 - val_loss: 2.3427 - learning_rate: 1.0000e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.2104 - loss: 2.8674\n",
      "Epoch 2: val_accuracy improved from 0.18452 to 0.27381, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 274ms/step - accuracy: 0.2108 - loss: 2.8645 - val_accuracy: 0.2738 - val_loss: 2.1483 - learning_rate: 1.0000e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.2110 - loss: 2.7583\n",
      "Epoch 3: val_accuracy improved from 0.27381 to 0.30357, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 276ms/step - accuracy: 0.2112 - loss: 2.7588 - val_accuracy: 0.3036 - val_loss: 1.9791 - learning_rate: 1.0000e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.2234 - loss: 2.5945\n",
      "Epoch 4: val_accuracy improved from 0.30357 to 0.35119, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 278ms/step - accuracy: 0.2230 - loss: 2.6017 - val_accuracy: 0.3512 - val_loss: 1.9797 - learning_rate: 1.0000e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.2807 - loss: 2.5551\n",
      "Epoch 5: val_accuracy improved from 0.35119 to 0.40476, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 281ms/step - accuracy: 0.2797 - loss: 2.5524 - val_accuracy: 0.4048 - val_loss: 1.8651 - learning_rate: 1.0000e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.2914 - loss: 2.3448\n",
      "Epoch 6: val_accuracy improved from 0.40476 to 0.42262, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 278ms/step - accuracy: 0.2921 - loss: 2.3426 - val_accuracy: 0.4226 - val_loss: 1.8730 - learning_rate: 1.0000e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.3361 - loss: 2.3228\n",
      "Epoch 7: val_accuracy improved from 0.42262 to 0.43452, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 283ms/step - accuracy: 0.3354 - loss: 2.3198 - val_accuracy: 0.4345 - val_loss: 1.8444 - learning_rate: 1.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.3274 - loss: 2.1188\n",
      "Epoch 8: val_accuracy improved from 0.43452 to 0.47024, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.3266 - loss: 2.1202 - val_accuracy: 0.4702 - val_loss: 1.7889 - learning_rate: 1.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.3294 - loss: 2.0668\n",
      "Epoch 9: val_accuracy improved from 0.47024 to 0.48214, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.3291 - loss: 2.0744 - val_accuracy: 0.4821 - val_loss: 1.7646 - learning_rate: 1.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.3469 - loss: 2.0585\n",
      "Epoch 10: val_accuracy did not improve from 0.48214\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 286ms/step - accuracy: 0.3480 - loss: 2.0570 - val_accuracy: 0.4345 - val_loss: 1.7443 - learning_rate: 1.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.3207 - loss: 2.1006\n",
      "Epoch 11: val_accuracy did not improve from 0.48214\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.3227 - loss: 2.0960 - val_accuracy: 0.4464 - val_loss: 1.6695 - learning_rate: 1.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.3574 - loss: 2.1525\n",
      "Epoch 12: val_accuracy did not improve from 0.48214\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 288ms/step - accuracy: 0.3585 - loss: 2.1422 - val_accuracy: 0.4643 - val_loss: 1.6922 - learning_rate: 1.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.4120 - loss: 1.8580\n",
      "Epoch 13: val_accuracy improved from 0.48214 to 0.49405, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 289ms/step - accuracy: 0.4108 - loss: 1.8609 - val_accuracy: 0.4940 - val_loss: 1.6569 - learning_rate: 1.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.3848 - loss: 1.8180\n",
      "Epoch 14: val_accuracy did not improve from 0.49405\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.3858 - loss: 1.8157 - val_accuracy: 0.4940 - val_loss: 1.6260 - learning_rate: 1.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.4019 - loss: 1.8614\n",
      "Epoch 15: val_accuracy did not improve from 0.49405\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 290ms/step - accuracy: 0.4013 - loss: 1.8590 - val_accuracy: 0.4881 - val_loss: 1.6564 - learning_rate: 1.0000e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.4221 - loss: 1.8367\n",
      "Epoch 16: val_accuracy improved from 0.49405 to 0.50000, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.4226 - loss: 1.8306 - val_accuracy: 0.5000 - val_loss: 1.6020 - learning_rate: 1.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.4080 - loss: 1.7774\n",
      "Epoch 17: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 290ms/step - accuracy: 0.4067 - loss: 1.7782 - val_accuracy: 0.4702 - val_loss: 1.6655 - learning_rate: 1.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.4331 - loss: 1.6591\n",
      "Epoch 18: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 290ms/step - accuracy: 0.4315 - loss: 1.6648 - val_accuracy: 0.4940 - val_loss: 1.6062 - learning_rate: 1.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.4136 - loss: 1.7562\n",
      "Epoch 19: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 282ms/step - accuracy: 0.4140 - loss: 1.7532 - val_accuracy: 0.4464 - val_loss: 1.6620 - learning_rate: 1.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.4796 - loss: 1.7303\n",
      "Epoch 20: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 276ms/step - accuracy: 0.4783 - loss: 1.7295 - val_accuracy: 0.5000 - val_loss: 1.6259 - learning_rate: 1.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.4540 - loss: 1.5561\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 289ms/step - accuracy: 0.4535 - loss: 1.5570 - val_accuracy: 0.4702 - val_loss: 1.6359 - learning_rate: 1.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.4297 - loss: 1.8242\n",
      "Epoch 22: val_accuracy improved from 0.50000 to 0.52381, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 286ms/step - accuracy: 0.4313 - loss: 1.8122 - val_accuracy: 0.5238 - val_loss: 1.5578 - learning_rate: 5.0000e-05\n",
      "Epoch 23/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.4127 - loss: 1.6616\n",
      "Epoch 23: val_accuracy did not improve from 0.52381\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 280ms/step - accuracy: 0.4140 - loss: 1.6575 - val_accuracy: 0.4643 - val_loss: 1.5806 - learning_rate: 5.0000e-05\n",
      "Epoch 24/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.5081 - loss: 1.5007\n",
      "Epoch 24: val_accuracy did not improve from 0.52381\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 309ms/step - accuracy: 0.5076 - loss: 1.5022 - val_accuracy: 0.4583 - val_loss: 1.5401 - learning_rate: 5.0000e-05\n",
      "Epoch 25/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.4349 - loss: 1.5699\n",
      "Epoch 25: val_accuracy did not improve from 0.52381\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 285ms/step - accuracy: 0.4354 - loss: 1.5687 - val_accuracy: 0.5179 - val_loss: 1.4608 - learning_rate: 5.0000e-05\n",
      "Epoch 26/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.5120 - loss: 1.3934\n",
      "Epoch 26: val_accuracy improved from 0.52381 to 0.52976, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 287ms/step - accuracy: 0.5100 - loss: 1.3985 - val_accuracy: 0.5298 - val_loss: 1.4426 - learning_rate: 5.0000e-05\n",
      "Epoch 27/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.4691 - loss: 1.4909\n",
      "Epoch 27: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.4692 - loss: 1.4920 - val_accuracy: 0.5060 - val_loss: 1.4637 - learning_rate: 5.0000e-05\n",
      "Epoch 28/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.4548 - loss: 1.5170\n",
      "Epoch 28: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.4549 - loss: 1.5156 - val_accuracy: 0.5238 - val_loss: 1.4458 - learning_rate: 5.0000e-05\n",
      "Epoch 29/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.4604 - loss: 1.5653\n",
      "Epoch 29: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 288ms/step - accuracy: 0.4610 - loss: 1.5630 - val_accuracy: 0.5298 - val_loss: 1.4221 - learning_rate: 5.0000e-05\n",
      "Epoch 30/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.5040 - loss: 1.4665\n",
      "Epoch 30: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 287ms/step - accuracy: 0.5033 - loss: 1.4692 - val_accuracy: 0.5179 - val_loss: 1.3814 - learning_rate: 5.0000e-05\n",
      "Epoch 31/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.4516 - loss: 1.4858\n",
      "Epoch 31: val_accuracy did not improve from 0.52976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.4518 - loss: 1.4863 - val_accuracy: 0.5238 - val_loss: 1.3672 - learning_rate: 5.0000e-05\n",
      "Epoch 32/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.5076 - loss: 1.4406\n",
      "Epoch 32: val_accuracy improved from 0.52976 to 0.54762, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 297ms/step - accuracy: 0.5064 - loss: 1.4437 - val_accuracy: 0.5476 - val_loss: 1.4352 - learning_rate: 5.0000e-05\n",
      "Epoch 33/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - accuracy: 0.5313 - loss: 1.3425\n",
      "Epoch 33: val_accuracy improved from 0.54762 to 0.55952, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 313ms/step - accuracy: 0.5305 - loss: 1.3458 - val_accuracy: 0.5595 - val_loss: 1.3541 - learning_rate: 5.0000e-05\n",
      "Epoch 34/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.4914 - loss: 1.4659\n",
      "Epoch 34: val_accuracy did not improve from 0.55952\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.4921 - loss: 1.4636 - val_accuracy: 0.5357 - val_loss: 1.3611 - learning_rate: 5.0000e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.4767 - loss: 1.4382\n",
      "Epoch 35: val_accuracy did not improve from 0.55952\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 285ms/step - accuracy: 0.4768 - loss: 1.4384 - val_accuracy: 0.5417 - val_loss: 1.3413 - learning_rate: 5.0000e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.4481 - loss: 1.5140\n",
      "Epoch 36: val_accuracy did not improve from 0.55952\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 298ms/step - accuracy: 0.4492 - loss: 1.5092 - val_accuracy: 0.5357 - val_loss: 1.3984 - learning_rate: 5.0000e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.4592 - loss: 1.6333\n",
      "Epoch 37: val_accuracy did not improve from 0.55952\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 288ms/step - accuracy: 0.4626 - loss: 1.6215 - val_accuracy: 0.5595 - val_loss: 1.3577 - learning_rate: 5.0000e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.5306 - loss: 1.3012\n",
      "Epoch 38: val_accuracy did not improve from 0.55952\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 287ms/step - accuracy: 0.5294 - loss: 1.3034 - val_accuracy: 0.5476 - val_loss: 1.3159 - learning_rate: 5.0000e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.4887 - loss: 1.3920\n",
      "Epoch 39: val_accuracy improved from 0.55952 to 0.57738, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 283ms/step - accuracy: 0.4892 - loss: 1.3912 - val_accuracy: 0.5774 - val_loss: 1.3327 - learning_rate: 5.0000e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.4790 - loss: 1.4601\n",
      "Epoch 40: val_accuracy did not improve from 0.57738\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 284ms/step - accuracy: 0.4793 - loss: 1.4563 - val_accuracy: 0.5536 - val_loss: 1.3503 - learning_rate: 5.0000e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.4981 - loss: 1.3278\n",
      "Epoch 41: val_accuracy did not improve from 0.57738\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 287ms/step - accuracy: 0.4982 - loss: 1.3329 - val_accuracy: 0.5774 - val_loss: 1.2624 - learning_rate: 5.0000e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.5294 - loss: 1.3825\n",
      "Epoch 42: val_accuracy did not improve from 0.57738\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 278ms/step - accuracy: 0.5295 - loss: 1.3812 - val_accuracy: 0.5774 - val_loss: 1.2523 - learning_rate: 5.0000e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.5364 - loss: 1.3453\n",
      "Epoch 43: val_accuracy improved from 0.57738 to 0.58333, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 289ms/step - accuracy: 0.5354 - loss: 1.3480 - val_accuracy: 0.5833 - val_loss: 1.2702 - learning_rate: 5.0000e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.4916 - loss: 1.4092\n",
      "Epoch 44: val_accuracy did not improve from 0.58333\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 288ms/step - accuracy: 0.4934 - loss: 1.4055 - val_accuracy: 0.5833 - val_loss: 1.2466 - learning_rate: 5.0000e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.5012 - loss: 1.3804\n",
      "Epoch 45: val_accuracy did not improve from 0.58333\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 282ms/step - accuracy: 0.4997 - loss: 1.3858 - val_accuracy: 0.5536 - val_loss: 1.2962 - learning_rate: 5.0000e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.5085 - loss: 1.4956\n",
      "Epoch 46: val_accuracy improved from 0.58333 to 0.60119, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 303ms/step - accuracy: 0.5099 - loss: 1.4888 - val_accuracy: 0.6012 - val_loss: 1.1913 - learning_rate: 5.0000e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.5344 - loss: 1.2422\n",
      "Epoch 47: val_accuracy did not improve from 0.60119\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.5350 - loss: 1.2442 - val_accuracy: 0.5833 - val_loss: 1.2088 - learning_rate: 5.0000e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.5166 - loss: 1.3732\n",
      "Epoch 48: val_accuracy did not improve from 0.60119\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 284ms/step - accuracy: 0.5160 - loss: 1.3734 - val_accuracy: 0.5714 - val_loss: 1.2744 - learning_rate: 5.0000e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - accuracy: 0.5133 - loss: 1.3337\n",
      "Epoch 49: val_accuracy did not improve from 0.60119\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 296ms/step - accuracy: 0.5135 - loss: 1.3340 - val_accuracy: 0.5833 - val_loss: 1.2438 - learning_rate: 5.0000e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.5167 - loss: 1.3187\n",
      "Epoch 50: val_accuracy did not improve from 0.60119\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 287ms/step - accuracy: 0.5175 - loss: 1.3190 - val_accuracy: 0.6012 - val_loss: 1.2984 - learning_rate: 5.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 46.\n",
      "\n",
      "âœ“ Training complete in 4.2 minutes\n",
      "\n",
      "ğŸ“Š Evaluating...\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š EXPERIMENT 1 RESULTS\n",
      "======================================================================\n",
      "\n",
      "Final Metrics:\n",
      "  Training Accuracy:   53.18%\n",
      "  Validation Accuracy: 58.93%\n",
      "  Best Val Accuracy:   60.12%\n",
      "  Validation Loss:     1.2241\n",
      "  Epochs Trained:      50/50\n",
      "  Training Time:       4.2 minutes\n",
      "\n",
      "======================================================================\n",
      "ğŸ“ˆ COMPARISON: Baseline vs Experiment 1\n",
      "======================================================================\n",
      "\n",
      "Baseline (60% frozen):\n",
      "  Validation Accuracy: 55.36%\n",
      "\n",
      "Experiment 1 (0% frozen):\n",
      "  Validation Accuracy: 58.93%\n",
      "\n",
      "Improvement: +3.57 percentage points\n",
      "âš ï¸  HYPOTHESIS WEAK: Unfreezing helped slightly\n",
      "\n",
      "ğŸ’¾ Saving results...\n",
      "âœ“ Results saved to: /Users/vishal/Desktop/ML study/project/CNN type/first/experiment1_results/\n",
      "  - training_history.csv\n",
      "  - experiment_summary.csv\n",
      "  - training_curves.png\n",
      "\n",
      "======================================================================\n",
      "âœ… EXPERIMENT 1 COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ Next Steps:\n",
      "  âš ï¸  Unfreezing didn't help much. Try:\n",
      "    - Experiment 2: Increase learning rate significantly\n",
      "    - Check if data quality is the issue\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EXPERIMENT 1: Unfreeze All Layers\n",
    "\n",
    "Baseline: 55.36% val accuracy (60% frozen)\n",
    "Change: Unfreeze ALL layers (0% frozen)\n",
    "Keep same: Learning rate 0.0001, Max epochs 50\n",
    "\n",
    "Goal: Test if freezing was the problem\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "PROJECT_ROOT = '/Users/vishal/Desktop/ML study/project/CNN type/first'\n",
    "\n",
    "CURATED_DATA = os.path.join(PROJECT_ROOT, 'data set fine tune')\n",
    "PRETRAIN_MODEL_PATH = os.path.join(PROJECT_ROOT, 'pyramidnet18_pretrained.keras')\n",
    "\n",
    "# Experiment 1 outputs\n",
    "EXP1_MODEL_PATH = os.path.join(PROJECT_ROOT, 'experiment1_model.keras')\n",
    "EXP1_RESULTS_PATH = os.path.join(PROJECT_ROOT, 'experiment1_results')\n",
    "\n",
    "os.makedirs(EXP1_RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "# Parameters (KEEP SAME AS BASELINE except unfreezing)\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50  # Same as baseline\n",
    "LEARNING_RATE = 0.0001  # Same as baseline\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" \"*10 + \"EXPERIMENT 1: UNFREEZE ALL LAYERS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nğŸ“Š BASELINE (Previous Run):\")\n",
    "print(\"  Frozen layers: 60%\")\n",
    "print(\"  Learning rate: 0.0001\")\n",
    "print(\"  Max epochs: 50\")\n",
    "print(\"  Result: 55.36% val accuracy\")\n",
    "\n",
    "print(\"\\nğŸ§ª EXPERIMENT 1:\")\n",
    "print(\"  Frozen layers: 0% (ALL TRAINABLE)\")\n",
    "print(\"  Learning rate: 0.0001 (unchanged)\")\n",
    "print(\"  Max epochs: 50 (unchanged)\")\n",
    "print(\"  Hypothesis: Unfreezing will improve accuracy to 70-85%\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# LOAD DATA\n",
    "# ============================================\n",
    "print(\"\\nğŸ“ Loading your curated data...\")\n",
    "\n",
    "finetune_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.25\n",
    ")\n",
    "\n",
    "train_generator = finetune_datagen.flow_from_directory(\n",
    "    CURATED_DATA,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = finetune_datagen.flow_from_directory(\n",
    "    CURATED_DATA,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded:\")\n",
    "print(f\"  Training:   {train_generator.samples} images\")\n",
    "print(f\"  Validation: {val_generator.samples} images\")\n",
    "print(f\"  Classes:    {num_classes}\")\n",
    "\n",
    "# ============================================\n",
    "# PYRAMIDNET ARCHITECTURE (for rebuilding)\n",
    "# ============================================\n",
    "\n",
    "def pyramidnet_basicblock(x, in_filters, out_filters, stride=1):\n",
    "    \"\"\"PyramidNet Basic Block\"\"\"\n",
    "    shortcut = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(out_filters, 3, strides=stride, padding='same', use_bias=False)(x)\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(out_filters, 3, strides=1, padding='same', use_bias=False)(x)\n",
    "    \n",
    "    if in_filters != out_filters or stride != 1:\n",
    "        shortcut = layers.Conv2D(out_filters, 1, strides=stride, use_bias=False)(shortcut)\n",
    "    \n",
    "    x = layers.Add()([x, shortcut])\n",
    "    return x\n",
    "\n",
    "def build_pyramidnet18(num_classes, alpha=48, input_shape=(224, 224, 3)):\n",
    "    \"\"\"Build PyramidNet-18\"\"\"\n",
    "    num_blocks = 8\n",
    "    add_channels_per_block = alpha / num_blocks\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    start_filters = 16\n",
    "    x = layers.Conv2D(start_filters, 7, strides=2, padding='same', use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "    \n",
    "    current_filters = start_filters\n",
    "    \n",
    "    # 4 stages, 2 blocks each\n",
    "    for stage in range(4):\n",
    "        for block in range(2):\n",
    "            block_idx = stage * 2 + block\n",
    "            out_filters = int(start_filters + add_channels_per_block * (block_idx + 1))\n",
    "            stride = 2 if (block == 0 and stage > 0) else 1\n",
    "            x = pyramidnet_basicblock(x, int(current_filters), out_filters, stride=stride)\n",
    "            current_filters = out_filters\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='PyramidNet18')\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# LOAD PRE-TRAINED MODEL\n",
    "# ============================================\n",
    "print(\"\\nğŸ”§ Loading pre-trained model...\")\n",
    "\n",
    "pretrained_model = keras.models.load_model(PRETRAIN_MODEL_PATH)\n",
    "print(\"âœ“ Pre-trained model loaded\")\n",
    "\n",
    "# Build new model with correct number of classes\n",
    "print(f\"ğŸ”§ Building model for {num_classes} classes...\")\n",
    "model = build_pyramidnet18(num_classes=num_classes, alpha=48)\n",
    "\n",
    "# Transfer weights (except final layer)\n",
    "print(\"ğŸ“¦ Transferring pre-trained weights...\")\n",
    "for pretrained_layer, new_layer in zip(pretrained_model.layers[:-1], model.layers[:-1]):\n",
    "    if len(pretrained_layer.get_weights()) > 0:\n",
    "        new_layer.set_weights(pretrained_layer.get_weights())\n",
    "\n",
    "print(\"âœ“ Pre-trained weights transferred\")\n",
    "\n",
    "# ============================================\n",
    "# EXPERIMENT 1: UNFREEZE ALL LAYERS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”“ UNFREEZING ALL LAYERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_layers = len(model.layers)\n",
    "trainable_before = sum([1 for layer in model.layers if layer.trainable])\n",
    "\n",
    "# UNFREEZE ALL!\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "trainable_after = sum([1 for layer in model.layers if layer.trainable])\n",
    "\n",
    "print(f\"\\nâœ“ Layer status:\")\n",
    "print(f\"  Total layers: {total_layers}\")\n",
    "print(f\"  Trainable before: {trainable_before}\")\n",
    "print(f\"  Trainable after: {trainable_after}\")\n",
    "print(f\"  Frozen: 0 layers (0%)\")\n",
    "print(f\"  Training: {trainable_after} layers (100%)\")\n",
    "\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "print(f\"\\nâœ“ Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# ============================================\n",
    "# COMPILE MODEL\n",
    "# ============================================\n",
    "print(\"\\nâš™ï¸  Compiling model...\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"âœ“ Loss: Categorical Crossentropy\")\n",
    "\n",
    "# ============================================\n",
    "# CALLBACKS\n",
    "# ============================================\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        EXP1_MODEL_PATH,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# ============================================\n",
    "# TRAIN\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸš€ STARTING EXPERIMENT 1 TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"Max epochs: {EPOCHS}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ“ Training complete in {training_time/60:.1f} minutes\")\n",
    "\n",
    "# ============================================\n",
    "# EVALUATE\n",
    "# ============================================\n",
    "print(\"\\nğŸ“Š Evaluating...\")\n",
    "\n",
    "val_loss, val_accuracy = model.evaluate(val_generator, verbose=0)\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "epochs_trained = len(history.history['accuracy'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š EXPERIMENT 1 RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "print(f\"  Training Accuracy:   {final_train_acc*100:.2f}%\")\n",
    "print(f\"  Validation Accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"  Best Val Accuracy:   {best_val_acc*100:.2f}%\")\n",
    "print(f\"  Validation Loss:     {val_loss:.4f}\")\n",
    "print(f\"  Epochs Trained:      {epochs_trained}/{EPOCHS}\")\n",
    "print(f\"  Training Time:       {training_time/60:.1f} minutes\")\n",
    "\n",
    "# ============================================\n",
    "# COMPARISON\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“ˆ COMPARISON: Baseline vs Experiment 1\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "baseline_val_acc = 55.36\n",
    "improvement = val_accuracy * 100 - baseline_val_acc\n",
    "\n",
    "print(f\"\\nBaseline (60% frozen):\")\n",
    "print(f\"  Validation Accuracy: {baseline_val_acc:.2f}%\")\n",
    "\n",
    "print(f\"\\nExperiment 1 (0% frozen):\")\n",
    "print(f\"  Validation Accuracy: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nImprovement: {improvement:+.2f} percentage points\")\n",
    "\n",
    "if improvement > 15:\n",
    "    print(\"âœ… HYPOTHESIS CONFIRMED: Unfreezing improved accuracy significantly!\")\n",
    "elif improvement > 5:\n",
    "    print(\"âœ“ HYPOTHESIS PARTIALLY CONFIRMED: Unfreezing helped moderately\")\n",
    "elif improvement > 0:\n",
    "    print(\"âš ï¸  HYPOTHESIS WEAK: Unfreezing helped slightly\")\n",
    "else:\n",
    "    print(\"âŒ HYPOTHESIS REJECTED: Unfreezing did not help\")\n",
    "\n",
    "# ============================================\n",
    "# SAVE RESULTS\n",
    "# ============================================\n",
    "print(\"\\nğŸ’¾ Saving results...\")\n",
    "\n",
    "# Save history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(EXP1_RESULTS_PATH, 'training_history.csv'), index=False)\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'experiment': 'Experiment 1: Unfreeze All Layers',\n",
    "    'baseline_val_acc': baseline_val_acc,\n",
    "    'exp1_val_acc': val_accuracy * 100,\n",
    "    'improvement': improvement,\n",
    "    'frozen_layers': 0,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'epochs': EPOCHS,\n",
    "    'epochs_trained': epochs_trained,\n",
    "    'training_time_min': training_time/60,\n",
    "    'final_train_acc': final_train_acc * 100,\n",
    "    'best_val_acc': best_val_acc * 100\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df.to_csv(os.path.join(EXP1_RESULTS_PATH, 'experiment_summary.csv'), index=False)\n",
    "\n",
    "# Plot curves\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train', linewidth=2, color='#4ECDC4')\n",
    "plt.plot(history.history['val_accuracy'], label='Val', linewidth=2, color='#FF6B6B')\n",
    "plt.axhline(y=baseline_val_acc/100, color='gray', linestyle='--', label='Baseline (55.36%)')\n",
    "plt.title('Experiment 1: Accuracy (All Layers Trainable)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train', linewidth=2, color='#4ECDC4')\n",
    "plt.plot(history.history['val_loss'], label='Val', linewidth=2, color='#FF6B6B')\n",
    "plt.title('Experiment 1: Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(EXP1_RESULTS_PATH, 'training_curves.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"âœ“ Results saved to: {EXP1_RESULTS_PATH}/\")\n",
    "print(f\"  - training_history.csv\")\n",
    "print(f\"  - experiment_summary.csv\")\n",
    "print(f\"  - training_curves.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… EXPERIMENT 1 COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ“ Next Steps:\")\n",
    "if improvement > 15:\n",
    "    print(\"  âœ“ Unfreezing worked! Can try other improvements:\")\n",
    "    print(\"    - Experiment 2: Increase learning rate\")\n",
    "    print(\"    - Experiment 3: Train for more epochs\")\n",
    "elif improvement < 5:\n",
    "    print(\"  âš ï¸  Unfreezing didn't help much. Try:\")\n",
    "    print(\"    - Experiment 2: Increase learning rate significantly\")\n",
    "    print(\"    - Check if data quality is the issue\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b941c004-3707-49e3-b388-2dc427d939e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab88712c-ad75-4264-9be9-f6857250ea5d",
   "metadata": {},
   "source": [
    "# EXPERIMENT 2: Increase Learning Rate 10x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b105ab6-8c49-4a72-8149-35eb7c5d80c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "        EXPERIMENT 2: INCREASE LEARNING RATE 10x\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š BASELINE:\n",
      "  Frozen layers: 60%\n",
      "  Learning rate: 0.0001\n",
      "  Result: 55.36% val accuracy\n",
      "\n",
      "ğŸ“Š EXPERIMENT 1:\n",
      "  Frozen layers: 0%\n",
      "  Learning rate: 0.0001\n",
      "  Result: 58.93% val accuracy (+3.57%)\n",
      "  Issue: Training acc stuck at 53%, loss still decreasing\n",
      "\n",
      "ğŸ§ª EXPERIMENT 2:\n",
      "  Frozen layers: 0% (unchanged)\n",
      "  Learning rate: 0.001 (10x INCREASE!)\n",
      "  Max epochs: 50 (unchanged)\n",
      "\n",
      "ğŸ’¡ Hypothesis: LR too low prevented convergence\n",
      "   Expected: Model will converge faster â†’ 70-85% accuracy\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ Loading your curated data...\n",
      "Found 519 images belonging to 8 classes.\n",
      "Found 168 images belonging to 8 classes.\n",
      "\n",
      "âœ“ Data loaded:\n",
      "  Training:   519 images\n",
      "  Validation: 168 images\n",
      "  Classes:    8\n",
      "\n",
      "ğŸ”§ Loading pre-trained model...\n",
      "ğŸ“¦ Transferring pre-trained weights...\n",
      "âœ“ Pre-trained weights transferred\n",
      "\n",
      "ğŸ”“ Setting all layers trainable (from Exp 1)...\n",
      "âœ“ Total layers: 74\n",
      "âœ“ Trainable: 74 (100%)\n",
      "âœ“ Frozen: 0 (0%)\n",
      "\n",
      "======================================================================\n",
      "âš™ï¸  COMPILING WITH 10x HIGHER LEARNING RATE\n",
      "======================================================================\n",
      "\n",
      "âœ“ Optimizer: Adam\n",
      "âœ“ Learning Rate: 0.001 (was 0.0001)\n",
      "âœ“ Increase: 10x\n",
      "âœ“ Loss: Categorical Crossentropy\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ STARTING EXPERIMENT 2 TRAINING\n",
      "======================================================================\n",
      "Started at: 05:35:04\n",
      "Learning rate: 0.001 (10x previous experiments)\n",
      "Max epochs: 50\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.1982 - loss: 2.9600"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.23214, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 330ms/step - accuracy: 0.1996 - loss: 2.9481 - val_accuracy: 0.2321 - val_loss: 17.8231 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.3532 - loss: 2.1717\n",
      "Epoch 2: val_accuracy improved from 0.23214 to 0.29762, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.3544 - loss: 2.1621 - val_accuracy: 0.2976 - val_loss: 13.8605 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.4380 - loss: 1.8233\n",
      "Epoch 3: val_accuracy improved from 0.29762 to 0.32738, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 284ms/step - accuracy: 0.4389 - loss: 1.8207 - val_accuracy: 0.3274 - val_loss: 6.9677 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.5152 - loss: 1.5223\n",
      "Epoch 4: val_accuracy improved from 0.32738 to 0.37500, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.5137 - loss: 1.5246 - val_accuracy: 0.3750 - val_loss: 6.5779 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.4889 - loss: 1.6199\n",
      "Epoch 5: val_accuracy did not improve from 0.37500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 305ms/step - accuracy: 0.4886 - loss: 1.6130 - val_accuracy: 0.3750 - val_loss: 6.3791 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.5682 - loss: 1.2799\n",
      "Epoch 6: val_accuracy improved from 0.37500 to 0.49405, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 291ms/step - accuracy: 0.5662 - loss: 1.2849 - val_accuracy: 0.4940 - val_loss: 2.9357 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.4569 - loss: 1.4811\n",
      "Epoch 7: val_accuracy improved from 0.49405 to 0.50000, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 286ms/step - accuracy: 0.4570 - loss: 1.4816 - val_accuracy: 0.5000 - val_loss: 2.2269 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.5109 - loss: 1.3301\n",
      "Epoch 8: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 288ms/step - accuracy: 0.5113 - loss: 1.3292 - val_accuracy: 0.4821 - val_loss: 2.8058 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.5403 - loss: 1.2429\n",
      "Epoch 9: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.5401 - loss: 1.2435 - val_accuracy: 0.4643 - val_loss: 1.5855 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.5145 - loss: 1.2414\n",
      "Epoch 10: val_accuracy improved from 0.50000 to 0.51190, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.5150 - loss: 1.2430 - val_accuracy: 0.5119 - val_loss: 1.4023 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - accuracy: 0.5611 - loss: 1.2014\n",
      "Epoch 11: val_accuracy improved from 0.51190 to 0.58929, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 301ms/step - accuracy: 0.5612 - loss: 1.2044 - val_accuracy: 0.5893 - val_loss: 1.1930 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.5591 - loss: 1.2170\n",
      "Epoch 12: val_accuracy did not improve from 0.58929\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.5586 - loss: 1.2160 - val_accuracy: 0.4821 - val_loss: 1.9770 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.5687 - loss: 1.1105\n",
      "Epoch 13: val_accuracy did not improve from 0.58929\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 287ms/step - accuracy: 0.5678 - loss: 1.1155 - val_accuracy: 0.5298 - val_loss: 1.3520 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.6018 - loss: 1.1526\n",
      "Epoch 14: val_accuracy did not improve from 0.58929\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 280ms/step - accuracy: 0.6002 - loss: 1.1562 - val_accuracy: 0.5238 - val_loss: 2.0689 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.6010 - loss: 1.0390\n",
      "Epoch 15: val_accuracy did not improve from 0.58929\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 281ms/step - accuracy: 0.5994 - loss: 1.0458 - val_accuracy: 0.5714 - val_loss: 1.3196 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.5939 - loss: 1.0972\n",
      "Epoch 16: val_accuracy improved from 0.58929 to 0.60714, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 287ms/step - accuracy: 0.5946 - loss: 1.0928 - val_accuracy: 0.6071 - val_loss: 1.1441 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.6092 - loss: 1.0439\n",
      "Epoch 17: val_accuracy improved from 0.60714 to 0.61905, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 283ms/step - accuracy: 0.6096 - loss: 1.0438 - val_accuracy: 0.6190 - val_loss: 1.1105 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.6100 - loss: 1.1175\n",
      "Epoch 18: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 281ms/step - accuracy: 0.6085 - loss: 1.1202 - val_accuracy: 0.5298 - val_loss: 1.3248 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.5704 - loss: 1.1086\n",
      "Epoch 19: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 289ms/step - accuracy: 0.5724 - loss: 1.1047 - val_accuracy: 0.5655 - val_loss: 1.1125 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.5876 - loss: 1.0703\n",
      "Epoch 20: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 282ms/step - accuracy: 0.5886 - loss: 1.0699 - val_accuracy: 0.5893 - val_loss: 1.0712 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.6172 - loss: 0.9887\n",
      "Epoch 21: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 284ms/step - accuracy: 0.6170 - loss: 0.9925 - val_accuracy: 0.4881 - val_loss: 1.5543 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.5612 - loss: 1.1162\n",
      "Epoch 22: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 286ms/step - accuracy: 0.5618 - loss: 1.1140 - val_accuracy: 0.5714 - val_loss: 1.1595 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.6189 - loss: 0.9678\n",
      "Epoch 23: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 286ms/step - accuracy: 0.6195 - loss: 0.9677 - val_accuracy: 0.5774 - val_loss: 1.4354 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.5734 - loss: 1.1527\n",
      "Epoch 24: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 280ms/step - accuracy: 0.5744 - loss: 1.1504 - val_accuracy: 0.5179 - val_loss: 1.3199 - learning_rate: 0.0010\n",
      "Epoch 25/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.6330 - loss: 0.9652\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.6351 - loss: 0.9617 - val_accuracy: 0.5774 - val_loss: 1.2802 - learning_rate: 0.0010\n",
      "Epoch 26/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.6388 - loss: 0.9876\n",
      "Epoch 26: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 273ms/step - accuracy: 0.6368 - loss: 0.9931 - val_accuracy: 0.5179 - val_loss: 1.4391 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.6677 - loss: 0.8964\n",
      "Epoch 27: val_accuracy improved from 0.61905 to 0.66667, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 285ms/step - accuracy: 0.6686 - loss: 0.8944 - val_accuracy: 0.6667 - val_loss: 1.0490 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.6881 - loss: 0.8623\n",
      "Epoch 28: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 281ms/step - accuracy: 0.6878 - loss: 0.8613 - val_accuracy: 0.6310 - val_loss: 1.0429 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.6822 - loss: 0.7521\n",
      "Epoch 29: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 278ms/step - accuracy: 0.6825 - loss: 0.7549 - val_accuracy: 0.6190 - val_loss: 0.9750 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.6557 - loss: 0.8202\n",
      "Epoch 30: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 277ms/step - accuracy: 0.6555 - loss: 0.8209 - val_accuracy: 0.6548 - val_loss: 1.0147 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.6972 - loss: 0.7786\n",
      "Epoch 31: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 283ms/step - accuracy: 0.6974 - loss: 0.7791 - val_accuracy: 0.5952 - val_loss: 1.1917 - learning_rate: 5.0000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.7099 - loss: 0.7870\n",
      "Epoch 32: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 276ms/step - accuracy: 0.7090 - loss: 0.7876 - val_accuracy: 0.5952 - val_loss: 1.0989 - learning_rate: 5.0000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.7179 - loss: 0.7819\n",
      "Epoch 33: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 285ms/step - accuracy: 0.7158 - loss: 0.7847 - val_accuracy: 0.5952 - val_loss: 1.1281 - learning_rate: 5.0000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.6827 - loss: 0.7656\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 278ms/step - accuracy: 0.6818 - loss: 0.7698 - val_accuracy: 0.5655 - val_loss: 1.1188 - learning_rate: 5.0000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.7352 - loss: 0.7030\n",
      "Epoch 35: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 285ms/step - accuracy: 0.7330 - loss: 0.7071 - val_accuracy: 0.6250 - val_loss: 0.9923 - learning_rate: 2.5000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.6974 - loss: 0.7888\n",
      "Epoch 36: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 281ms/step - accuracy: 0.6984 - loss: 0.7868 - val_accuracy: 0.6429 - val_loss: 0.9591 - learning_rate: 2.5000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.7514 - loss: 0.7431\n",
      "Epoch 37: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 281ms/step - accuracy: 0.7510 - loss: 0.7439 - val_accuracy: 0.6369 - val_loss: 0.9545 - learning_rate: 2.5000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.7326 - loss: 0.7147\n",
      "Epoch 38: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 289ms/step - accuracy: 0.7320 - loss: 0.7158 - val_accuracy: 0.6071 - val_loss: 1.0343 - learning_rate: 2.5000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.7461 - loss: 0.6712\n",
      "Epoch 39: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.7470 - loss: 0.6704 - val_accuracy: 0.6250 - val_loss: 1.0542 - learning_rate: 2.5000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.7113 - loss: 0.7391\n",
      "Epoch 40: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 277ms/step - accuracy: 0.7110 - loss: 0.7405 - val_accuracy: 0.6488 - val_loss: 0.9405 - learning_rate: 2.5000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.7442 - loss: 0.7020\n",
      "Epoch 41: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 290ms/step - accuracy: 0.7444 - loss: 0.7026 - val_accuracy: 0.6607 - val_loss: 0.9499 - learning_rate: 2.5000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.7461 - loss: 0.6819\n",
      "Epoch 42: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 274ms/step - accuracy: 0.7446 - loss: 0.6845 - val_accuracy: 0.6369 - val_loss: 0.9860 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.7049 - loss: 0.7656\n",
      "Epoch 43: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 279ms/step - accuracy: 0.7065 - loss: 0.7616 - val_accuracy: 0.6310 - val_loss: 0.9974 - learning_rate: 2.5000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.7176 - loss: 0.7545\n",
      "Epoch 44: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 279ms/step - accuracy: 0.7190 - loss: 0.7528 - val_accuracy: 0.6548 - val_loss: 1.0164 - learning_rate: 2.5000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.7935 - loss: 0.6599\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 276ms/step - accuracy: 0.7928 - loss: 0.6585 - val_accuracy: 0.6429 - val_loss: 0.9706 - learning_rate: 2.5000e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.7448 - loss: 0.7018\n",
      "Epoch 46: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 276ms/step - accuracy: 0.7442 - loss: 0.7014 - val_accuracy: 0.6667 - val_loss: 0.9908 - learning_rate: 1.2500e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.7678 - loss: 0.6103\n",
      "Epoch 47: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 284ms/step - accuracy: 0.7666 - loss: 0.6132 - val_accuracy: 0.6369 - val_loss: 0.9396 - learning_rate: 1.2500e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 248ms/step - accuracy: 0.7579 - loss: 0.8012\n",
      "Epoch 48: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 304ms/step - accuracy: 0.7581 - loss: 0.7924 - val_accuracy: 0.6429 - val_loss: 1.0018 - learning_rate: 1.2500e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.6901 - loss: 0.7367\n",
      "Epoch 49: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 274ms/step - accuracy: 0.6925 - loss: 0.7338 - val_accuracy: 0.6071 - val_loss: 1.0965 - learning_rate: 1.2500e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.7799 - loss: 0.6277\n",
      "Epoch 50: val_accuracy did not improve from 0.66667\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 277ms/step - accuracy: 0.7783 - loss: 0.6282 - val_accuracy: 0.6369 - val_loss: 0.9611 - learning_rate: 1.2500e-04\n",
      "Restoring model weights from the end of the best epoch: 47.\n",
      "\n",
      "âœ“ Training complete in 4.2 minutes\n",
      "\n",
      "ğŸ“Š Evaluating...\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š EXPERIMENT 2 RESULTS\n",
      "======================================================================\n",
      "\n",
      "Final Metrics:\n",
      "  Training Accuracy:   75.14%\n",
      "  Validation Accuracy: 66.67%\n",
      "  Best Val Accuracy:   66.67%\n",
      "  Validation Loss:     0.9958\n",
      "  Epochs Trained:      50/50\n",
      "  Training Time:       4.2 minutes\n",
      "\n",
      "======================================================================\n",
      "ğŸ“ˆ FULL EXPERIMENT COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Baseline (60% frozen, LR=0.0001):\n",
      "  Validation Accuracy: 55.36%\n",
      "\n",
      "Experiment 1 (0% frozen, LR=0.0001):\n",
      "  Validation Accuracy: 58.93%\n",
      "  vs Baseline: +3.57pp\n",
      "\n",
      "Experiment 2 (0% frozen, LR=0.001):\n",
      "  Validation Accuracy: 66.67%\n",
      "  vs Baseline: +11.31pp\n",
      "  vs Exp 1: +7.74pp\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ HYPOTHESIS EVALUATION\n",
      "======================================================================\n",
      "âœ“ HYPOTHESIS MOSTLY CONFIRMED: Significant improvement\n",
      "  Learning rate was a major factor, may need further tuning\n",
      "\n",
      "======================================================================\n",
      "ğŸ” TRAINING BEHAVIOR ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "âš ï¸  Model STRUGGLES to fit training data\n",
      "  Training accuracy only 75.14%\n",
      "\n",
      "âœ“ Healthy train/val gap: 8.5pp\n",
      "  Some overfitting but acceptable\n",
      "\n",
      "ğŸ’¾ Saving results...\n",
      "âœ“ Results saved to: /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_results/\n",
      "\n",
      "======================================================================\n",
      "âœ… EXPERIMENT 2 COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ Key Takeaways:\n",
      "  âš ï¸  LR helped but wasn't the main issue\n",
      "  ? Next: Check data quality, try data augmentation changes\n",
      "  ? Or: Domain shift problem (Fruit-360 â†’ real world)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EXPERIMENT 2: Increase Learning Rate 10x\n",
    "\n",
    "Baseline: 55.36% val accuracy (60% frozen, LR=0.0001)\n",
    "Experiment 1: 58.93% val accuracy (0% frozen, LR=0.0001)\n",
    "Experiment 2: ? (0% frozen, LR=0.001)\n",
    "\n",
    "Change: Learning rate 0.0001 â†’ 0.001 (10x increase)\n",
    "Keep same: All layers unfrozen, Max epochs 50\n",
    "\n",
    "Hypothesis: Low LR was preventing convergence. \n",
    "           10x LR will allow model to converge â†’ 70-85% accuracy\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "PROJECT_ROOT = '/Users/vishal/Desktop/ML study/project/CNN type/first'\n",
    "\n",
    "CURATED_DATA = os.path.join(PROJECT_ROOT, 'data set fine tune')\n",
    "PRETRAIN_MODEL_PATH = os.path.join(PROJECT_ROOT, 'pyramidnet18_pretrained.keras')\n",
    "\n",
    "# Experiment 2 outputs\n",
    "EXP2_MODEL_PATH = os.path.join(PROJECT_ROOT, 'experiment2_model.keras')\n",
    "EXP2_RESULTS_PATH = os.path.join(PROJECT_ROOT, 'experiment2_results')\n",
    "\n",
    "os.makedirs(EXP2_RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001  # 10x increase from 0.0001!\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" \"*8 + \"EXPERIMENT 2: INCREASE LEARNING RATE 10x\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ“Š BASELINE:\")\n",
    "print(\"  Frozen layers: 60%\")\n",
    "print(\"  Learning rate: 0.0001\")\n",
    "print(\"  Result: 55.36% val accuracy\")\n",
    "\n",
    "print(\"\\nğŸ“Š EXPERIMENT 1:\")\n",
    "print(\"  Frozen layers: 0%\")\n",
    "print(\"  Learning rate: 0.0001\")\n",
    "print(\"  Result: 58.93% val accuracy (+3.57%)\")\n",
    "print(\"  Issue: Training acc stuck at 53%, loss still decreasing\")\n",
    "\n",
    "print(\"\\nğŸ§ª EXPERIMENT 2:\")\n",
    "print(\"  Frozen layers: 0% (unchanged)\")\n",
    "print(\"  Learning rate: 0.001 (10x INCREASE!)\")\n",
    "print(\"  Max epochs: 50 (unchanged)\")\n",
    "print(\"\\nğŸ’¡ Hypothesis: LR too low prevented convergence\")\n",
    "print(\"   Expected: Model will converge faster â†’ 70-85% accuracy\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# LOAD DATA\n",
    "# ============================================\n",
    "print(\"\\nğŸ“ Loading your curated data...\")\n",
    "\n",
    "finetune_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.25\n",
    ")\n",
    "\n",
    "train_generator = finetune_datagen.flow_from_directory(\n",
    "    CURATED_DATA,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = finetune_datagen.flow_from_directory(\n",
    "    CURATED_DATA,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded:\")\n",
    "print(f\"  Training:   {train_generator.samples} images\")\n",
    "print(f\"  Validation: {val_generator.samples} images\")\n",
    "print(f\"  Classes:    {num_classes}\")\n",
    "\n",
    "# ============================================\n",
    "# PYRAMIDNET ARCHITECTURE\n",
    "# ============================================\n",
    "\n",
    "def pyramidnet_basicblock(x, in_filters, out_filters, stride=1):\n",
    "    \"\"\"PyramidNet Basic Block\"\"\"\n",
    "    shortcut = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(out_filters, 3, strides=stride, padding='same', use_bias=False)(x)\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(out_filters, 3, strides=1, padding='same', use_bias=False)(x)\n",
    "    \n",
    "    if in_filters != out_filters or stride != 1:\n",
    "        shortcut = layers.Conv2D(out_filters, 1, strides=stride, use_bias=False)(shortcut)\n",
    "    \n",
    "    x = layers.Add()([x, shortcut])\n",
    "    return x\n",
    "\n",
    "def build_pyramidnet18(num_classes, alpha=48, input_shape=(224, 224, 3)):\n",
    "    \"\"\"Build PyramidNet-18\"\"\"\n",
    "    num_blocks = 8\n",
    "    add_channels_per_block = alpha / num_blocks\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    start_filters = 16\n",
    "    x = layers.Conv2D(start_filters, 7, strides=2, padding='same', use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "    \n",
    "    current_filters = start_filters\n",
    "    \n",
    "    for stage in range(4):\n",
    "        for block in range(2):\n",
    "            block_idx = stage * 2 + block\n",
    "            out_filters = int(start_filters + add_channels_per_block * (block_idx + 1))\n",
    "            stride = 2 if (block == 0 and stage > 0) else 1\n",
    "            x = pyramidnet_basicblock(x, int(current_filters), out_filters, stride=stride)\n",
    "            current_filters = out_filters\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='PyramidNet18')\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# LOAD PRE-TRAINED MODEL\n",
    "# ============================================\n",
    "print(\"\\nğŸ”§ Loading pre-trained model...\")\n",
    "\n",
    "pretrained_model = keras.models.load_model(PRETRAIN_MODEL_PATH)\n",
    "model = build_pyramidnet18(num_classes=num_classes, alpha=48)\n",
    "\n",
    "print(\"ğŸ“¦ Transferring pre-trained weights...\")\n",
    "for pretrained_layer, new_layer in zip(pretrained_model.layers[:-1], model.layers[:-1]):\n",
    "    if len(pretrained_layer.get_weights()) > 0:\n",
    "        new_layer.set_weights(pretrained_layer.get_weights())\n",
    "\n",
    "print(\"âœ“ Pre-trained weights transferred\")\n",
    "\n",
    "# ============================================\n",
    "# UNFREEZE ALL LAYERS (from Exp 1)\n",
    "# ============================================\n",
    "print(\"\\nğŸ”“ Setting all layers trainable (from Exp 1)...\")\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "total_layers = len(model.layers)\n",
    "trainable_layers = sum([1 for layer in model.layers if layer.trainable])\n",
    "\n",
    "print(f\"âœ“ Total layers: {total_layers}\")\n",
    "print(f\"âœ“ Trainable: {trainable_layers} (100%)\")\n",
    "print(f\"âœ“ Frozen: 0 (0%)\")\n",
    "\n",
    "# ============================================\n",
    "# COMPILE WITH HIGHER LEARNING RATE\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âš™ï¸  COMPILING WITH 10x HIGHER LEARNING RATE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Optimizer: Adam\")\n",
    "print(f\"âœ“ Learning Rate: {LEARNING_RATE} (was 0.0001)\")\n",
    "print(f\"âœ“ Increase: 10x\")\n",
    "print(f\"âœ“ Loss: Categorical Crossentropy\")\n",
    "\n",
    "# ============================================\n",
    "# CALLBACKS\n",
    "# ============================================\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        EXP2_MODEL_PATH,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# ============================================\n",
    "# TRAIN\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸš€ STARTING EXPERIMENT 2 TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE} (10x previous experiments)\")\n",
    "print(f\"Max epochs: {EPOCHS}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ“ Training complete in {training_time/60:.1f} minutes\")\n",
    "\n",
    "# ============================================\n",
    "# EVALUATE\n",
    "# ============================================\n",
    "print(\"\\nğŸ“Š Evaluating...\")\n",
    "\n",
    "val_loss, val_accuracy = model.evaluate(val_generator, verbose=0)\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "epochs_trained = len(history.history['accuracy'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š EXPERIMENT 2 RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "print(f\"  Training Accuracy:   {final_train_acc*100:.2f}%\")\n",
    "print(f\"  Validation Accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"  Best Val Accuracy:   {best_val_acc*100:.2f}%\")\n",
    "print(f\"  Validation Loss:     {val_loss:.4f}\")\n",
    "print(f\"  Epochs Trained:      {epochs_trained}/{EPOCHS}\")\n",
    "print(f\"  Training Time:       {training_time/60:.1f} minutes\")\n",
    "\n",
    "# ============================================\n",
    "# COMPARISON WITH ALL EXPERIMENTS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“ˆ FULL EXPERIMENT COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "baseline_val = 55.36\n",
    "exp1_val = 58.93\n",
    "exp2_val = val_accuracy * 100\n",
    "\n",
    "improvement_from_baseline = exp2_val - baseline_val\n",
    "improvement_from_exp1 = exp2_val - exp1_val\n",
    "\n",
    "print(f\"\\nBaseline (60% frozen, LR=0.0001):\")\n",
    "print(f\"  Validation Accuracy: {baseline_val:.2f}%\")\n",
    "\n",
    "print(f\"\\nExperiment 1 (0% frozen, LR=0.0001):\")\n",
    "print(f\"  Validation Accuracy: {exp1_val:.2f}%\")\n",
    "print(f\"  vs Baseline: {exp1_val - baseline_val:+.2f}pp\")\n",
    "\n",
    "print(f\"\\nExperiment 2 (0% frozen, LR=0.001):\")\n",
    "print(f\"  Validation Accuracy: {exp2_val:.2f}%\")\n",
    "print(f\"  vs Baseline: {improvement_from_baseline:+.2f}pp\")\n",
    "print(f\"  vs Exp 1: {improvement_from_exp1:+.2f}pp\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ HYPOTHESIS EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if exp2_val >= 70:\n",
    "    print(\"âœ… HYPOTHESIS CONFIRMED: 10x LR achieved target (70-85%)!\")\n",
    "    print(\"   Learning rate was indeed the bottleneck.\")\n",
    "elif exp2_val >= 65:\n",
    "    print(\"âœ“ HYPOTHESIS MOSTLY CONFIRMED: Significant improvement\")\n",
    "    print(\"  Learning rate was a major factor, may need further tuning\")\n",
    "elif improvement_from_exp1 >= 5:\n",
    "    print(\"âš ï¸  HYPOTHESIS PARTIALLY CONFIRMED: LR helped moderately\")\n",
    "    print(\"   Other factors may also be limiting performance\")\n",
    "else:\n",
    "    print(\"âŒ HYPOTHESIS REJECTED: LR increase didn't help significantly\")\n",
    "    print(\"   The problem lies elsewhere (data quality, domain shift?)\")\n",
    "\n",
    "# ============================================\n",
    "# TRAINING BEHAVIOR ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ” TRAINING BEHAVIOR ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if model fit training data\n",
    "if final_train_acc >= 0.8:\n",
    "    print(\"\\nâœ“ Model CAN fit training data (80%+ train acc)\")\n",
    "    print(\"  This is healthy - model has sufficient capacity\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Model STRUGGLES to fit training data\")\n",
    "    print(f\"  Training accuracy only {final_train_acc*100:.2f}%\")\n",
    "\n",
    "# Check overfitting\n",
    "gap = final_train_acc - val_accuracy\n",
    "if gap > 0.15:\n",
    "    print(f\"\\nâš ï¸  Overfitting detected: {gap*100:.1f}pp gap\")\n",
    "    print(\"  Model memorizing training data, not generalizing\")\n",
    "elif gap > 0:\n",
    "    print(f\"\\nâœ“ Healthy train/val gap: {gap*100:.1f}pp\")\n",
    "    print(\"  Some overfitting but acceptable\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Unusual: Val acc > Train acc\")\n",
    "    print(\"  May indicate training issues or very small val set\")\n",
    "\n",
    "# ============================================\n",
    "# SAVE RESULTS\n",
    "# ============================================\n",
    "print(\"\\nğŸ’¾ Saving results...\")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(EXP2_RESULTS_PATH, 'training_history.csv'), index=False)\n",
    "\n",
    "summary = {\n",
    "    'experiment': 'Experiment 2: Increase Learning Rate 10x',\n",
    "    'baseline_val_acc': baseline_val,\n",
    "    'exp1_val_acc': exp1_val,\n",
    "    'exp2_val_acc': exp2_val,\n",
    "    'improvement_from_baseline': improvement_from_baseline,\n",
    "    'improvement_from_exp1': improvement_from_exp1,\n",
    "    'frozen_layers': 0,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'epochs': EPOCHS,\n",
    "    'epochs_trained': epochs_trained,\n",
    "    'training_time_min': training_time/60,\n",
    "    'final_train_acc': final_train_acc * 100,\n",
    "    'best_val_acc': best_val_acc * 100,\n",
    "    'train_val_gap': gap * 100\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df.to_csv(os.path.join(EXP2_RESULTS_PATH, 'experiment_summary.csv'), index=False)\n",
    "\n",
    "# Comparison plot\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "experiments = ['Baseline\\n(60% frozen\\nLR=0.0001)', \n",
    "               'Exp 1\\n(0% frozen\\nLR=0.0001)', \n",
    "               'Exp 2\\n(0% frozen\\nLR=0.001)']\n",
    "accuracies = [baseline_val, exp1_val, exp2_val]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "bars = plt.bar(experiments, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "plt.ylabel('Validation Accuracy (%)', fontweight='bold')\n",
    "plt.title('Experiment Comparison', fontsize=14, fontweight='bold')\n",
    "plt.ylim([0, 100])\n",
    "plt.axhline(y=70, color='green', linestyle='--', label='Target (70%)', linewidth=2)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.legend()\n",
    "for i, (bar, acc) in enumerate(zip(bars, accuracies)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, acc + 2, f'{acc:.1f}%', \n",
    "             ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['accuracy'], label='Train', linewidth=2.5, color='#4ECDC4')\n",
    "plt.plot(history.history['val_accuracy'], label='Val', linewidth=2.5, color='#FF6B6B')\n",
    "plt.title('Experiment 2: Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['loss'], label='Train', linewidth=2.5, color='#4ECDC4')\n",
    "plt.plot(history.history['val_loss'], label='Val', linewidth=2.5, color='#FF6B6B')\n",
    "plt.title('Experiment 2: Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(EXP2_RESULTS_PATH, 'experiment_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"âœ“ Results saved to: {EXP2_RESULTS_PATH}/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… EXPERIMENT 2 COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ“ Key Takeaways:\")\n",
    "if exp2_val >= 70:\n",
    "    print(\"  âœ“ Learning rate WAS the bottleneck!\")\n",
    "    print(\"  âœ“ 10x increase solved the problem\")\n",
    "    print(\"  âœ“ Next: Could try even more epochs or different LR\")\n",
    "elif improvement_from_exp1 >= 10:\n",
    "    print(\"  âœ“ Learning rate was a MAJOR factor\")\n",
    "    print(\"  âœ“ Significant improvement achieved\")\n",
    "    print(\"  âœ“ Next: Fine-tune LR further or try more epochs\")\n",
    "else:\n",
    "    print(\"  âš ï¸  LR helped but wasn't the main issue\")\n",
    "    print(\"  ? Next: Check data quality, try data augmentation changes\")\n",
    "    print(\"  ? Or: Domain shift problem (Fruit-360 â†’ real world)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26092ffe-39a7-4549-a9fb-e85c2db16e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe91f9-55b7-4cdb-9403-38d6bcafac47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3adf5ab-f0f5-46f7-81b1-a4be2fec6236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "        EXPERIMENT 2A: MODERATE LR + MORE EPOCHS\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š BASELINE:\n",
      "  Frozen layers: 60%\n",
      "  Learning rate: 0.0001\n",
      "  Epochs: 50\n",
      "  Result: 55.36% val accuracy\n",
      "  Issue: Training acc stuck at 50%, loss still decreasing\n",
      "\n",
      "ğŸ“Š EXPERIMENT 1:\n",
      "  Frozen layers: 0%\n",
      "  Learning rate: 0.0001\n",
      "  Epochs: 50\n",
      "  Result: 58.93% val accuracy (+3.57pp)\n",
      "  Issue: Training acc still only 53%, LR too low\n",
      "\n",
      "ğŸ§ª EXPERIMENT 2A (Conservative Approach):\n",
      "  Frozen layers: 0% (unchanged)\n",
      "  Learning rate: 0.0005 (5x INCREASE - moderate)\n",
      "  Max epochs: 75 (50% MORE TIME)\n",
      "\n",
      "ğŸ’¡ Strategy:\n",
      "   - Safer than 10x increase (less divergence risk)\n",
      "   - More epochs allows smooth convergence\n",
      "   - Expected: 70-80% validation accuracy\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ Loading your curated data...\n",
      "Found 519 images belonging to 8 classes.\n",
      "Found 168 images belonging to 8 classes.\n",
      "\n",
      "âœ“ Data loaded:\n",
      "  Training:   519 images\n",
      "  Validation: 168 images\n",
      "  Classes:    8\n",
      "\n",
      "ğŸ”§ Loading pre-trained model...\n",
      "ğŸ“¦ Transferring pre-trained weights...\n",
      "âœ“ Pre-trained weights transferred\n",
      "\n",
      "ğŸ”“ Setting all layers trainable...\n",
      "âœ“ Total layers: 74\n",
      "âœ“ Trainable: 74 (100%)\n",
      "âœ“ Frozen: 0 (0%)\n",
      "\n",
      "======================================================================\n",
      "âš™ï¸  COMPILING WITH MODERATE (5x) LEARNING RATE\n",
      "======================================================================\n",
      "\n",
      "âœ“ Optimizer: Adam\n",
      "âœ“ Learning Rate: 0.0005 (was 0.0001)\n",
      "âœ“ Increase: 5x (conservative)\n",
      "âœ“ Max Epochs: 75 (+50% more time)\n",
      "âœ“ Strategy: Smooth convergence without divergence risk\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ STARTING EXPERIMENT 2A TRAINING\n",
      "======================================================================\n",
      "Started at: 06:06:09\n",
      "Learning rate: 0.0005 (5x previous experiments)\n",
      "Max epochs: 75\n",
      "Strategy: Conservative LR increase with more time to converge\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.1144 - loss: 3.5687"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.20238, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2a_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 323ms/step - accuracy: 0.1168 - loss: 3.5486 - val_accuracy: 0.2024 - val_loss: 2.9151 - learning_rate: 5.0000e-04\n",
      "Epoch 2/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.2730 - loss: 2.4461\n",
      "Epoch 2: val_accuracy improved from 0.20238 to 0.23214, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2a_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 282ms/step - accuracy: 0.2735 - loss: 2.4445 - val_accuracy: 0.2321 - val_loss: 3.3718 - learning_rate: 5.0000e-04\n",
      "Epoch 3/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.3201 - loss: 2.0588\n",
      "Epoch 3: val_accuracy improved from 0.23214 to 0.30952, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2a_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 285ms/step - accuracy: 0.3221 - loss: 2.0565 - val_accuracy: 0.3095 - val_loss: 4.4263 - learning_rate: 5.0000e-04\n",
      "Epoch 4/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.4012 - loss: 1.9528\n",
      "Epoch 4: val_accuracy did not improve from 0.30952\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 279ms/step - accuracy: 0.4021 - loss: 1.9472 - val_accuracy: 0.3095 - val_loss: 4.6285 - learning_rate: 5.0000e-04\n",
      "Epoch 5/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.4265 - loss: 1.7717\n",
      "Epoch 5: val_accuracy improved from 0.30952 to 0.54167, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2a_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 282ms/step - accuracy: 0.4265 - loss: 1.7666 - val_accuracy: 0.5417 - val_loss: 1.7967 - learning_rate: 5.0000e-04\n",
      "Epoch 6/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.4770 - loss: 1.7591\n",
      "Epoch 6: val_accuracy improved from 0.54167 to 0.55952, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2a_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 279ms/step - accuracy: 0.4764 - loss: 1.7556 - val_accuracy: 0.5595 - val_loss: 1.8771 - learning_rate: 5.0000e-04\n",
      "Epoch 7/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.4848 - loss: 1.5804\n",
      "Epoch 7: val_accuracy did not improve from 0.55952\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 281ms/step - accuracy: 0.4853 - loss: 1.5747 - val_accuracy: 0.5298 - val_loss: 1.6828 - learning_rate: 5.0000e-04\n",
      "Epoch 8/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.5031 - loss: 1.4268\n",
      "Epoch 8: val_accuracy did not improve from 0.55952\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 283ms/step - accuracy: 0.5030 - loss: 1.4268 - val_accuracy: 0.5060 - val_loss: 1.7192 - learning_rate: 5.0000e-04\n",
      "Epoch 9/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.4890 - loss: 1.4071\n",
      "Epoch 9: val_accuracy did not improve from 0.55952\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 279ms/step - accuracy: 0.4903 - loss: 1.4078 - val_accuracy: 0.5595 - val_loss: 1.2873 - learning_rate: 5.0000e-04\n",
      "Epoch 10/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.4339 - loss: 1.6282\n",
      "Epoch 10: val_accuracy did not improve from 0.55952\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 277ms/step - accuracy: 0.4376 - loss: 1.6199 - val_accuracy: 0.5357 - val_loss: 1.6143 - learning_rate: 5.0000e-04\n",
      "Epoch 11/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.5231 - loss: 1.4784\n",
      "Epoch 11: val_accuracy did not improve from 0.55952\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 273ms/step - accuracy: 0.5238 - loss: 1.4705 - val_accuracy: 0.5298 - val_loss: 1.5652 - learning_rate: 5.0000e-04\n",
      "Epoch 12/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.5362 - loss: 1.2687\n",
      "Epoch 12: val_accuracy improved from 0.55952 to 0.59524, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2a_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 276ms/step - accuracy: 0.5359 - loss: 1.2707 - val_accuracy: 0.5952 - val_loss: 1.3884 - learning_rate: 5.0000e-04\n",
      "Epoch 13/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step - accuracy: 0.5774 - loss: 1.2352\n",
      "Epoch 13: val_accuracy did not improve from 0.59524\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 271ms/step - accuracy: 0.5769 - loss: 1.2352 - val_accuracy: 0.5595 - val_loss: 1.3505 - learning_rate: 5.0000e-04\n",
      "Epoch 14/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.6294 - loss: 1.1280\n",
      "Epoch 14: val_accuracy did not improve from 0.59524\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 277ms/step - accuracy: 0.6269 - loss: 1.1297 - val_accuracy: 0.5595 - val_loss: 1.4405 - learning_rate: 5.0000e-04\n",
      "Epoch 15/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.5967 - loss: 1.1140\n",
      "Epoch 15: val_accuracy did not improve from 0.59524\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 279ms/step - accuracy: 0.5948 - loss: 1.1154 - val_accuracy: 0.5595 - val_loss: 1.2454 - learning_rate: 5.0000e-04\n",
      "Epoch 16/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.5632 - loss: 1.1838\n",
      "Epoch 16: val_accuracy did not improve from 0.59524\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 273ms/step - accuracy: 0.5632 - loss: 1.1854 - val_accuracy: 0.5714 - val_loss: 1.3679 - learning_rate: 5.0000e-04\n",
      "Epoch 17/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.5480 - loss: 1.2914\n",
      "Epoch 17: val_accuracy did not improve from 0.59524\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 280ms/step - accuracy: 0.5501 - loss: 1.2864 - val_accuracy: 0.5893 - val_loss: 1.1160 - learning_rate: 5.0000e-04\n",
      "Epoch 18/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.6322 - loss: 1.0071\n",
      "Epoch 18: val_accuracy improved from 0.59524 to 0.61310, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2a_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 285ms/step - accuracy: 0.6317 - loss: 1.0088 - val_accuracy: 0.6131 - val_loss: 1.1449 - learning_rate: 5.0000e-04\n",
      "Epoch 19/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.5735 - loss: 1.1543\n",
      "Epoch 19: val_accuracy did not improve from 0.61310\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 279ms/step - accuracy: 0.5731 - loss: 1.1568 - val_accuracy: 0.5833 - val_loss: 1.1650 - learning_rate: 5.0000e-04\n",
      "Epoch 20/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.5250 - loss: 1.3100\n",
      "Epoch 20: val_accuracy did not improve from 0.61310\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 284ms/step - accuracy: 0.5268 - loss: 1.3010 - val_accuracy: 0.6071 - val_loss: 1.2021 - learning_rate: 5.0000e-04\n",
      "Epoch 21/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.6048 - loss: 0.9963\n",
      "Epoch 21: val_accuracy did not improve from 0.61310\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 284ms/step - accuracy: 0.6051 - loss: 0.9960 - val_accuracy: 0.5774 - val_loss: 1.3470 - learning_rate: 5.0000e-04\n",
      "Epoch 22/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.6110 - loss: 1.1313\n",
      "Epoch 22: val_accuracy did not improve from 0.61310\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 278ms/step - accuracy: 0.6095 - loss: 1.1358 - val_accuracy: 0.5476 - val_loss: 1.3360 - learning_rate: 5.0000e-04\n",
      "Epoch 23/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.5777 - loss: 1.2384\n",
      "Epoch 23: val_accuracy did not improve from 0.61310\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 278ms/step - accuracy: 0.5778 - loss: 1.2347 - val_accuracy: 0.5298 - val_loss: 1.2659 - learning_rate: 5.0000e-04\n",
      "Epoch 24/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.6027 - loss: 1.0908\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.61310\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 275ms/step - accuracy: 0.6032 - loss: 1.0894 - val_accuracy: 0.5536 - val_loss: 1.2627 - learning_rate: 5.0000e-04\n",
      "Epoch 25/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.6442 - loss: 1.1350\n",
      "Epoch 25: val_accuracy did not improve from 0.61310\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 279ms/step - accuracy: 0.6449 - loss: 1.1277 - val_accuracy: 0.6012 - val_loss: 1.1036 - learning_rate: 2.5000e-04\n",
      "Epoch 26/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.6279 - loss: 0.9441\n",
      "Epoch 26: val_accuracy did not improve from 0.61310\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 278ms/step - accuracy: 0.6285 - loss: 0.9451 - val_accuracy: 0.6131 - val_loss: 1.0512 - learning_rate: 2.5000e-04\n",
      "Epoch 27/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.6411 - loss: 0.9062\n",
      "Epoch 27: val_accuracy did not improve from 0.61310\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 277ms/step - accuracy: 0.6411 - loss: 0.9065 - val_accuracy: 0.5774 - val_loss: 1.1275 - learning_rate: 2.5000e-04\n",
      "Epoch 28/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.6529 - loss: 0.9580\n",
      "Epoch 28: val_accuracy did not improve from 0.61310\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 275ms/step - accuracy: 0.6528 - loss: 0.9573 - val_accuracy: 0.6012 - val_loss: 1.0627 - learning_rate: 2.5000e-04\n",
      "Epoch 29/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.6663 - loss: 0.9299\n",
      "Epoch 29: val_accuracy did not improve from 0.61310\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 283ms/step - accuracy: 0.6667 - loss: 0.9298 - val_accuracy: 0.5952 - val_loss: 1.1155 - learning_rate: 2.5000e-04\n",
      "Epoch 30/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.6561 - loss: 0.9014\n",
      "Epoch 30: val_accuracy improved from 0.61310 to 0.63690, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2a_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 287ms/step - accuracy: 0.6569 - loss: 0.9003 - val_accuracy: 0.6369 - val_loss: 0.9881 - learning_rate: 2.5000e-04\n",
      "Epoch 31/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.6347 - loss: 0.9075\n",
      "Epoch 31: val_accuracy did not improve from 0.63690\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 288ms/step - accuracy: 0.6366 - loss: 0.9061 - val_accuracy: 0.6131 - val_loss: 1.0186 - learning_rate: 2.5000e-04\n",
      "Epoch 32/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.6503 - loss: 0.9403\n",
      "Epoch 32: val_accuracy did not improve from 0.63690\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 300ms/step - accuracy: 0.6510 - loss: 0.9413 - val_accuracy: 0.6131 - val_loss: 1.1027 - learning_rate: 2.5000e-04\n",
      "Epoch 33/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.6357 - loss: 0.9229\n",
      "Epoch 33: val_accuracy did not improve from 0.63690\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 282ms/step - accuracy: 0.6355 - loss: 0.9234 - val_accuracy: 0.6369 - val_loss: 1.0023 - learning_rate: 2.5000e-04\n",
      "Epoch 34/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.7256 - loss: 0.7840\n",
      "Epoch 34: val_accuracy did not improve from 0.63690\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 292ms/step - accuracy: 0.7244 - loss: 0.7867 - val_accuracy: 0.6071 - val_loss: 1.0462 - learning_rate: 2.5000e-04\n",
      "Epoch 35/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.6570 - loss: 0.9129\n",
      "Epoch 35: val_accuracy did not improve from 0.63690\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 278ms/step - accuracy: 0.6581 - loss: 0.9092 - val_accuracy: 0.6071 - val_loss: 1.1644 - learning_rate: 2.5000e-04\n",
      "Epoch 36/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.7318 - loss: 0.7998\n",
      "Epoch 36: val_accuracy did not improve from 0.63690\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 288ms/step - accuracy: 0.7303 - loss: 0.8022 - val_accuracy: 0.6250 - val_loss: 1.0996 - learning_rate: 2.5000e-04\n",
      "Epoch 37/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.7183 - loss: 0.7393\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.63690\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 286ms/step - accuracy: 0.7169 - loss: 0.7448 - val_accuracy: 0.6190 - val_loss: 1.0305 - learning_rate: 2.5000e-04\n",
      "Epoch 38/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.7057 - loss: 0.8004\n",
      "Epoch 38: val_accuracy did not improve from 0.63690\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 278ms/step - accuracy: 0.7060 - loss: 0.8005 - val_accuracy: 0.6012 - val_loss: 1.0686 - learning_rate: 1.2500e-04\n",
      "Epoch 39/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step - accuracy: 0.6888 - loss: 0.8289\n",
      "Epoch 39: val_accuracy did not improve from 0.63690\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 272ms/step - accuracy: 0.6891 - loss: 0.8278 - val_accuracy: 0.6131 - val_loss: 1.0795 - learning_rate: 1.2500e-04\n",
      "Epoch 40/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - accuracy: 0.7120 - loss: 0.8489\n",
      "Epoch 40: val_accuracy did not improve from 0.63690\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.7124 - loss: 0.8480 - val_accuracy: 0.6369 - val_loss: 1.0174 - learning_rate: 1.2500e-04\n",
      "Epoch 41/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.7154 - loss: 0.8374\n",
      "Epoch 41: val_accuracy did not improve from 0.63690\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 293ms/step - accuracy: 0.7161 - loss: 0.8338 - val_accuracy: 0.6310 - val_loss: 1.0715 - learning_rate: 1.2500e-04\n",
      "Epoch 42/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.6995 - loss: 0.8462\n",
      "Epoch 42: val_accuracy did not improve from 0.63690\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 287ms/step - accuracy: 0.6986 - loss: 0.8470 - val_accuracy: 0.6250 - val_loss: 1.0504 - learning_rate: 1.2500e-04\n",
      "Epoch 43/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.7021 - loss: 0.8101\n",
      "Epoch 43: val_accuracy did not improve from 0.63690\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 294ms/step - accuracy: 0.7012 - loss: 0.8119 - val_accuracy: 0.6131 - val_loss: 1.0293 - learning_rate: 1.2500e-04\n",
      "Epoch 44/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - accuracy: 0.6867 - loss: 0.8526\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.63690\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 295ms/step - accuracy: 0.6874 - loss: 0.8517 - val_accuracy: 0.6250 - val_loss: 1.0720 - learning_rate: 1.2500e-04\n",
      "Epoch 45/75\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 217ms/step - accuracy: 0.7489 - loss: 0.6777\n",
      "Epoch 45: val_accuracy did not improve from 0.63690\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 272ms/step - accuracy: 0.7486 - loss: 0.6790 - val_accuracy: 0.6131 - val_loss: 1.0098 - learning_rate: 6.2500e-05\n",
      "Epoch 45: early stopping\n",
      "Restoring model weights from the end of the best epoch: 30.\n",
      "\n",
      "âœ“ Training complete in 3.7 minutes\n",
      "\n",
      "ğŸ“Š Evaluating...\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š EXPERIMENT 2A RESULTS\n",
      "======================================================================\n",
      "\n",
      "Final Metrics:\n",
      "  Training Accuracy:   74.37%\n",
      "  Validation Accuracy: 60.12%\n",
      "  Best Val Accuracy:   63.69%\n",
      "  Validation Loss:     1.0104\n",
      "  Epochs Trained:      45/75\n",
      "  Training Time:       3.7 minutes\n",
      "\n",
      "======================================================================\n",
      "ğŸ“ˆ FULL EXPERIMENT COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Baseline (60% frozen, LR=0.0001, 50ep):\n",
      "  Validation Accuracy: 55.36%\n",
      "\n",
      "Experiment 1 (0% frozen, LR=0.0001, 50ep):\n",
      "  Validation Accuracy: 58.93%\n",
      "  vs Baseline: +3.57pp\n",
      "\n",
      "Experiment 2A (0% frozen, LR=0.0005, 75ep):\n",
      "  Validation Accuracy: 60.12%\n",
      "  vs Baseline: +4.76pp\n",
      "  vs Exp 1: +1.19pp\n",
      "\n",
      "======================================================================\n",
      "ğŸ¯ HYPOTHESIS EVALUATION\n",
      "======================================================================\n",
      "âŒ LIMITED SUCCESS: LR increase didn't help much\n",
      "   Problem may be data quality or domain shift\n",
      "   Next: Investigate data issues or try different approach\n",
      "\n",
      "======================================================================\n",
      "ğŸ” TRAINING BEHAVIOR ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "âœ“ Model starting to fit training data (70%+ train acc)\n",
      "  Progress! More epochs or higher LR could help\n",
      "\n",
      "âœ“ Healthy train/val gap: 14.3pp\n",
      "  Some overfitting but acceptable\n",
      "\n",
      "âœ“ Early stopping triggered at epoch 45\n",
      "  Model converged before max epochs\n",
      "\n",
      "ğŸ’¾ Saving results...\n",
      "âœ“ Results saved to: /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2a_results/\n",
      "  - training_history.csv\n",
      "  - experiment_summary.csv\n",
      "  - comprehensive_analysis.png\n",
      "\n",
      "======================================================================\n",
      "âœ… EXPERIMENT 2A COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "ğŸ“ Key Takeaways:\n",
      "  âš ï¸  5x LR helped but not enough\n",
      "  â†’ Next: Try aggressive 10x LR (Experiment 2B)\n",
      "  â†’ Or: Investigate data quality issues\n",
      "\n",
      "ğŸ’¡ Suggested Next Steps:\n",
      "  1. Review comprehensive_analysis.png for insights\n",
      "  2. Check if train accuracy improved (target: 70%+)\n",
      "  3. Decide: Document success OR try Experiment 2B (10x LR)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EXPERIMENT 2A: Moderate Learning Rate Increase (5x) + More Epochs\n",
    "\n",
    "Baseline: 55.36% val accuracy (60% frozen, LR=0.0001, 50 epochs)\n",
    "Experiment 1: 58.93% val accuracy (0% frozen, LR=0.0001, 50 epochs)\n",
    "Experiment 2A: ? (0% frozen, LR=0.0005, 75 epochs)\n",
    "\n",
    "Strategy: Conservative LR increase (5x instead of 10x) with more time to converge\n",
    "Goal: 70-80% validation accuracy without divergence risk\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "PROJECT_ROOT = '/Users/vishal/Desktop/ML study/project/CNN type/first'\n",
    "\n",
    "CURATED_DATA = os.path.join(PROJECT_ROOT, 'data set fine tune')\n",
    "PRETRAIN_MODEL_PATH = os.path.join(PROJECT_ROOT, 'pyramidnet18_pretrained.keras')\n",
    "\n",
    "# Experiment 2A outputs\n",
    "EXP2A_MODEL_PATH = os.path.join(PROJECT_ROOT, 'experiment2a_model.keras')\n",
    "EXP2A_RESULTS_PATH = os.path.join(PROJECT_ROOT, 'experiment2a_results')\n",
    "\n",
    "os.makedirs(EXP2A_RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "# Parameters\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 75  # More epochs to allow convergence\n",
    "LEARNING_RATE = 0.0005  # 5x increase (conservative approach)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" \"*8 + \"EXPERIMENT 2A: MODERATE LR + MORE EPOCHS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ“Š BASELINE:\")\n",
    "print(\"  Frozen layers: 60%\")\n",
    "print(\"  Learning rate: 0.0001\")\n",
    "print(\"  Epochs: 50\")\n",
    "print(\"  Result: 55.36% val accuracy\")\n",
    "print(\"  Issue: Training acc stuck at 50%, loss still decreasing\")\n",
    "\n",
    "print(\"\\nğŸ“Š EXPERIMENT 1:\")\n",
    "print(\"  Frozen layers: 0%\")\n",
    "print(\"  Learning rate: 0.0001\")\n",
    "print(\"  Epochs: 50\")\n",
    "print(\"  Result: 58.93% val accuracy (+3.57pp)\")\n",
    "print(\"  Issue: Training acc still only 53%, LR too low\")\n",
    "\n",
    "print(\"\\nğŸ§ª EXPERIMENT 2A (Conservative Approach):\")\n",
    "print(\"  Frozen layers: 0% (unchanged)\")\n",
    "print(\"  Learning rate: 0.0005 (5x INCREASE - moderate)\")\n",
    "print(\"  Max epochs: 75 (50% MORE TIME)\")\n",
    "print(\"\\nğŸ’¡ Strategy:\")\n",
    "print(\"   - Safer than 10x increase (less divergence risk)\")\n",
    "print(\"   - More epochs allows smooth convergence\")\n",
    "print(\"   - Expected: 70-80% validation accuracy\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ============================================\n",
    "# LOAD DATA\n",
    "# ============================================\n",
    "print(\"\\nğŸ“ Loading your curated data...\")\n",
    "\n",
    "finetune_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.25\n",
    ")\n",
    "\n",
    "train_generator = finetune_datagen.flow_from_directory(\n",
    "    CURATED_DATA,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = finetune_datagen.flow_from_directory(\n",
    "    CURATED_DATA,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded:\")\n",
    "print(f\"  Training:   {train_generator.samples} images\")\n",
    "print(f\"  Validation: {val_generator.samples} images\")\n",
    "print(f\"  Classes:    {num_classes}\")\n",
    "\n",
    "# ============================================\n",
    "# PYRAMIDNET ARCHITECTURE\n",
    "# ============================================\n",
    "\n",
    "def pyramidnet_basicblock(x, in_filters, out_filters, stride=1):\n",
    "    \"\"\"PyramidNet Basic Block\"\"\"\n",
    "    shortcut = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(out_filters, 3, strides=stride, padding='same', use_bias=False)(x)\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(out_filters, 3, strides=1, padding='same', use_bias=False)(x)\n",
    "    \n",
    "    if in_filters != out_filters or stride != 1:\n",
    "        shortcut = layers.Conv2D(out_filters, 1, strides=stride, use_bias=False)(shortcut)\n",
    "    \n",
    "    x = layers.Add()([x, shortcut])\n",
    "    return x\n",
    "\n",
    "def build_pyramidnet18(num_classes, alpha=48, input_shape=(224, 224, 3)):\n",
    "    \"\"\"Build PyramidNet-18\"\"\"\n",
    "    num_blocks = 8\n",
    "    add_channels_per_block = alpha / num_blocks\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    start_filters = 16\n",
    "    x = layers.Conv2D(start_filters, 7, strides=2, padding='same', use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "    \n",
    "    current_filters = start_filters\n",
    "    \n",
    "    for stage in range(4):\n",
    "        for block in range(2):\n",
    "            block_idx = stage * 2 + block\n",
    "            out_filters = int(start_filters + add_channels_per_block * (block_idx + 1))\n",
    "            stride = 2 if (block == 0 and stage > 0) else 1\n",
    "            x = pyramidnet_basicblock(x, int(current_filters), out_filters, stride=stride)\n",
    "            current_filters = out_filters\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='PyramidNet18')\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# LOAD PRE-TRAINED MODEL\n",
    "# ============================================\n",
    "print(\"\\nğŸ”§ Loading pre-trained model...\")\n",
    "\n",
    "pretrained_model = keras.models.load_model(PRETRAIN_MODEL_PATH)\n",
    "model = build_pyramidnet18(num_classes=num_classes, alpha=48)\n",
    "\n",
    "print(\"ğŸ“¦ Transferring pre-trained weights...\")\n",
    "for pretrained_layer, new_layer in zip(pretrained_model.layers[:-1], model.layers[:-1]):\n",
    "    if len(pretrained_layer.get_weights()) > 0:\n",
    "        new_layer.set_weights(pretrained_layer.get_weights())\n",
    "\n",
    "print(\"âœ“ Pre-trained weights transferred\")\n",
    "\n",
    "# ============================================\n",
    "# UNFREEZE ALL LAYERS (from Exp 1)\n",
    "# ============================================\n",
    "print(\"\\nğŸ”“ Setting all layers trainable...\")\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "total_layers = len(model.layers)\n",
    "trainable_layers = sum([1 for layer in model.layers if layer.trainable])\n",
    "\n",
    "print(f\"âœ“ Total layers: {total_layers}\")\n",
    "print(f\"âœ“ Trainable: {trainable_layers} (100%)\")\n",
    "print(f\"âœ“ Frozen: 0 (0%)\")\n",
    "\n",
    "# ============================================\n",
    "# COMPILE WITH MODERATE LEARNING RATE\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âš™ï¸  COMPILING WITH MODERATE (5x) LEARNING RATE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Optimizer: Adam\")\n",
    "print(f\"âœ“ Learning Rate: {LEARNING_RATE} (was 0.0001)\")\n",
    "print(f\"âœ“ Increase: 5x (conservative)\")\n",
    "print(f\"âœ“ Max Epochs: {EPOCHS} (+50% more time)\")\n",
    "print(f\"âœ“ Strategy: Smooth convergence without divergence risk\")\n",
    "\n",
    "# ============================================\n",
    "# CALLBACKS\n",
    "# ============================================\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,  # More patience for 75 epochs\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=7,  # Adjusted for longer training\n",
    "        min_lr=1e-8,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        EXP2A_MODEL_PATH,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# ============================================\n",
    "# TRAIN\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸš€ STARTING EXPERIMENT 2A TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE} (5x previous experiments)\")\n",
    "print(f\"Max epochs: {EPOCHS}\")\n",
    "print(f\"Strategy: Conservative LR increase with more time to converge\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ“ Training complete in {training_time/60:.1f} minutes\")\n",
    "\n",
    "# ============================================\n",
    "# EVALUATE\n",
    "# ============================================\n",
    "print(\"\\nğŸ“Š Evaluating...\")\n",
    "\n",
    "val_loss, val_accuracy = model.evaluate(val_generator, verbose=0)\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "epochs_trained = len(history.history['accuracy'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š EXPERIMENT 2A RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "print(f\"  Training Accuracy:   {final_train_acc*100:.2f}%\")\n",
    "print(f\"  Validation Accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"  Best Val Accuracy:   {best_val_acc*100:.2f}%\")\n",
    "print(f\"  Validation Loss:     {val_loss:.4f}\")\n",
    "print(f\"  Epochs Trained:      {epochs_trained}/{EPOCHS}\")\n",
    "print(f\"  Training Time:       {training_time/60:.1f} minutes\")\n",
    "\n",
    "# ============================================\n",
    "# COMPARISON WITH ALL EXPERIMENTS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“ˆ FULL EXPERIMENT COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "baseline_val = 55.36\n",
    "exp1_val = 58.93\n",
    "exp2a_val = val_accuracy * 100\n",
    "\n",
    "improvement_from_baseline = exp2a_val - baseline_val\n",
    "improvement_from_exp1 = exp2a_val - exp1_val\n",
    "\n",
    "print(f\"\\nBaseline (60% frozen, LR=0.0001, 50ep):\")\n",
    "print(f\"  Validation Accuracy: {baseline_val:.2f}%\")\n",
    "\n",
    "print(f\"\\nExperiment 1 (0% frozen, LR=0.0001, 50ep):\")\n",
    "print(f\"  Validation Accuracy: {exp1_val:.2f}%\")\n",
    "print(f\"  vs Baseline: {exp1_val - baseline_val:+.2f}pp\")\n",
    "\n",
    "print(f\"\\nExperiment 2A (0% frozen, LR=0.0005, 75ep):\")\n",
    "print(f\"  Validation Accuracy: {exp2a_val:.2f}%\")\n",
    "print(f\"  vs Baseline: {improvement_from_baseline:+.2f}pp\")\n",
    "print(f\"  vs Exp 1: {improvement_from_exp1:+.2f}pp\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ¯ HYPOTHESIS EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if exp2a_val >= 75:\n",
    "    print(\"âœ… EXCELLENT: Conservative approach achieved target (75%+)!\")\n",
    "    print(\"   5x LR + more epochs was the right balance\")\n",
    "    print(\"   Next: Could try 10x LR for comparison (Experiment 2B)\")\n",
    "elif exp2a_val >= 70:\n",
    "    print(\"âœ… HYPOTHESIS CONFIRMED: Achieved 70%+ target!\")\n",
    "    print(\"   Moderate LR increase worked without divergence\")\n",
    "    print(\"   Next: Fine-tune further or try 10x LR\")\n",
    "elif exp2a_val >= 65:\n",
    "    print(\"âœ“ GOOD PROGRESS: Significant improvement\")\n",
    "    print(\"  5x LR helped substantially\")\n",
    "    print(\"  Next: Try 10x LR or even more epochs\")\n",
    "elif improvement_from_exp1 >= 5:\n",
    "    print(\"âš ï¸  MODERATE SUCCESS: 5x LR helped\")\n",
    "    print(\"   But still below 70% target\")\n",
    "    print(\"   Next: Try aggressive 10x LR or investigate data quality\")\n",
    "else:\n",
    "    print(\"âŒ LIMITED SUCCESS: LR increase didn't help much\")\n",
    "    print(\"   Problem may be data quality or domain shift\")\n",
    "    print(\"   Next: Investigate data issues or try different approach\")\n",
    "\n",
    "# ============================================\n",
    "# TRAINING BEHAVIOR ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ” TRAINING BEHAVIOR ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if model fit training data\n",
    "if final_train_acc >= 0.8:\n",
    "    print(\"\\nâœ… Model CAN fit training data (80%+ train acc)\")\n",
    "    print(\"   This is healthy - model has sufficient capacity\")\n",
    "elif final_train_acc >= 0.7:\n",
    "    print(\"\\nâœ“ Model starting to fit training data (70%+ train acc)\")\n",
    "    print(\"  Progress! More epochs or higher LR could help\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Model still struggles to fit training data\")\n",
    "    print(f\"  Training accuracy only {final_train_acc*100:.2f}%\")\n",
    "    print(\"  Consider: Higher LR or data quality issues\")\n",
    "\n",
    "# Check overfitting\n",
    "gap = final_train_acc - val_accuracy\n",
    "if gap > 0.15:\n",
    "    print(f\"\\nâš ï¸  Overfitting detected: {gap*100:.1f}pp gap\")\n",
    "    print(\"  Model memorizing training data, not generalizing\")\n",
    "    print(\"  Solutions: More data, stronger augmentation, regularization\")\n",
    "elif gap > 0.05:\n",
    "    print(f\"\\nâœ“ Healthy train/val gap: {gap*100:.1f}pp\")\n",
    "    print(\"  Some overfitting but acceptable\")\n",
    "elif gap > 0:\n",
    "    print(f\"\\nâœ“ Minimal train/val gap: {gap*100:.1f}pp\")\n",
    "    print(\"  Good generalization!\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Unusual: Val acc > Train acc by {-gap*100:.1f}pp\")\n",
    "    print(\"  May indicate: Training issues or very small validation set\")\n",
    "\n",
    "# Check convergence\n",
    "if epochs_trained < EPOCHS:\n",
    "    print(f\"\\nâœ“ Early stopping triggered at epoch {epochs_trained}\")\n",
    "    print(\"  Model converged before max epochs\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Reached max epochs ({EPOCHS})\")\n",
    "    last_5_val = history.history['val_accuracy'][-5:]\n",
    "    if max(last_5_val) - min(last_5_val) < 0.01:\n",
    "        print(\"  âœ“ But validation accuracy plateaued (converged)\")\n",
    "    else:\n",
    "        print(\"  âš ï¸  Validation still improving - could train longer\")\n",
    "\n",
    "# ============================================\n",
    "# SAVE RESULTS\n",
    "# ============================================\n",
    "print(\"\\nğŸ’¾ Saving results...\")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(os.path.join(EXP2A_RESULTS_PATH, 'training_history.csv'), index=False)\n",
    "\n",
    "summary = {\n",
    "    'experiment': 'Experiment 2A: Moderate LR (5x) + More Epochs',\n",
    "    'baseline_val_acc': baseline_val,\n",
    "    'exp1_val_acc': exp1_val,\n",
    "    'exp2a_val_acc': exp2a_val,\n",
    "    'improvement_from_baseline': improvement_from_baseline,\n",
    "    'improvement_from_exp1': improvement_from_exp1,\n",
    "    'frozen_layers': 0,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'max_epochs': EPOCHS,\n",
    "    'epochs_trained': epochs_trained,\n",
    "    'training_time_min': training_time/60,\n",
    "    'final_train_acc': final_train_acc * 100,\n",
    "    'best_val_acc': best_val_acc * 100,\n",
    "    'train_val_gap': gap * 100\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df.to_csv(os.path.join(EXP2A_RESULTS_PATH, 'experiment_summary.csv'), index=False)\n",
    "\n",
    "# Comprehensive visualization\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Experiment comparison bar chart\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "experiments = ['Baseline\\n(60% frz, 0.0001, 50ep)', \n",
    "               'Exp 1\\n(0% frz, 0.0001, 50ep)', \n",
    "               'Exp 2A\\n(0% frz, 0.0005, 75ep)']\n",
    "accuracies = [baseline_val, exp1_val, exp2a_val]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "bars = ax1.bar(experiments, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax1.axhline(y=70, color='green', linestyle='--', label='Target (70%)', linewidth=2)\n",
    "ax1.axhline(y=80, color='darkgreen', linestyle='--', label='Stretch (80%)', linewidth=2)\n",
    "ax1.set_ylabel('Validation Accuracy (%)', fontweight='bold', fontsize=12)\n",
    "ax1.set_title('Experiment Progression', fontsize=16, fontweight='bold')\n",
    "ax1.set_ylim([0, 100])\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.legend(fontsize=10)\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, height + 2, \n",
    "             f'{acc:.1f}%', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# 2. Accuracy curves\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.plot(history.history['accuracy'], label='Train', linewidth=2.5, color='#4ECDC4')\n",
    "ax2.plot(history.history['val_accuracy'], label='Val', linewidth=2.5, color='#FF6B6B')\n",
    "ax2.axhline(y=baseline_val/100, color='gray', linestyle='--', alpha=0.5, label='Baseline')\n",
    "ax2.set_title('Experiment 2A: Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Loss curves\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "ax3.plot(history.history['loss'], label='Train', linewidth=2.5, color='#4ECDC4')\n",
    "ax3.plot(history.history['val_loss'], label='Val', linewidth=2.5, color='#FF6B6B')\n",
    "ax3.set_title('Experiment 2A: Loss', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Learning rate schedule\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "if 'lr' in history.history:\n",
    "    ax4.plot(history.history['lr'], linewidth=2.5, color='#9B59B6')\n",
    "    ax4.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Learning Rate')\n",
    "    ax4.set_yscale('log')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax4.text(0.5, 0.5, 'LR history not available', \n",
    "             ha='center', va='center', transform=ax4.transAxes)\n",
    "    ax4.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 5. Train vs Val accuracy gap\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "train_val_gap = [t - v for t, v in zip(history.history['accuracy'], history.history['val_accuracy'])]\n",
    "ax5.plot(train_val_gap, linewidth=2.5, color='#E74C3C')\n",
    "ax5.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "ax5.axhline(y=0.15, color='orange', linestyle='--', label='Overfitting threshold')\n",
    "ax5.set_title('Overfitting Monitor (Train-Val Gap)', fontsize=14, fontweight='bold')\n",
    "ax5.set_xlabel('Epoch')\n",
    "ax5.set_ylabel('Accuracy Gap')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Metrics summary table\n",
    "ax6 = fig.add_subplot(gs[2, 1:])\n",
    "ax6.axis('tight')\n",
    "ax6.axis('off')\n",
    "\n",
    "table_data = [\n",
    "    ['Metric', 'Value'],\n",
    "    ['Final Train Acc', f'{final_train_acc*100:.2f}%'],\n",
    "    ['Final Val Acc', f'{val_accuracy*100:.2f}%'],\n",
    "    ['Best Val Acc', f'{best_val_acc*100:.2f}%'],\n",
    "    ['Train-Val Gap', f'{gap*100:.2f}pp'],\n",
    "    ['Epochs Trained', f'{epochs_trained}/{EPOCHS}'],\n",
    "    ['Training Time', f'{training_time/60:.1f} min'],\n",
    "    ['Improvement vs Baseline', f'{improvement_from_baseline:+.2f}pp'],\n",
    "    ['Improvement vs Exp 1', f'{improvement_from_exp1:+.2f}pp']\n",
    "]\n",
    "\n",
    "table = ax6.table(cellText=table_data, cellLoc='left', loc='center',\n",
    "                  colWidths=[0.6, 0.4])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2)\n",
    "\n",
    "# Style header row\n",
    "for i in range(2):\n",
    "    table[(0, i)].set_facecolor('#3498DB')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Alternate row colors\n",
    "for i in range(1, len(table_data)):\n",
    "    for j in range(2):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#ECF0F1')\n",
    "\n",
    "ax6.set_title('Experiment 2A Summary', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.savefig(os.path.join(EXP2A_RESULTS_PATH, 'comprehensive_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"âœ“ Results saved to: {EXP2A_RESULTS_PATH}/\")\n",
    "print(f\"  - training_history.csv\")\n",
    "print(f\"  - experiment_summary.csv\")\n",
    "print(f\"  - comprehensive_analysis.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âœ… EXPERIMENT 2A COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nğŸ“ Key Takeaways:\")\n",
    "if exp2a_val >= 75:\n",
    "    print(\"  âœ“ Conservative 5x LR strategy worked excellently!\")\n",
    "    print(\"  âœ“ Achieved strong performance without divergence\")\n",
    "    print(\"  â†’ Next: Can document this as the successful approach\")\n",
    "    print(\"  â†’ Or: Try Exp 2B (10x LR) for comparison\")\n",
    "elif exp2a_val >= 70:\n",
    "    print(\"  âœ“ Hit target! 5x LR was the right choice\")\n",
    "    print(\"  âœ“ More epochs helped model converge properly\")\n",
    "    print(\"  â†’ Next: Optionally try 10x LR to see if we can push higher\")\n",
    "elif exp2a_val >= 65:\n",
    "    print(\"  âœ“ Significant improvement with moderate LR\")\n",
    "    print(\"  â†’ Next: Try Experiment 2B with 10x LR (more aggressive)\")\n",
    "else:\n",
    "    print(\"  âš ï¸  5x LR helped but not enough\")\n",
    "    print(\"  â†’ Next: Try aggressive 10x LR (Experiment 2B)\")\n",
    "    print(\"  â†’ Or: Investigate data quality issues\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Suggested Next Steps:\")\n",
    "print(\"  1. Review comprehensive_analysis.png for insights\")\n",
    "print(\"  2. Check if train accuracy improved (target: 70%+)\")\n",
    "print(\"  3. Decide: Document success OR try Experiment 2B (10x LR)\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176dc412-c06a-4b5f-9652-8aaf9e2a8c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f5c3b1-0788-4b31-8ce7-4342f713d37c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac768c-cc8a-4460-a65e-8c8d013e512f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dbc9cdb-9a92-4738-94d2-51572752fb47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                    EXPERIMENT 3: ARCHITECTURE COMPARISON\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ Goal: Fair comparison of 5 CNN architectures from scratch\n",
      "ğŸ“Š Dataset: Your 800 curated real-world apple images\n",
      "ğŸ”§ Training: All models use identical hyperparameters\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Loading your curated data...\n",
      "Found 519 images belonging to 8 classes.\n",
      "Found 168 images belonging to 8 classes.\n",
      "âœ“ Training:   519 images\n",
      "âœ“ Validation: 168 images\n",
      "âœ“ Classes:    8\n",
      "\n",
      "================================================================================\n",
      "ğŸ”„ TRAINING ALL ARCHITECTURES FROM SCRATCH\n",
      "================================================================================\n",
      "\n",
      "Started at: 07:03:42\n",
      "Total models: 5\n",
      "Estimated time: 25-35 minutes\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ TRAINING: PyramidNet18\n",
      "================================================================================\n",
      "Parameters: 293,448 (0.3M)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "/opt/anaconda3/lib/python3.12/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Training complete:\n",
      "  Val Accuracy: 64.29%\n",
      "  Train/Val Gap: 13.36pp\n",
      "  Epochs: 75/75\n",
      "  Time: 6.2 min\n",
      "  Inference: 20.4 ms/image\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ TRAINING: ResNet18\n",
      "================================================================================\n",
      "Parameters: 11,180,616 (11.2M)\n",
      "âœ“ Training complete:\n",
      "  Val Accuracy: 17.26%\n",
      "  Train/Val Gap: 41.70pp\n",
      "  Epochs: 16/75\n",
      "  Time: 5.2 min\n",
      "  Inference: 32.0 ms/image\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ TRAINING: MobileNetV2\n",
      "================================================================================\n",
      "Parameters: 2,234,120 (2.2M)\n",
      "âœ“ Training complete:\n",
      "  Val Accuracy: 11.90%\n",
      "  Train/Val Gap: 55.73pp\n",
      "  Epochs: 29/75\n",
      "  Time: 8.0 min\n",
      "  Inference: 29.1 ms/image\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ TRAINING: EfficientNetB0\n",
      "================================================================================\n",
      "Parameters: 4,017,796 (4.0M)\n",
      "âœ“ Training complete:\n",
      "  Val Accuracy: 19.64%\n",
      "  Train/Val Gap: 34.69pp\n",
      "  Epochs: 19/75\n",
      "  Time: 7.4 min\n",
      "  Inference: 37.3 ms/image\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ TRAINING: DenseNet121\n",
      "================================================================================\n",
      "Parameters: 6,962,056 (7.0M)\n",
      "âœ“ Training complete:\n",
      "  Val Accuracy: 11.90%\n",
      "  Train/Val Gap: 50.91pp\n",
      "  Epochs: 16/75\n",
      "  Time: 156.3 min\n",
      "  Inference: 53.3 ms/image\n",
      "\n",
      "================================================================================\n",
      "âœ… ALL MODELS TRAINED!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š COMPARATIVE RESULTS:\n",
      "================================================================================\n",
      "         Model Parameters (M) Val Acc (%) Best Val (%) Train-Val Gap  Epochs Training (min) Inference (ms)\n",
      "  PyramidNet18            0.3       64.29        65.48         13.36      75            6.2           20.4\n",
      "      ResNet18           11.2       17.26        22.62         41.70      16            5.2           32.0\n",
      "   MobileNetV2            2.2       11.90        19.64         55.73      29            8.0           29.1\n",
      "EfficientNetB0            4.0       19.64        19.64         34.69      19            7.4           37.3\n",
      "   DenseNet121            7.0       11.90        19.64         50.91      16          156.3           53.3\n",
      "\n",
      "ğŸ† WINNERS:\n",
      "  Best Accuracy: PyramidNet18 (64.29%)\n",
      "  Fastest Inference: PyramidNet18 (20.4 ms)\n",
      "  Least Overfitting: PyramidNet18 (13.36pp gap)\n",
      "\n",
      "ğŸ“ˆ Creating visualizations...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type int64 is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 519\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(COMPARISON_RESULTS, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdetailed_results.json\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;66;03m# Remove history from JSON (too large)\u001b[39;00m\n\u001b[1;32m    518\u001b[0m     results_for_json \u001b[38;5;241m=\u001b[39m [{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m'\u001b[39m} \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m all_results]\n\u001b[0;32m--> 519\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(results_for_json, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ“ Results saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCOMPARISON_RESULTS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - architecture_comparison.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    180\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/json/encoder.py:430\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m _floatstr(o)\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/json/encoder.py:326\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 326\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/json/encoder.py:406\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 406\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/json/encoder.py:439\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    438\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 439\u001b[0m o \u001b[38;5;241m=\u001b[39m _default(o)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/json/encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type int64 is not JSON serializable"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EXPERIMENT 3: Multi-Architecture Comparison from Scratch\n",
    "\n",
    "Goal: Compare 5 modern CNN architectures trained from scratch on your 800 curated images\n",
    "\n",
    "Models:\n",
    "1. PyramidNet-18 (~11M params) - Current baseline\n",
    "2. ResNet-18 (~11M params) - Classic residual network\n",
    "3. MobileNetV2 (~3.5M params) - Efficient mobile architecture\n",
    "4. EfficientNet-B0 (~5M params) - Compound scaling\n",
    "5. DenseNet-121 (~8M params) - Dense connections\n",
    "\n",
    "All trained identically:\n",
    "- No pre-training (random initialization)\n",
    "- Same data augmentation\n",
    "- Same optimizer (Adam, LR=0.0005)\n",
    "- Same max epochs (75)\n",
    "- Same evaluation protocol\n",
    "\n",
    "This creates a fair comparison to determine which architecture generalizes best with limited data.\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.applications import ResNet50, MobileNetV2, EfficientNetB0, DenseNet121\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "PROJECT_ROOT = '/Users/vishal/Desktop/ML study/project/CNN type/first'\n",
    "CURATED_DATA = os.path.join(PROJECT_ROOT, 'data set fine tune')\n",
    "COMPARISON_RESULTS = os.path.join(PROJECT_ROOT, 'architecture_comparison')\n",
    "\n",
    "os.makedirs(COMPARISON_RESULTS, exist_ok=True)\n",
    "\n",
    "# Training parameters (identical for all models)\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 75\n",
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \"*20 + \"EXPERIMENT 3: ARCHITECTURE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nğŸ¯ Goal: Fair comparison of 5 CNN architectures from scratch\")\n",
    "print(\"ğŸ“Š Dataset: Your 800 curated real-world apple images\")\n",
    "print(\"ğŸ”§ Training: All models use identical hyperparameters\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# ============================================\n",
    "# LOAD DATA (Same for all models)\n",
    "# ============================================\n",
    "print(\"\\nğŸ“ Loading your curated data...\")\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.7, 1.3],\n",
    "    zoom_range=0.2,\n",
    "    validation_split=0.25\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    CURATED_DATA,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    CURATED_DATA,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "print(f\"âœ“ Training:   {train_generator.samples} images\")\n",
    "print(f\"âœ“ Validation: {val_generator.samples} images\")\n",
    "print(f\"âœ“ Classes:    {num_classes}\")\n",
    "\n",
    "# ============================================\n",
    "# MODEL ARCHITECTURES\n",
    "# ============================================\n",
    "\n",
    "def build_pyramidnet18(num_classes):\n",
    "    \"\"\"PyramidNet-18 (11M params)\"\"\"\n",
    "    \n",
    "    def pyramidnet_basicblock(x, in_filters, out_filters, stride=1):\n",
    "        shortcut = x\n",
    "        \n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.Conv2D(out_filters, 3, strides=stride, padding='same', use_bias=False)(x)\n",
    "        \n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.Conv2D(out_filters, 3, strides=1, padding='same', use_bias=False)(x)\n",
    "        \n",
    "        if in_filters != out_filters or stride != 1:\n",
    "            shortcut = layers.Conv2D(out_filters, 1, strides=stride, use_bias=False)(shortcut)\n",
    "        \n",
    "        x = layers.Add()([x, shortcut])\n",
    "        return x\n",
    "    \n",
    "    alpha = 48\n",
    "    num_blocks = 8\n",
    "    add_channels_per_block = alpha / num_blocks\n",
    "    \n",
    "    inputs = layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    \n",
    "    start_filters = 16\n",
    "    x = layers.Conv2D(start_filters, 7, strides=2, padding='same', use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "    \n",
    "    current_filters = start_filters\n",
    "    \n",
    "    for stage in range(4):\n",
    "        for block in range(2):\n",
    "            block_idx = stage * 2 + block\n",
    "            out_filters = int(start_filters + add_channels_per_block * (block_idx + 1))\n",
    "            stride = 2 if (block == 0 and stage > 0) else 1\n",
    "            x = pyramidnet_basicblock(x, int(current_filters), out_filters, stride=stride)\n",
    "            current_filters = out_filters\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return models.Model(inputs=inputs, outputs=outputs, name='PyramidNet18')\n",
    "\n",
    "def build_resnet18(num_classes):\n",
    "    \"\"\"ResNet-18 (11M params)\"\"\"\n",
    "    \n",
    "    def resnet_block(x, filters, stride=1):\n",
    "        shortcut = x\n",
    "        \n",
    "        x = layers.Conv2D(filters, 3, strides=stride, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        \n",
    "        x = layers.Conv2D(filters, 3, strides=1, padding='same', use_bias=False)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        if stride != 1:\n",
    "            shortcut = layers.Conv2D(filters, 1, strides=stride, use_bias=False)(shortcut)\n",
    "            shortcut = layers.BatchNormalization()(shortcut)\n",
    "        \n",
    "        x = layers.Add()([x, shortcut])\n",
    "        x = layers.ReLU()(x)\n",
    "        return x\n",
    "    \n",
    "    inputs = layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "    \n",
    "    x = layers.Conv2D(64, 7, strides=2, padding='same', use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "    \n",
    "    # 4 stages, 2 blocks each\n",
    "    for stage, filters in enumerate([64, 128, 256, 512]):\n",
    "        for block in range(2):\n",
    "            stride = 2 if (block == 0 and stage > 0) else 1\n",
    "            x = resnet_block(x, filters, stride)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return models.Model(inputs=inputs, outputs=outputs, name='ResNet18')\n",
    "\n",
    "def build_mobilenetv2(num_classes):\n",
    "    \"\"\"MobileNetV2 (3.5M params)\"\"\"\n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "        include_top=False,\n",
    "        weights=None  # Train from scratch!\n",
    "    )\n",
    "    \n",
    "    inputs = base_model.input\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return models.Model(inputs=inputs, outputs=outputs, name='MobileNetV2')\n",
    "\n",
    "def build_efficientnetb0(num_classes):\n",
    "    \"\"\"EfficientNet-B0 (5M params)\"\"\"\n",
    "    base_model = EfficientNetB0(\n",
    "        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "        include_top=False,\n",
    "        weights=None  # Train from scratch!\n",
    "    )\n",
    "    \n",
    "    inputs = base_model.input\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return models.Model(inputs=inputs, outputs=outputs, name='EfficientNetB0')\n",
    "\n",
    "def build_densenet121(num_classes):\n",
    "    \"\"\"DenseNet-121 (8M params)\"\"\"\n",
    "    base_model = DenseNet121(\n",
    "        input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "        include_top=False,\n",
    "        weights=None  # Train from scratch!\n",
    "    )\n",
    "    \n",
    "    inputs = base_model.input\n",
    "    x = base_model.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return models.Model(inputs=inputs, outputs=outputs, name='DenseNet121')\n",
    "\n",
    "# ============================================\n",
    "# TRAINING FUNCTION\n",
    "# ============================================\n",
    "\n",
    "def train_model(model, model_name):\n",
    "    \"\"\"Train a model and return results\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ğŸš€ TRAINING: {model_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Count parameters\n",
    "    trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "    print(f\"Parameters: {trainable_params:,} ({trainable_params/1e6:.1f}M)\")\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    model_path = os.path.join(COMPARISON_RESULTS, f'{model_name}_best.keras')\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=0\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=7,\n",
    "            min_lr=1e-8,\n",
    "            verbose=0\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            model_path,\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=0\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train\n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_generator,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0  # Silent training, we'll show summary\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate\n",
    "    val_loss, val_accuracy = model.evaluate(val_generator, verbose=0)\n",
    "    \n",
    "    # Extract metrics\n",
    "    final_train_acc = history.history['accuracy'][-1]\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    epochs_trained = len(history.history['accuracy'])\n",
    "    train_val_gap = final_train_acc - val_accuracy\n",
    "    \n",
    "    # Measure inference time\n",
    "    sample_batch = next(iter(val_generator))[0][:1]  # Single image\n",
    "    inference_times = []\n",
    "    for _ in range(100):\n",
    "        start = time.time()\n",
    "        _ = model.predict(sample_batch, verbose=0)\n",
    "        inference_times.append((time.time() - start) * 1000)  # ms\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'parameters': trainable_params,\n",
    "        'parameters_m': trainable_params / 1e6,\n",
    "        'final_train_acc': final_train_acc * 100,\n",
    "        'final_val_acc': val_accuracy * 100,\n",
    "        'best_val_acc': best_val_acc * 100,\n",
    "        'train_val_gap': train_val_gap * 100,\n",
    "        'epochs_trained': epochs_trained,\n",
    "        'training_time_min': training_time / 60,\n",
    "        'avg_epoch_time_sec': training_time / epochs_trained,\n",
    "        'inference_time_ms': avg_inference_time,\n",
    "        'history': history.history\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ“ Training complete:\")\n",
    "    print(f\"  Val Accuracy: {val_accuracy*100:.2f}%\")\n",
    "    print(f\"  Train/Val Gap: {train_val_gap*100:.2f}pp\")\n",
    "    print(f\"  Epochs: {epochs_trained}/{EPOCHS}\")\n",
    "    print(f\"  Time: {training_time/60:.1f} min\")\n",
    "    print(f\"  Inference: {avg_inference_time:.1f} ms/image\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ============================================\n",
    "# TRAIN ALL MODELS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ”„ TRAINING ALL ARCHITECTURES FROM SCRATCH\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nStarted at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"Total models: 5\")\n",
    "print(f\"Estimated time: 25-35 minutes\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# 1. PyramidNet-18\n",
    "model = build_pyramidnet18(num_classes)\n",
    "results = train_model(model, 'PyramidNet18')\n",
    "all_results.append(results)\n",
    "del model  # Free memory\n",
    "\n",
    "# 2. ResNet-18\n",
    "model = build_resnet18(num_classes)\n",
    "results = train_model(model, 'ResNet18')\n",
    "all_results.append(results)\n",
    "del model\n",
    "\n",
    "# 3. MobileNetV2\n",
    "model = build_mobilenetv2(num_classes)\n",
    "results = train_model(model, 'MobileNetV2')\n",
    "all_results.append(results)\n",
    "del model\n",
    "\n",
    "# 4. EfficientNet-B0\n",
    "model = build_efficientnetb0(num_classes)\n",
    "results = train_model(model, 'EfficientNetB0')\n",
    "all_results.append(results)\n",
    "del model\n",
    "\n",
    "# 5. DenseNet-121\n",
    "model = build_densenet121(num_classes)\n",
    "results = train_model(model, 'DenseNet121')\n",
    "all_results.append(results)\n",
    "del model\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ALL MODELS TRAINED!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================\n",
    "# COMPARATIVE ANALYSIS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nğŸ“Š COMPARATIVE RESULTS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': r['model_name'],\n",
    "        'Parameters (M)': f\"{r['parameters_m']:.1f}\",\n",
    "        'Val Acc (%)': f\"{r['final_val_acc']:.2f}\",\n",
    "        'Best Val (%)': f\"{r['best_val_acc']:.2f}\",\n",
    "        'Train-Val Gap': f\"{r['train_val_gap']:.2f}\",\n",
    "        'Epochs': r['epochs_trained'],\n",
    "        'Training (min)': f\"{r['training_time_min']:.1f}\",\n",
    "        'Inference (ms)': f\"{r['inference_time_ms']:.1f}\"\n",
    "    }\n",
    "    for r in all_results\n",
    "])\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Find best models\n",
    "best_accuracy = max(all_results, key=lambda x: x['final_val_acc'])\n",
    "fastest_inference = min(all_results, key=lambda x: x['inference_time_ms'])\n",
    "least_overfit = min(all_results, key=lambda x: abs(x['train_val_gap']))\n",
    "\n",
    "print(\"\\nğŸ† WINNERS:\")\n",
    "print(f\"  Best Accuracy: {best_accuracy['model_name']} ({best_accuracy['final_val_acc']:.2f}%)\")\n",
    "print(f\"  Fastest Inference: {fastest_inference['model_name']} ({fastest_inference['inference_time_ms']:.1f} ms)\")\n",
    "print(f\"  Least Overfitting: {least_overfit['model_name']} ({least_overfit['train_val_gap']:.2f}pp gap)\")\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nğŸ“ˆ Creating visualizations...\")\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Validation Accuracy Comparison\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "models = [r['model_name'] for r in all_results]\n",
    "val_accs = [r['final_val_acc'] for r in all_results]\n",
    "colors = ['#3498DB', '#E74C3C', '#2ECC71', '#F39C12', '#9B59B6']\n",
    "\n",
    "bars = ax1.bar(models, val_accs, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('Validation Accuracy (%)', fontweight='bold', fontsize=12)\n",
    "ax1.set_title('Architecture Comparison: Validation Accuracy', fontsize=16, fontweight='bold')\n",
    "ax1.set_ylim([0, 100])\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, acc in zip(bars, val_accs):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, height + 2,\n",
    "             f'{acc:.1f}%', ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 2. Parameters vs Accuracy\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "params = [r['parameters_m'] for r in all_results]\n",
    "ax2.scatter(params, val_accs, s=200, c=colors, alpha=0.7, edgecolors='black', linewidth=2)\n",
    "for i, model in enumerate(models):\n",
    "    ax2.annotate(model, (params[i], val_accs[i]), fontsize=9, ha='center', va='bottom')\n",
    "ax2.set_xlabel('Parameters (Millions)', fontweight='bold')\n",
    "ax2.set_ylabel('Validation Accuracy (%)', fontweight='bold')\n",
    "ax2.set_title('Parameters vs Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Inference Time\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "inference_times = [r['inference_time_ms'] for r in all_results]\n",
    "bars = ax3.barh(models, inference_times, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax3.set_xlabel('Inference Time (ms)', fontweight='bold')\n",
    "ax3.set_title('Inference Speed', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='x')\n",
    "for bar, time in zip(bars, inference_times):\n",
    "    width = bar.get_width()\n",
    "    ax3.text(width + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "             f'{time:.1f}', ha='left', va='center', fontweight='bold')\n",
    "\n",
    "# 4. Overfitting Analysis\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "gaps = [r['train_val_gap'] for r in all_results]\n",
    "bars = ax4.barh(models, gaps, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax4.axvline(x=15, color='red', linestyle='--', label='High overfitting threshold')\n",
    "ax4.set_xlabel('Train-Val Gap (pp)', fontweight='bold')\n",
    "ax4.set_title('Overfitting Analysis', fontsize=14, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "ax4.legend()\n",
    "\n",
    "# 5-9. Individual training curves\n",
    "for idx, result in enumerate(all_results):\n",
    "    row = 2\n",
    "    col = idx if idx < 3 else idx - 3\n",
    "    if idx >= 3:\n",
    "        row = 2\n",
    "        \n",
    "    if idx < 3:\n",
    "        ax = fig.add_subplot(gs[2, idx])\n",
    "    else:\n",
    "        # Add extra row if needed\n",
    "        continue\n",
    "        \n",
    "    history = result['history']\n",
    "    ax.plot(history['accuracy'], label='Train', linewidth=2, alpha=0.8)\n",
    "    ax.plot(history['val_accuracy'], label='Val', linewidth=2, alpha=0.8)\n",
    "    ax.set_title(f\"{result['model_name']}\", fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch', fontsize=9)\n",
    "    ax.set_ylabel('Accuracy', fontsize=9)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.savefig(os.path.join(COMPARISON_RESULTS, 'architecture_comparison.png'), \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Save detailed results\n",
    "comparison_df.to_csv(os.path.join(COMPARISON_RESULTS, 'comparison_summary.csv'), index=False)\n",
    "\n",
    "# Save all results as JSON\n",
    "with open(os.path.join(COMPARISON_RESULTS, 'detailed_results.json'), 'w') as f:\n",
    "    # Remove history from JSON (too large)\n",
    "    results_for_json = [{k: v for k, v in r.items() if k != 'history'} for r in all_results]\n",
    "    json.dump(results_for_json, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Results saved to: {COMPARISON_RESULTS}/\")\n",
    "print(\"  - architecture_comparison.png\")\n",
    "print(\"  - comparison_summary.csv\")\n",
    "print(\"  - detailed_results.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… EXPERIMENT 3 COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“ KEY FINDINGS:\")\n",
    "print(f\"  Best Model: {best_accuracy['model_name']} ({best_accuracy['final_val_acc']:.2f}%)\")\n",
    "print(f\"  Fastest: {fastest_inference['model_name']} ({fastest_inference['inference_time_ms']:.1f} ms)\")\n",
    "print(f\"  Most Generalizable: {least_overfit['model_name']} ({least_overfit['train_val_gap']:.2f}pp gap)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ INSIGHTS:\")\n",
    "print(\"  1. All models trained from scratch (no transfer learning)\")\n",
    "print(\"  2. Validation accuracy range:\", f\"{min(val_accs):.1f}% - {max(val_accs):.1f}%\")\n",
    "print(\"  3. This is your TRUE baseline with 800 images\")\n",
    "print(\"  4. Compare these results to your Fruit-360 pre-training experiments\")\n",
    "\n",
    "print(\"\\nğŸ¯ NEXT STEPS:\")\n",
    "print(\"  1. Review architecture_comparison.png\")\n",
    "print(\"  2. Document which architecture generalizes best\")\n",
    "print(\"  3. Use best model for your final application\")\n",
    "print(\"  4. Consider: Is 800 images sufficient for your use case?\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038f0de-6746-4a41-a3cc-1fea31e6972a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66aa541c-9e54-4670-9477-af0d705c4166",
   "metadata": {},
   "source": [
    "# EXPERIMENT 2 ENHANCED: Maximize PyramidNet-18 Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb2df0e-8929-48ee-a314-c9f2b667beae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "               EXPERIMENT 2 ENHANCED: ALL OPTIMIZATIONS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š BASELINE:\n",
      "  Frozen: 60%, LR=0.0001, 50 epochs, Basic augmentation\n",
      "  Result: 55.36% val accuracy\n",
      "\n",
      "ğŸ“Š EXPERIMENT 1:\n",
      "  Frozen: 0%, LR=0.0001, 50 epochs\n",
      "  Result: 58.93% val accuracy (+3.57pp)\n",
      "  Issue: Training acc 53%, LR too low\n",
      "\n",
      "ğŸš€ EXPERIMENT 2 ENHANCED (All improvements combined):\n",
      "  âœ“ Frozen: 0% (all trainable)\n",
      "  âœ“ Learning Rate: 0.001 (10x INCREASE)\n",
      "  âœ“ LR Schedule: Cosine Annealing (smooth decay)\n",
      "  âœ“ Data Augmentation: STRONGER (more transforms)\n",
      "  âœ“ Label Smoothing: 0.1 (regularization)\n",
      "  âœ“ Max Epochs: 100 (2x more time)\n",
      "\n",
      "ğŸ¯ Target: 70-80% validation accuracy\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Loading data with STRONGER augmentation...\n",
      "Found 519 images belonging to 8 classes.\n",
      "Found 168 images belonging to 8 classes.\n",
      "\n",
      "âœ“ Data loaded:\n",
      "  Training:   519 images\n",
      "  Validation: 168 images\n",
      "  Classes:    8\n",
      "\n",
      "âœ“ ENHANCED Augmentation:\n",
      "  Rotation: Â±30Â° (was Â±20Â°)\n",
      "  Shift: Â±20% (was Â±15%)\n",
      "  Shear: Â±15Â° (NEW)\n",
      "  Zoom: Â±30% (was Â±20%)\n",
      "  Brightness: 0.6-1.4 (was 0.7-1.3)\n",
      "\n",
      "ğŸ”§ Loading pre-trained model...\n",
      "ğŸ“¦ Transferring pre-trained weights...\n",
      "âœ“ Pre-trained weights transferred\n",
      "\n",
      "ğŸ”“ Unfreezing all layers...\n",
      "âœ“ Total layers: 74\n",
      "âœ“ Trainable: 74 (100%)\n",
      "âœ“ Frozen: 0 (0%)\n",
      "\n",
      "================================================================================\n",
      "âš™ï¸  COMPILING WITH ENHANCED SETTINGS\n",
      "================================================================================\n",
      "\n",
      "âœ“ Optimizer: Adam\n",
      "âœ“ Initial Learning Rate: 0.001 (10x from baseline)\n",
      "âœ“ LR Schedule: Cosine Annealing (smooth decay over 100 epochs)\n",
      "âœ“ Label Smoothing: 0.1 (regularization)\n",
      "âœ“ Loss: Categorical Crossentropy with label smoothing\n",
      "\n",
      "âœ“ LR schedule visualization saved\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ STARTING EXPERIMENT 2 ENHANCED TRAINING\n",
      "================================================================================\n",
      "Started at: 11:12:22\n",
      "Improvements:\n",
      "  âœ“ Learning rate: 10x increase (0.001)\n",
      "  âœ“ Cosine annealing: Smooth LR decay\n",
      "  âœ“ Stronger augmentation: More transforms\n",
      "  âœ“ Label smoothing: 0.1\n",
      "  âœ“ More epochs: 100 (vs 50 baseline)\n",
      "\n",
      "Expected training time: 60-90 minutes\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.1739 - loss: 3.1839"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25000, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_enhanced_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 270ms/step - accuracy: 0.1746 - loss: 3.1698 - val_accuracy: 0.2500 - val_loss: 4.6768 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.2993 - loss: 2.3467\n",
      "Epoch 2: val_accuracy improved from 0.25000 to 0.26190, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_enhanced_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 251ms/step - accuracy: 0.3003 - loss: 2.3430 - val_accuracy: 0.2619 - val_loss: 8.9927 - learning_rate: 9.9975e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.3579 - loss: 2.2273\n",
      "Epoch 3: val_accuracy improved from 0.26190 to 0.30357, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_enhanced_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 246ms/step - accuracy: 0.3576 - loss: 2.2229 - val_accuracy: 0.3036 - val_loss: 7.0372 - learning_rate: 9.9877e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.3689 - loss: 1.9966\n",
      "Epoch 4: val_accuracy improved from 0.30357 to 0.30952, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_enhanced_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 258ms/step - accuracy: 0.3698 - loss: 1.9949 - val_accuracy: 0.3095 - val_loss: 5.9428 - learning_rate: 9.9655e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.4170 - loss: 1.7855\n",
      "Epoch 5: val_accuracy improved from 0.30952 to 0.32143, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_enhanced_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 251ms/step - accuracy: 0.4161 - loss: 1.7883 - val_accuracy: 0.3214 - val_loss: 6.3854 - learning_rate: 9.9262e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.4340 - loss: 1.8733\n",
      "Epoch 6: val_accuracy improved from 0.32143 to 0.40476, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_enhanced_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 253ms/step - accuracy: 0.4344 - loss: 1.8711 - val_accuracy: 0.4048 - val_loss: 5.9353 - learning_rate: 9.8651e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.4753 - loss: 1.7245\n",
      "Epoch 7: val_accuracy improved from 0.40476 to 0.50000, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_enhanced_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 263ms/step - accuracy: 0.4734 - loss: 1.7306 - val_accuracy: 0.5000 - val_loss: 2.9569 - learning_rate: 9.7777e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 253ms/step - accuracy: 0.4261 - loss: 1.7350\n",
      "Epoch 8: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 277ms/step - accuracy: 0.4263 - loss: 1.7344 - val_accuracy: 0.5000 - val_loss: 3.0917 - learning_rate: 9.6600e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.4904 - loss: 1.6404\n",
      "Epoch 9: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 252ms/step - accuracy: 0.4912 - loss: 1.6396 - val_accuracy: 0.4702 - val_loss: 2.9893 - learning_rate: 9.5083e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.4641 - loss: 1.6826\n",
      "Epoch 10: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 254ms/step - accuracy: 0.4660 - loss: 1.6799 - val_accuracy: 0.4940 - val_loss: 2.0152 - learning_rate: 9.3195e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.5193 - loss: 1.5229\n",
      "Epoch 11: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 251ms/step - accuracy: 0.5187 - loss: 1.5266 - val_accuracy: 0.4464 - val_loss: 2.1084 - learning_rate: 9.0914e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.5117 - loss: 1.5141\n",
      "Epoch 12: val_accuracy improved from 0.50000 to 0.54167, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_enhanced_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 264ms/step - accuracy: 0.5108 - loss: 1.5171 - val_accuracy: 0.5417 - val_loss: 1.9137 - learning_rate: 8.8227e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.4827 - loss: 1.5780\n",
      "Epoch 13: val_accuracy improved from 0.54167 to 0.60119, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_enhanced_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 265ms/step - accuracy: 0.4849 - loss: 1.5762 - val_accuracy: 0.6012 - val_loss: 1.5215 - learning_rate: 8.5129e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.5507 - loss: 1.4595\n",
      "Epoch 14: val_accuracy did not improve from 0.60119\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 246ms/step - accuracy: 0.5494 - loss: 1.4616 - val_accuracy: 0.5774 - val_loss: 1.5143 - learning_rate: 8.1628e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.5234 - loss: 1.5513\n",
      "Epoch 15: val_accuracy did not improve from 0.60119\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 250ms/step - accuracy: 0.5232 - loss: 1.5493 - val_accuracy: 0.5774 - val_loss: 1.6817 - learning_rate: 7.7744e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step - accuracy: 0.5667 - loss: 1.4053\n",
      "Epoch 16: val_accuracy did not improve from 0.60119\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 244ms/step - accuracy: 0.5653 - loss: 1.4095 - val_accuracy: 0.5595 - val_loss: 1.9027 - learning_rate: 7.3507e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.5621 - loss: 1.4779\n",
      "Epoch 17: val_accuracy improved from 0.60119 to 0.62500, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_enhanced_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 258ms/step - accuracy: 0.5609 - loss: 1.4789 - val_accuracy: 0.6250 - val_loss: 1.4571 - learning_rate: 6.8961e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.5805 - loss: 1.4818\n",
      "Epoch 18: val_accuracy did not improve from 0.62500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 257ms/step - accuracy: 0.5798 - loss: 1.4809 - val_accuracy: 0.5714 - val_loss: 1.4039 - learning_rate: 6.4159e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.5321 - loss: 1.5136\n",
      "Epoch 19: val_accuracy did not improve from 0.62500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 265ms/step - accuracy: 0.5326 - loss: 1.5151 - val_accuracy: 0.5833 - val_loss: 1.4328 - learning_rate: 5.9165e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.5341 - loss: 1.4538\n",
      "Epoch 20: val_accuracy did not improve from 0.62500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 266ms/step - accuracy: 0.5348 - loss: 1.4540 - val_accuracy: 0.5774 - val_loss: 1.4382 - learning_rate: 5.4050e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 261ms/step - accuracy: 0.5108 - loss: 1.4448\n",
      "Epoch 21: val_accuracy did not improve from 0.62500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 286ms/step - accuracy: 0.5133 - loss: 1.4448 - val_accuracy: 0.5774 - val_loss: 1.3384 - learning_rate: 4.8889e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.6008 - loss: 1.3744\n",
      "Epoch 22: val_accuracy did not improve from 0.62500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 257ms/step - accuracy: 0.5989 - loss: 1.3760 - val_accuracy: 0.6071 - val_loss: 1.3145 - learning_rate: 4.3759e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.6003 - loss: 1.3899\n",
      "Epoch 23: val_accuracy did not improve from 0.62500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 258ms/step - accuracy: 0.6000 - loss: 1.3902 - val_accuracy: 0.6190 - val_loss: 1.3377 - learning_rate: 3.8738e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.5577 - loss: 1.4175\n",
      "Epoch 24: val_accuracy did not improve from 0.62500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 258ms/step - accuracy: 0.5591 - loss: 1.4176 - val_accuracy: 0.6012 - val_loss: 1.3375 - learning_rate: 3.3898e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.5539 - loss: 1.4299\n",
      "Epoch 25: val_accuracy did not improve from 0.62500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 265ms/step - accuracy: 0.5545 - loss: 1.4288 - val_accuracy: 0.6131 - val_loss: 1.3419 - learning_rate: 2.9304e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.5658 - loss: 1.3943\n",
      "Epoch 26: val_accuracy did not improve from 0.62500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 245ms/step - accuracy: 0.5659 - loss: 1.3933 - val_accuracy: 0.6131 - val_loss: 1.3458 - learning_rate: 2.5013e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.5707 - loss: 1.3484\n",
      "Epoch 27: val_accuracy did not improve from 0.62500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 269ms/step - accuracy: 0.5717 - loss: 1.3491 - val_accuracy: 0.6250 - val_loss: 1.3196 - learning_rate: 2.1068e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.5555 - loss: 1.3958\n",
      "Epoch 28: val_accuracy improved from 0.62500 to 0.64286, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_enhanced_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 264ms/step - accuracy: 0.5574 - loss: 1.3922 - val_accuracy: 0.6429 - val_loss: 1.3036 - learning_rate: 1.7500e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.6351 - loss: 1.2772\n",
      "Epoch 29: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 254ms/step - accuracy: 0.6347 - loss: 1.2785 - val_accuracy: 0.6369 - val_loss: 1.3143 - learning_rate: 1.4327e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 258ms/step - accuracy: 0.5991 - loss: 1.3374\n",
      "Epoch 30: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 282ms/step - accuracy: 0.5999 - loss: 1.3392 - val_accuracy: 0.6131 - val_loss: 1.3239 - learning_rate: 1.1554e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.5954 - loss: 1.3543\n",
      "Epoch 31: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 248ms/step - accuracy: 0.5951 - loss: 1.3529 - val_accuracy: 0.6190 - val_loss: 1.3183 - learning_rate: 9.1729e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.6009 - loss: 1.3312\n",
      "Epoch 32: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 259ms/step - accuracy: 0.6003 - loss: 1.3336 - val_accuracy: 0.6190 - val_loss: 1.3089 - learning_rate: 7.1644e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step - accuracy: 0.5973 - loss: 1.3512\n",
      "Epoch 33: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 242ms/step - accuracy: 0.5978 - loss: 1.3524 - val_accuracy: 0.6250 - val_loss: 1.3043 - learning_rate: 5.5017e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.5739 - loss: 1.3559\n",
      "Epoch 34: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 250ms/step - accuracy: 0.5741 - loss: 1.3551 - val_accuracy: 0.6310 - val_loss: 1.3071 - learning_rate: 4.1511e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.6256 - loss: 1.3198\n",
      "Epoch 35: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 254ms/step - accuracy: 0.6261 - loss: 1.3199 - val_accuracy: 0.6369 - val_loss: 1.3084 - learning_rate: 3.0755e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.6036 - loss: 1.3400\n",
      "Epoch 36: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.6037 - loss: 1.3394 - val_accuracy: 0.6310 - val_loss: 1.3089 - learning_rate: 2.2359e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step - accuracy: 0.6153 - loss: 1.3333\n",
      "Epoch 37: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 269ms/step - accuracy: 0.6135 - loss: 1.3374 - val_accuracy: 0.6310 - val_loss: 1.3068 - learning_rate: 1.5939e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.5855 - loss: 1.3171\n",
      "Epoch 38: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 256ms/step - accuracy: 0.5862 - loss: 1.3169 - val_accuracy: 0.6250 - val_loss: 1.3053 - learning_rate: 1.1135e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 252ms/step - accuracy: 0.6192 - loss: 1.3709\n",
      "Epoch 39: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 276ms/step - accuracy: 0.6194 - loss: 1.3697 - val_accuracy: 0.6250 - val_loss: 1.3050 - learning_rate: 7.6168e-06\n",
      "Epoch 40/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.5723 - loss: 1.3493\n",
      "Epoch 40: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 276ms/step - accuracy: 0.5740 - loss: 1.3467 - val_accuracy: 0.6250 - val_loss: 1.3065 - learning_rate: 5.0985e-06\n",
      "Epoch 41/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.5778 - loss: 1.4152\n",
      "Epoch 41: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 274ms/step - accuracy: 0.5785 - loss: 1.4144 - val_accuracy: 0.6250 - val_loss: 1.3084 - learning_rate: 3.3370e-06\n",
      "Epoch 42/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.6212 - loss: 1.3172\n",
      "Epoch 42: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 272ms/step - accuracy: 0.6200 - loss: 1.3196 - val_accuracy: 0.6310 - val_loss: 1.3091 - learning_rate: 2.1340e-06\n",
      "Epoch 43/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.5905 - loss: 1.3121\n",
      "Epoch 43: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 257ms/step - accuracy: 0.5904 - loss: 1.3134 - val_accuracy: 0.6310 - val_loss: 1.3100 - learning_rate: 1.3323e-06\n",
      "Epoch 44/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 222ms/step - accuracy: 0.6208 - loss: 1.2722\n",
      "Epoch 44: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 244ms/step - accuracy: 0.6196 - loss: 1.2744 - val_accuracy: 0.6310 - val_loss: 1.3097 - learning_rate: 8.1149e-07\n",
      "Epoch 45/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.6272 - loss: 1.2959\n",
      "Epoch 45: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 249ms/step - accuracy: 0.6273 - loss: 1.2956 - val_accuracy: 0.6310 - val_loss: 1.3095 - learning_rate: 4.8178e-07\n",
      "Epoch 46/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.6161 - loss: 1.3094\n",
      "Epoch 46: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 259ms/step - accuracy: 0.6163 - loss: 1.3098 - val_accuracy: 0.6310 - val_loss: 1.3109 - learning_rate: 2.7857e-07\n",
      "Epoch 47/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.6506 - loss: 1.2976\n",
      "Epoch 47: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 261ms/step - accuracy: 0.6487 - loss: 1.3005 - val_accuracy: 0.6250 - val_loss: 1.3117 - learning_rate: 1.5674e-07\n",
      "Epoch 48/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 243ms/step - accuracy: 0.6545 - loss: 1.2951\n",
      "Epoch 48: val_accuracy did not improve from 0.64286\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 266ms/step - accuracy: 0.6531 - loss: 1.2954 - val_accuracy: 0.6250 - val_loss: 1.3112 - learning_rate: 8.5747e-08\n",
      "Epoch 48: early stopping\n",
      "Restoring model weights from the end of the best epoch: 28.\n",
      "\n",
      "âœ“ Training complete in 3.7 minutes\n",
      "\n",
      "ğŸ“Š Evaluating...\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š EXPERIMENT 2 ENHANCED RESULTS\n",
      "================================================================================\n",
      "\n",
      "Final Metrics:\n",
      "  Training Accuracy:   62.81%\n",
      "  Validation Accuracy: 64.29%\n",
      "  Best Val Accuracy:   64.29%\n",
      "  Validation Loss:     1.3036\n",
      "  Train-Val Gap:       -1.47pp\n",
      "  Epochs Trained:      48/100\n",
      "  Training Time:       3.7 minutes\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ˆ COMPLETE EXPERIMENT HISTORY\n",
      "================================================================================\n",
      "\n",
      "    Experiment Frozen     LR  Epochs Augmentation LR Schedule Label Smooth Val Acc (%) Improvement\n",
      "      Baseline    60% 0.0001      50        Basic    ReduceLR           No       55.36           â€”\n",
      "         Exp 1     0% 0.0001      50        Basic    ReduceLR           No       58.93     +3.57pp\n",
      "Exp 2 Enhanced     0%  0.001      48       Strong      Cosine          Yes       64.29     +8.93pp\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ HYPOTHESIS EVALUATION\n",
      "================================================================================\n",
      "âš ï¸  MODERATE SUCCESS: Some improvement (60-65%)\n",
      "   Optimizations helped but not enough\n",
      "\n",
      "================================================================================\n",
      "ğŸ” DETAILED TRAINING ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. MODEL CAPACITY:\n",
      "   âŒ POOR: Model can't fit training data (62.81%)\n",
      "\n",
      "2. GENERALIZATION:\n",
      "   âœ“ EXCELLENT: -1.5pp gap (<5pp)\n",
      "      Very good generalization!\n",
      "\n",
      "3. CONVERGENCE:\n",
      "   âœ“ Early stopped at epoch 48\n",
      "      Model converged before max epochs\n",
      "\n",
      "4. LEARNING RATE STRATEGY:\n",
      "   âœ“ Started at: 0.001\n",
      "   âœ“ Ended at: 0.000547\n",
      "   âœ“ Decay: 45.3%\n",
      "   âœ“ Cosine annealing provided smooth decay\n",
      "\n",
      "ğŸ’¾ Saving results...\n",
      "\n",
      "ğŸ“Š Creating comprehensive visualizations...\n",
      "âœ“ Comprehensive visualizations saved\n",
      "\n",
      "================================================================================\n",
      "âœ… EXPERIMENT 2 ENHANCED COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ FINAL RESULTS:\n",
      "  Status: MODERATE\n",
      "  Validation Accuracy: 64.29%\n",
      "  Improvement from Baseline: +8.93pp\n",
      "  Training Time: 3.7 minutes\n",
      "\n",
      "ğŸ“ Results saved to: /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_enhanced_results/\n",
      "  - experiment_summary.csv\n",
      "  - training_history.csv\n",
      "  - all_experiments_comparison.csv\n",
      "  - comprehensive_analysis.png\n",
      "  - lr_schedule.png\n",
      "\n",
      "ğŸ’¾ Best model saved:\n",
      "  /Users/vishal/Desktop/ML study/project/CNN type/first/experiment2_enhanced_model.keras\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ KEY IMPROVEMENTS APPLIED\n",
      "================================================================================\n",
      "  âœ“ Learning Rate: 10x increase (0.001)\n",
      "  âœ“ LR Schedule: Cosine Annealing (smooth decay)\n",
      "  âœ“ Augmentation: Enhanced (stronger transforms)\n",
      "  âœ“ Label Smoothing: 0.1 (regularization)\n",
      "  âœ“ Epochs: 100 (double baseline)\n",
      "\n",
      "âš ï¸  Below expectations. Possible issues:\n",
      "  1. Data quality: Check if labels are correct\n",
      "  2. Domain shift: Fruit-360 pre-training may hurt\n",
      "  3. Dataset size: 800 images may be insufficient\n",
      "\n",
      "ğŸ’¡ NEXT STEPS:\n",
      "  1. â†’ Try training from scratch (no pre-training)\n",
      "  2. â†’ Or: Collect more curated data\n",
      "  3. â†’ Compare with Experiment 3 (architecture comparison)\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ EXPERIMENT 2 ENHANCED COMPLETED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EXPERIMENT 2 ENHANCED: Maximize PyramidNet-18 Performance\n",
    "\n",
    "All improvements combined:\n",
    "1. Higher Learning Rate: 0.001 (10x increase)\n",
    "2. Stronger Data Augmentation: Enhanced transforms\n",
    "3. Cosine Annealing LR Schedule: Better than ReduceLROnPlateau\n",
    "4. Label Smoothing: Helps with limited data\n",
    "5. More Epochs: 100 (double from baseline)\n",
    "\n",
    "Goal: Push PyramidNet-18 to 70-80% validation accuracy\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "PROJECT_ROOT = '/Users/vishal/Desktop/ML study/project/CNN type/first'\n",
    "\n",
    "CURATED_DATA = os.path.join(PROJECT_ROOT, 'data set fine tune')\n",
    "PRETRAIN_MODEL_PATH = os.path.join(PROJECT_ROOT, 'pyramidnet18_pretrained.keras')\n",
    "\n",
    "# Enhanced Experiment 2 outputs\n",
    "EXP2_ENH_MODEL_PATH = os.path.join(PROJECT_ROOT, 'experiment2_enhanced_model.keras')\n",
    "EXP2_ENH_RESULTS_PATH = os.path.join(PROJECT_ROOT, 'experiment2_enhanced_results')\n",
    "\n",
    "os.makedirs(EXP2_ENH_RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "# Enhanced parameters\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100  # Doubled!\n",
    "INITIAL_LEARNING_RATE = 0.001  # 10x increase\n",
    "LABEL_SMOOTHING = 0.1  # New!\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \"*15 + \"EXPERIMENT 2 ENHANCED: ALL OPTIMIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“Š BASELINE:\")\n",
    "print(\"  Frozen: 60%, LR=0.0001, 50 epochs, Basic augmentation\")\n",
    "print(\"  Result: 55.36% val accuracy\")\n",
    "\n",
    "print(\"\\nğŸ“Š EXPERIMENT 1:\")\n",
    "print(\"  Frozen: 0%, LR=0.0001, 50 epochs\")\n",
    "print(\"  Result: 58.93% val accuracy (+3.57pp)\")\n",
    "print(\"  Issue: Training acc 53%, LR too low\")\n",
    "\n",
    "print(\"\\nğŸš€ EXPERIMENT 2 ENHANCED (All improvements combined):\")\n",
    "print(\"  âœ“ Frozen: 0% (all trainable)\")\n",
    "print(\"  âœ“ Learning Rate: 0.001 (10x INCREASE)\")\n",
    "print(\"  âœ“ LR Schedule: Cosine Annealing (smooth decay)\")\n",
    "print(\"  âœ“ Data Augmentation: STRONGER (more transforms)\")\n",
    "print(\"  âœ“ Label Smoothing: 0.1 (regularization)\")\n",
    "print(\"  âœ“ Max Epochs: 100 (2x more time)\")\n",
    "print(\"\\nğŸ¯ Target: 70-80% validation accuracy\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================\n",
    "# STRONGER DATA AUGMENTATION\n",
    "# ============================================\n",
    "print(\"\\nğŸ“ Loading data with STRONGER augmentation...\")\n",
    "\n",
    "# ENHANCED augmentation (more aggressive)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,  # Increased from 20\n",
    "    width_shift_range=0.2,  # Increased from 0.15\n",
    "    height_shift_range=0.2,  # Increased from 0.15\n",
    "    shear_range=0.15,  # NEW: Shear transformation\n",
    "    zoom_range=0.3,  # Increased from 0.2\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.6, 1.4],  # Wider range (was 0.7-1.3)\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.25\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.25\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    CURATED_DATA,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    CURATED_DATA,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded:\")\n",
    "print(f\"  Training:   {train_generator.samples} images\")\n",
    "print(f\"  Validation: {val_generator.samples} images\")\n",
    "print(f\"  Classes:    {num_classes}\")\n",
    "\n",
    "print(f\"\\nâœ“ ENHANCED Augmentation:\")\n",
    "print(f\"  Rotation: Â±30Â° (was Â±20Â°)\")\n",
    "print(f\"  Shift: Â±20% (was Â±15%)\")\n",
    "print(f\"  Shear: Â±15Â° (NEW)\")\n",
    "print(f\"  Zoom: Â±30% (was Â±20%)\")\n",
    "print(f\"  Brightness: 0.6-1.4 (was 0.7-1.3)\")\n",
    "\n",
    "# ============================================\n",
    "# PYRAMIDNET ARCHITECTURE\n",
    "# ============================================\n",
    "\n",
    "def pyramidnet_basicblock(x, in_filters, out_filters, stride=1):\n",
    "    \"\"\"PyramidNet Basic Block\"\"\"\n",
    "    shortcut = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(out_filters, 3, strides=stride, padding='same', use_bias=False)(x)\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(out_filters, 3, strides=1, padding='same', use_bias=False)(x)\n",
    "    \n",
    "    if in_filters != out_filters or stride != 1:\n",
    "        shortcut = layers.Conv2D(out_filters, 1, strides=stride, use_bias=False)(shortcut)\n",
    "    \n",
    "    x = layers.Add()([x, shortcut])\n",
    "    return x\n",
    "\n",
    "def build_pyramidnet18(num_classes, alpha=48, input_shape=(224, 224, 3)):\n",
    "    \"\"\"Build PyramidNet-18\"\"\"\n",
    "    num_blocks = 8\n",
    "    add_channels_per_block = alpha / num_blocks\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    start_filters = 16\n",
    "    x = layers.Conv2D(start_filters, 7, strides=2, padding='same', use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "    \n",
    "    current_filters = start_filters\n",
    "    \n",
    "    for stage in range(4):\n",
    "        for block in range(2):\n",
    "            block_idx = stage * 2 + block\n",
    "            out_filters = int(start_filters + add_channels_per_block * (block_idx + 1))\n",
    "            stride = 2 if (block == 0 and stage > 0) else 1\n",
    "            x = pyramidnet_basicblock(x, int(current_filters), out_filters, stride=stride)\n",
    "            current_filters = out_filters\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='PyramidNet18')\n",
    "    return model\n",
    "\n",
    "# ============================================\n",
    "# LOAD PRE-TRAINED MODEL\n",
    "# ============================================\n",
    "print(\"\\nğŸ”§ Loading pre-trained model...\")\n",
    "\n",
    "pretrained_model = keras.models.load_model(PRETRAIN_MODEL_PATH)\n",
    "model = build_pyramidnet18(num_classes=num_classes, alpha=48)\n",
    "\n",
    "print(\"ğŸ“¦ Transferring pre-trained weights...\")\n",
    "for pretrained_layer, new_layer in zip(pretrained_model.layers[:-1], model.layers[:-1]):\n",
    "    if len(pretrained_layer.get_weights()) > 0:\n",
    "        new_layer.set_weights(pretrained_layer.get_weights())\n",
    "\n",
    "print(\"âœ“ Pre-trained weights transferred\")\n",
    "\n",
    "# ============================================\n",
    "# UNFREEZE ALL LAYERS\n",
    "# ============================================\n",
    "print(\"\\nğŸ”“ Unfreezing all layers...\")\n",
    "\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "total_layers = len(model.layers)\n",
    "trainable_layers = sum([1 for layer in model.layers if layer.trainable])\n",
    "\n",
    "print(f\"âœ“ Total layers: {total_layers}\")\n",
    "print(f\"âœ“ Trainable: {trainable_layers} (100%)\")\n",
    "print(f\"âœ“ Frozen: 0 (0%)\")\n",
    "\n",
    "# ============================================\n",
    "# COSINE ANNEALING LR SCHEDULE\n",
    "# ============================================\n",
    "\n",
    "def cosine_annealing_schedule(epoch, initial_lr=INITIAL_LEARNING_RATE, epochs=EPOCHS):\n",
    "    \"\"\"\n",
    "    Cosine annealing learning rate schedule.\n",
    "    Smoothly decreases LR from initial_lr to 0 following a cosine curve.\n",
    "    Better than ReduceLROnPlateau - provides smooth, predictable decay.\n",
    "    \"\"\"\n",
    "    lr = initial_lr * 0.5 * (1 + np.cos(np.pi * epoch / epochs))\n",
    "    return lr\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âš™ï¸  COMPILING WITH ENHANCED SETTINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compile with LABEL SMOOTHING\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=INITIAL_LEARNING_RATE),\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING),  # NEW!\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Optimizer: Adam\")\n",
    "print(f\"âœ“ Initial Learning Rate: {INITIAL_LEARNING_RATE} (10x from baseline)\")\n",
    "print(f\"âœ“ LR Schedule: Cosine Annealing (smooth decay over {EPOCHS} epochs)\")\n",
    "print(f\"âœ“ Label Smoothing: {LABEL_SMOOTHING} (regularization)\")\n",
    "print(f\"âœ“ Loss: Categorical Crossentropy with label smoothing\")\n",
    "\n",
    "# Visualize LR schedule\n",
    "epochs_range = list(range(EPOCHS))\n",
    "lrs = [cosine_annealing_schedule(e) for e in epochs_range]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(epochs_range, lrs, linewidth=2, color='#3498DB')\n",
    "plt.title('Cosine Annealing Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(EXP2_ENH_RESULTS_PATH, 'lr_schedule.png'), dpi=300)\n",
    "plt.close()\n",
    "print(f\"\\nâœ“ LR schedule visualization saved\")\n",
    "\n",
    "# ============================================\n",
    "# CALLBACKS\n",
    "# ============================================\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,  # More patience for 100 epochs\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    LearningRateScheduler(cosine_annealing_schedule, verbose=0),  # Cosine annealing!\n",
    "    ModelCheckpoint(\n",
    "        EXP2_ENH_MODEL_PATH,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# ============================================\n",
    "# TRAIN\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸš€ STARTING EXPERIMENT 2 ENHANCED TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"Improvements:\")\n",
    "print(f\"  âœ“ Learning rate: 10x increase (0.001)\")\n",
    "print(f\"  âœ“ Cosine annealing: Smooth LR decay\")\n",
    "print(f\"  âœ“ Stronger augmentation: More transforms\")\n",
    "print(f\"  âœ“ Label smoothing: 0.1\")\n",
    "print(f\"  âœ“ More epochs: 100 (vs 50 baseline)\")\n",
    "print(f\"\\nExpected training time: 60-90 minutes\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ“ Training complete in {training_time/60:.1f} minutes\")\n",
    "\n",
    "# ============================================\n",
    "# EVALUATE\n",
    "# ============================================\n",
    "print(\"\\nğŸ“Š Evaluating...\")\n",
    "\n",
    "val_loss, val_accuracy = model.evaluate(val_generator, verbose=0)\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "epochs_trained = len(history.history['accuracy'])\n",
    "train_val_gap = final_train_acc - val_accuracy\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š EXPERIMENT 2 ENHANCED RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "print(f\"  Training Accuracy:   {final_train_acc*100:.2f}%\")\n",
    "print(f\"  Validation Accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"  Best Val Accuracy:   {best_val_acc*100:.2f}%\")\n",
    "print(f\"  Validation Loss:     {val_loss:.4f}\")\n",
    "print(f\"  Train-Val Gap:       {train_val_gap*100:.2f}pp\")\n",
    "print(f\"  Epochs Trained:      {epochs_trained}/{EPOCHS}\")\n",
    "print(f\"  Training Time:       {training_time/60:.1f} minutes\")\n",
    "\n",
    "# ============================================\n",
    "# COMPREHENSIVE COMPARISON\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“ˆ COMPLETE EXPERIMENT HISTORY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_val = 55.36\n",
    "exp1_val = 58.93\n",
    "exp2_enh_val = val_accuracy * 100\n",
    "\n",
    "improvement_from_baseline = exp2_enh_val - baseline_val\n",
    "improvement_from_exp1 = exp2_enh_val - exp1_val\n",
    "\n",
    "experiments_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Experiment': 'Baseline',\n",
    "        'Frozen': '60%',\n",
    "        'LR': '0.0001',\n",
    "        'Epochs': 50,\n",
    "        'Augmentation': 'Basic',\n",
    "        'LR Schedule': 'ReduceLR',\n",
    "        'Label Smooth': 'No',\n",
    "        'Val Acc (%)': baseline_val,\n",
    "        'Improvement': 'â€”'\n",
    "    },\n",
    "    {\n",
    "        'Experiment': 'Exp 1',\n",
    "        'Frozen': '0%',\n",
    "        'LR': '0.0001',\n",
    "        'Epochs': 50,\n",
    "        'Augmentation': 'Basic',\n",
    "        'LR Schedule': 'ReduceLR',\n",
    "        'Label Smooth': 'No',\n",
    "        'Val Acc (%)': exp1_val,\n",
    "        'Improvement': f'+{exp1_val - baseline_val:.2f}pp'\n",
    "    },\n",
    "    {\n",
    "        'Experiment': 'Exp 2 Enhanced',\n",
    "        'Frozen': '0%',\n",
    "        'LR': '0.001',\n",
    "        'Epochs': epochs_trained,\n",
    "        'Augmentation': 'Strong',\n",
    "        'LR Schedule': 'Cosine',\n",
    "        'Label Smooth': 'Yes',\n",
    "        'Val Acc (%)': f'{exp2_enh_val:.2f}',\n",
    "        'Improvement': f'+{improvement_from_baseline:.2f}pp'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + experiments_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¯ HYPOTHESIS EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if exp2_enh_val >= 80:\n",
    "    print(\"ğŸ‰ OUTSTANDING: Exceeded stretch goal (80%+)!\")\n",
    "    print(\"   All optimizations worked synergistically!\")\n",
    "    status = \"EXCELLENT\"\n",
    "elif exp2_enh_val >= 75:\n",
    "    print(\"âœ… EXCELLENT: High performance (75-80%)!\")\n",
    "    print(\"   Combined improvements were highly effective!\")\n",
    "    status = \"EXCELLENT\"\n",
    "elif exp2_enh_val >= 70:\n",
    "    print(\"âœ… HYPOTHESIS CONFIRMED: Achieved target (70-80%)!\")\n",
    "    print(\"   Multiple optimizations successfully boosted performance!\")\n",
    "    status = \"SUCCESS\"\n",
    "elif exp2_enh_val >= 65:\n",
    "    print(\"âœ“ GOOD PROGRESS: Approaching target (65-70%)\")\n",
    "    print(\"   Improvements helped significantly\")\n",
    "    status = \"GOOD\"\n",
    "elif exp2_enh_val >= 60:\n",
    "    print(\"âš ï¸  MODERATE SUCCESS: Some improvement (60-65%)\")\n",
    "    print(\"   Optimizations helped but not enough\")\n",
    "    status = \"MODERATE\"\n",
    "else:\n",
    "    print(\"âŒ LIMITED SUCCESS: Below expectations (<60%)\")\n",
    "    print(\"   May need to investigate data quality issues\")\n",
    "    status = \"LIMITED\"\n",
    "\n",
    "# ============================================\n",
    "# DETAILED ANALYSIS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ” DETAILED TRAINING ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Training capability\n",
    "print(f\"\\n1. MODEL CAPACITY:\")\n",
    "if final_train_acc >= 0.85:\n",
    "    print(f\"   âœ… EXCELLENT: Model fits training data well ({final_train_acc*100:.2f}%)\")\n",
    "elif final_train_acc >= 0.75:\n",
    "    print(f\"   âœ“ GOOD: Model learning training data ({final_train_acc*100:.2f}%)\")\n",
    "elif final_train_acc >= 0.65:\n",
    "    print(f\"   âš ï¸  MODERATE: Model struggles with training data ({final_train_acc*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"   âŒ POOR: Model can't fit training data ({final_train_acc*100:.2f}%)\")\n",
    "\n",
    "# Overfitting analysis\n",
    "print(f\"\\n2. GENERALIZATION:\")\n",
    "if train_val_gap > 0.20:\n",
    "    print(f\"   âŒ HIGH OVERFITTING: {train_val_gap*100:.1f}pp gap (>20pp)\")\n",
    "    print(\"      Solution: More data, stronger regularization\")\n",
    "elif train_val_gap > 0.15:\n",
    "    print(f\"   âš ï¸  MODERATE OVERFITTING: {train_val_gap*100:.1f}pp gap (15-20pp)\")\n",
    "    print(\"      Acceptable but could improve\")\n",
    "elif train_val_gap > 0.05:\n",
    "    print(f\"   âœ“ HEALTHY GAP: {train_val_gap*100:.1f}pp gap (5-15pp)\")\n",
    "    print(\"      Good generalization!\")\n",
    "else:\n",
    "    print(f\"   âœ“ EXCELLENT: {train_val_gap*100:.1f}pp gap (<5pp)\")\n",
    "    print(\"      Very good generalization!\")\n",
    "\n",
    "# Convergence\n",
    "print(f\"\\n3. CONVERGENCE:\")\n",
    "if epochs_trained < EPOCHS:\n",
    "    print(f\"   âœ“ Early stopped at epoch {epochs_trained}\")\n",
    "    print(\"      Model converged before max epochs\")\n",
    "else:\n",
    "    last_10_val = history.history['val_accuracy'][-10:]\n",
    "    improvement_last_10 = max(last_10_val) - min(last_10_val)\n",
    "    if improvement_last_10 < 0.01:\n",
    "        print(f\"   âœ“ Converged at epoch {EPOCHS}\")\n",
    "        print(\"      Validation accuracy plateaued\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  May need more epochs\")\n",
    "        print(f\"      Still improving in last 10 epochs (+{improvement_last_10*100:.1f}%)\")\n",
    "\n",
    "# Learning rate analysis\n",
    "print(f\"\\n4. LEARNING RATE STRATEGY:\")\n",
    "print(f\"   âœ“ Started at: {INITIAL_LEARNING_RATE}\")\n",
    "final_lr = cosine_annealing_schedule(epochs_trained - 1)\n",
    "print(f\"   âœ“ Ended at: {final_lr:.6f}\")\n",
    "print(f\"   âœ“ Decay: {(1 - final_lr/INITIAL_LEARNING_RATE)*100:.1f}%\")\n",
    "print(\"   âœ“ Cosine annealing provided smooth decay\")\n",
    "\n",
    "# ============================================\n",
    "# SAVE RESULTS\n",
    "# ============================================\n",
    "print(\"\\nğŸ’¾ Saving results...\")\n",
    "\n",
    "# Save history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df['learning_rate'] = [cosine_annealing_schedule(e) for e in range(len(history_df))]\n",
    "history_df.to_csv(os.path.join(EXP2_ENH_RESULTS_PATH, 'training_history.csv'), index=False)\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'experiment': 'Experiment 2 Enhanced: All Optimizations',\n",
    "    'status': status,\n",
    "    'baseline_val_acc': baseline_val,\n",
    "    'exp1_val_acc': exp1_val,\n",
    "    'exp2_enh_val_acc': exp2_enh_val,\n",
    "    'improvement_from_baseline': improvement_from_baseline,\n",
    "    'improvement_from_exp1': improvement_from_exp1,\n",
    "    'frozen_layers': 0,\n",
    "    'initial_learning_rate': INITIAL_LEARNING_RATE,\n",
    "    'lr_schedule': 'Cosine Annealing',\n",
    "    'label_smoothing': LABEL_SMOOTHING,\n",
    "    'augmentation': 'Enhanced (rotation=30, shift=20%, shear=15%, zoom=30%)',\n",
    "    'max_epochs': EPOCHS,\n",
    "    'epochs_trained': epochs_trained,\n",
    "    'training_time_min': training_time/60,\n",
    "    'final_train_acc': final_train_acc * 100,\n",
    "    'final_val_acc': val_accuracy * 100,\n",
    "    'best_val_acc': best_val_acc * 100,\n",
    "    'train_val_gap': train_val_gap * 100\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df.to_csv(os.path.join(EXP2_ENH_RESULTS_PATH, 'experiment_summary.csv'), index=False)\n",
    "\n",
    "# Save experiments comparison\n",
    "experiments_summary.to_csv(os.path.join(EXP2_ENH_RESULTS_PATH, 'all_experiments_comparison.csv'), index=False)\n",
    "\n",
    "# ============================================\n",
    "# COMPREHENSIVE VISUALIZATIONS\n",
    "# ============================================\n",
    "print(\"\\nğŸ“Š Creating comprehensive visualizations...\")\n",
    "\n",
    "fig = plt.figure(figsize=(20, 14))\n",
    "gs = fig.add_gridspec(4, 3, hspace=0.35, wspace=0.3)\n",
    "\n",
    "# 1. Experiment progression\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "experiments = ['Baseline\\n(60% frz, 0.0001, 50ep)', \n",
    "               'Exp 1\\n(0% frz, 0.0001, 50ep)', \n",
    "               'Exp 2 Enhanced\\n(0% frz, 0.001, 100ep)']\n",
    "accuracies = [baseline_val, exp1_val, exp2_enh_val]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#2ECC71']\n",
    "\n",
    "bars = ax1.bar(experiments, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax1.axhline(y=70, color='green', linestyle='--', label='Target (70%)', linewidth=2)\n",
    "ax1.axhline(y=80, color='darkgreen', linestyle='--', label='Stretch (80%)', linewidth=2)\n",
    "ax1.set_ylabel('Validation Accuracy (%)', fontweight='bold', fontsize=13)\n",
    "ax1.set_title('Complete Experiment Progression', fontsize=18, fontweight='bold')\n",
    "ax1.set_ylim([0, 100])\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.legend(fontsize=11)\n",
    "\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, height + 2,\n",
    "             f'{acc:.1f}%', ha='center', fontweight='bold', fontsize=13)\n",
    "\n",
    "# 2. Accuracy curves\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.plot(history.history['accuracy'], label='Train', linewidth=2.5, color='#3498DB')\n",
    "ax2.plot(history.history['val_accuracy'], label='Val', linewidth=2.5, color='#E74C3C')\n",
    "ax2.axhline(y=baseline_val/100, color='gray', linestyle='--', alpha=0.5, label='Baseline')\n",
    "ax2.set_title('Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Loss curves\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "ax3.plot(history.history['loss'], label='Train', linewidth=2.5, color='#3498DB')\n",
    "ax3.plot(history.history['val_loss'], label='Val', linewidth=2.5, color='#E74C3C')\n",
    "ax3.set_title('Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Learning rate schedule\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "lrs_actual = [cosine_annealing_schedule(e) for e in range(epochs_trained)]\n",
    "ax4.plot(lrs_actual, linewidth=2.5, color='#9B59B6')\n",
    "ax4.set_title('Cosine Annealing LR Schedule', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Learning Rate')\n",
    "ax4.set_yscale('log')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Train-Val gap\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "train_val_gaps = [t - v for t, v in zip(history.history['accuracy'], history.history['val_accuracy'])]\n",
    "ax5.plot(train_val_gaps, linewidth=2.5, color='#E67E22')\n",
    "ax5.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "ax5.axhline(y=0.15, color='red', linestyle='--', label='High overfit threshold', linewidth=2)\n",
    "ax5.axhline(y=0.05, color='green', linestyle='--', label='Healthy threshold', linewidth=2)\n",
    "ax5.set_title('Overfitting Monitor (Train-Val Gap)', fontsize=14, fontweight='bold')\n",
    "ax5.set_xlabel('Epoch')\n",
    "ax5.set_ylabel('Accuracy Gap')\n",
    "ax5.legend(fontsize=9)\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Improvement breakdown\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "improvements = ['Baseline\\nâ†’ Exp 1', 'Exp 1\\nâ†’ Exp 2 Enh']\n",
    "improvement_values = [exp1_val - baseline_val, exp2_enh_val - exp1_val]\n",
    "colors_imp = ['#4ECDC4', '#2ECC71']\n",
    "\n",
    "bars = ax6.bar(improvements, improvement_values, color=colors_imp, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax6.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "ax6.set_ylabel('Accuracy Improvement (pp)', fontweight='bold')\n",
    "ax6.set_title('Incremental Improvements', fontsize=14, fontweight='bold')\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, val in zip(bars, improvement_values):\n",
    "    height = bar.get_height()\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2, height + 0.5 if height > 0 else height - 1,\n",
    "             f'{val:+.2f}pp', ha='center', fontweight='bold')\n",
    "\n",
    "# 7. Validation accuracy smoothed\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "# Moving average for smoothness\n",
    "window = 5\n",
    "val_acc_smooth = pd.Series(history.history['val_accuracy']).rolling(window=window, center=True).mean()\n",
    "ax7.plot(history.history['val_accuracy'], alpha=0.3, color='#E74C3C', label='Raw')\n",
    "ax7.plot(val_acc_smooth, linewidth=2.5, color='#E74C3C', label=f'{window}-epoch MA')\n",
    "ax7.axhline(y=best_val_acc, color='green', linestyle='--', label=f'Best ({best_val_acc*100:.2f}%)')\n",
    "ax7.set_title('Validation Accuracy (Smoothed)', fontsize=14, fontweight='bold')\n",
    "ax7.set_xlabel('Epoch')\n",
    "ax7.set_ylabel('Accuracy')\n",
    "ax7.legend()\n",
    "ax7.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Summary table\n",
    "ax8 = fig.add_subplot(gs[3, :])\n",
    "ax8.axis('tight')\n",
    "ax8.axis('off')\n",
    "\n",
    "table_data = [\n",
    "    ['Metric', 'Baseline', 'Exp 1', 'Exp 2 Enhanced', 'Change vs Baseline'],\n",
    "    ['Val Accuracy', f'{baseline_val:.2f}%', f'{exp1_val:.2f}%', \n",
    "     f'{exp2_enh_val:.2f}%', f'{improvement_from_baseline:+.2f}pp'],\n",
    "    ['Train Accuracy', '50%', '53%', f'{final_train_acc*100:.2f}%', \n",
    "     f'{final_train_acc*100 - 50:+.2f}pp'],\n",
    "    ['Train-Val Gap', 'â€”', 'â€”', f'{train_val_gap*100:.2f}pp', 'â€”'],\n",
    "    ['Frozen Layers', '60%', '0%', '0%', 'â€”'],\n",
    "    ['Learning Rate', '0.0001', '0.0001', '0.001', '10x increase'],\n",
    "    ['LR Schedule', 'ReduceLR', 'ReduceLR', 'Cosine', 'Improved'],\n",
    "    ['Augmentation', 'Basic', 'Basic', 'Enhanced', 'Stronger'],\n",
    "    ['Label Smoothing', 'No', 'No', 'Yes (0.1)', 'Added'],\n",
    "    ['Epochs', '50', '50', f'{epochs_trained}', f'+{epochs_trained-50}'],\n",
    "    ['Training Time', 'â€”', 'â€”', f'{training_time/60:.1f} min', 'â€”']\n",
    "]\n",
    "\n",
    "table = ax8.table(cellText=table_data, cellLoc='left', loc='center',\n",
    "                  colWidths=[0.22, 0.18, 0.18, 0.22, 0.20])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.2)\n",
    "\n",
    "# Style header row\n",
    "for i in range(5):\n",
    "    table[(0, i)].set_facecolor('#2C3E50')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Alternate row colors\n",
    "for i in range(1, len(table_data)):\n",
    "    for j in range(5):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#ECF0F1')\n",
    "\n",
    "# Highlight final results\n",
    "for j in range(5):\n",
    "    table[(1, j)].set_facecolor('#D5F4E6')  # Light green for accuracy row\n",
    "\n",
    "ax8.set_title('Complete Experiment Summary', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "plt.savefig(os.path.join(EXP2_ENH_RESULTS_PATH, 'comprehensive_analysis.png'), \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"âœ“ Comprehensive visualizations saved\")\n",
    "\n",
    "# ============================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… EXPERIMENT 2 ENHANCED COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ¯ FINAL RESULTS:\")\n",
    "print(f\"  Status: {status}\")\n",
    "print(f\"  Validation Accuracy: {exp2_enh_val:.2f}%\")\n",
    "print(f\"  Improvement from Baseline: {improvement_from_baseline:+.2f}pp\")\n",
    "print(f\"  Training Time: {training_time/60:.1f} minutes\")\n",
    "\n",
    "print(f\"\\nğŸ“ Results saved to: {EXP2_ENH_RESULTS_PATH}/\")\n",
    "print(f\"  - experiment_summary.csv\")\n",
    "print(f\"  - training_history.csv\")\n",
    "print(f\"  - all_experiments_comparison.csv\")\n",
    "print(f\"  - comprehensive_analysis.png\")\n",
    "print(f\"  - lr_schedule.png\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Best model saved:\")\n",
    "print(f\"  {EXP2_ENH_MODEL_PATH}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“ KEY IMPROVEMENTS APPLIED\")\n",
    "print(\"=\"*80)\n",
    "print(\"  âœ“ Learning Rate: 10x increase (0.001)\")\n",
    "print(\"  âœ“ LR Schedule: Cosine Annealing (smooth decay)\")\n",
    "print(\"  âœ“ Augmentation: Enhanced (stronger transforms)\")\n",
    "print(\"  âœ“ Label Smoothing: 0.1 (regularization)\")\n",
    "print(\"  âœ“ Epochs: 100 (double baseline)\")\n",
    "\n",
    "if exp2_enh_val >= 70:\n",
    "    print(\"\\nğŸ‰ SUCCESS! Achieved target performance (70%+)\")\n",
    "    print(\"\\nğŸ’¡ NEXT STEPS:\")\n",
    "    print(\"  1. âœ“ PyramidNet-18 is your best model\")\n",
    "    print(\"  2. â†’ Document this approach in your project report\")\n",
    "    print(\"  3. â†’ Use this model for your final application\")\n",
    "    print(\"  4. â†’ Optional: Compare with ViT for CV showcase\")\n",
    "elif exp2_enh_val >= 65:\n",
    "    print(\"\\nâœ“ Good progress! Close to target (65-70%)\")\n",
    "    print(\"\\nğŸ’¡ NEXT STEPS:\")\n",
    "    print(\"  1. â†’ Try even more epochs (150-200)\")\n",
    "    print(\"  2. â†’ Consider collecting more data (target: 1500-2000 images)\")\n",
    "    print(\"  3. â†’ Or: Accept 65-70% as reasonable for 800 images\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Below expectations. Possible issues:\")\n",
    "    print(\"  1. Data quality: Check if labels are correct\")\n",
    "    print(\"  2. Domain shift: Fruit-360 pre-training may hurt\")\n",
    "    print(\"  3. Dataset size: 800 images may be insufficient\")\n",
    "    print(\"\\nğŸ’¡ NEXT STEPS:\")\n",
    "    print(\"  1. â†’ Try training from scratch (no pre-training)\")\n",
    "    print(\"  2. â†’ Or: Collect more curated data\")\n",
    "    print(\"  3. â†’ Compare with Experiment 3 (architecture comparison)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸš€ EXPERIMENT 2 ENHANCED COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39dcd16-5f24-4aec-a857-0c09ce8dee92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef58a514-4ccf-48f5-8039-d31f7f5d982a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96965176-ec14-4d7c-aced-475653ab760d",
   "metadata": {},
   "source": [
    "# EXPERIMENT: ImageNet Pre-training + Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cdc2adf-2bf1-417b-a09a-df7e3c80c1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "               EXPERIMENT: ImageNet Pre-training\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š PREVIOUS RESULTS:\n",
      "  Fruit-360 pre-train â†’ Fine-tune: 55.36% â†’ 64.29% (negative transfer)\n",
      "  From scratch:                    64.29%\n",
      "  Issue: Studio photos â‰  Real-world photos\n",
      "\n",
      "ğŸ¯ THIS EXPERIMENT:\n",
      "  ImageNet pre-train â†’ Fine-tune: ??%\n",
      "  Hypothesis: Real-world pre-training will give POSITIVE transfer\n",
      "  Expected: 70-75% validation accuracy\n",
      "\n",
      "ğŸ’¡ WHY ImageNet Should Work Better:\n",
      "  âœ“ 1.2M+ diverse real-world images\n",
      "  âœ“ Natural lighting, varied backgrounds\n",
      "  âœ“ Same domain as your curated data\n",
      "  âœ“ Standard practice in computer vision\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Loading data with enhanced augmentation...\n",
      "Found 519 images belonging to 8 classes.\n",
      "Found 168 images belonging to 8 classes.\n",
      "\n",
      "âœ“ Data loaded:\n",
      "  Training:   519 images\n",
      "  Validation: 168 images\n",
      "  Classes:    8\n",
      "\n",
      "ğŸ”§ Building PyramidNet-18 with ImageNet pre-training...\n",
      "\n",
      "âš ï¸  Note: Using ResNet18 with ImageNet weights\n",
      "   (PyramidNet-18 doesn't have official ImageNet pre-trained weights)\n",
      "   ResNet18 has similar architecture and performance\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 0us/step\n",
      "âœ“ Loaded ResNet50 with ImageNet weights\n",
      "âœ“ Pre-trained on 1.2M+ diverse images\n",
      "âœ“ Model architecture built\n",
      "âœ“ Base: ResNet50 (ImageNet pre-trained)\n",
      "âœ“ Head: Custom classification layers\n",
      "\n",
      "ğŸ”“ Setting layer freezing strategy...\n",
      "\n",
      "âœ“ Base model layers: 175\n",
      "âœ“ Freezing first: 122 layers (70%)\n",
      "âœ“ Fine-tuning last: 53 layers (30%)\n",
      "\n",
      "âœ“ Total model layers: 182\n",
      "âœ“ Frozen: 122\n",
      "âœ“ Trainable: 60\n",
      "\n",
      "âš™ï¸  Compiling model...\n",
      "âœ“ Optimizer: Adam (LR=0.001)\n",
      "âœ“ Loss: Categorical Crossentropy (Label Smoothing=0.1)\n",
      "âœ“ LR Schedule: Cosine Annealing\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ STARTING ImageNet PRE-TRAINING EXPERIMENT\n",
      "================================================================================\n",
      "Started at: 11:30:04\n",
      "\n",
      "Key Settings:\n",
      "  âœ“ Pre-training: ImageNet (real-world diverse images)\n",
      "  âœ“ Frozen: 70% of base model (keep generic features)\n",
      "  âœ“ Fine-tune: 30% of base + classification head\n",
      "  âœ“ Learning Rate: 0.001 with cosine annealing\n",
      "  âœ“ Label Smoothing: 0.1\n",
      "  âœ“ Strong augmentation\n",
      "  âœ“ Max epochs: 100\n",
      "\n",
      "Expected time: 30-45 minutes\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1337 - loss: 3.0885"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.19643, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_imagenet_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1s/step - accuracy: 0.1353 - loss: 3.0796 - val_accuracy: 0.1964 - val_loss: 29.5888 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1665 - loss: 2.6357  \n",
      "Epoch 2: val_accuracy did not improve from 0.19643\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.1672 - loss: 2.6347 - val_accuracy: 0.1964 - val_loss: 127.2525 - learning_rate: 9.9975e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1764 - loss: 2.4508\n",
      "Epoch 3: val_accuracy did not improve from 0.19643\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.1776 - loss: 2.4443 - val_accuracy: 0.1964 - val_loss: 83.3995 - learning_rate: 9.9877e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.1952 - loss: 2.3381\n",
      "Epoch 4: val_accuracy did not improve from 0.19643\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.1967 - loss: 2.3357 - val_accuracy: 0.1964 - val_loss: 50.9914 - learning_rate: 9.9655e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2996 - loss: 2.1220\n",
      "Epoch 5: val_accuracy did not improve from 0.19643\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2989 - loss: 2.1215 - val_accuracy: 0.1310 - val_loss: 23.6659 - learning_rate: 9.9262e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2833 - loss: 2.1347\n",
      "Epoch 6: val_accuracy did not improve from 0.19643\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2823 - loss: 2.1355 - val_accuracy: 0.1369 - val_loss: 9.0882 - learning_rate: 9.8651e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3206 - loss: 2.0256\n",
      "Epoch 7: val_accuracy did not improve from 0.19643\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.3191 - loss: 2.0287 - val_accuracy: 0.1310 - val_loss: 6.7589 - learning_rate: 9.7777e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3371 - loss: 1.9804\n",
      "Epoch 8: val_accuracy did not improve from 0.19643\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.3364 - loss: 1.9807 - val_accuracy: 0.1310 - val_loss: 8.9476 - learning_rate: 9.6600e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3108 - loss: 2.0158\n",
      "Epoch 9: val_accuracy did not improve from 0.19643\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.3107 - loss: 2.0164 - val_accuracy: 0.1190 - val_loss: 4.8306 - learning_rate: 9.5083e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3447 - loss: 1.9541\n",
      "Epoch 10: val_accuracy did not improve from 0.19643\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.3437 - loss: 1.9557 - val_accuracy: 0.1250 - val_loss: 4.3008 - learning_rate: 9.3195e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 998ms/step - accuracy: 0.2737 - loss: 2.0646\n",
      "Epoch 11: val_accuracy did not improve from 0.19643\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.2739 - loss: 2.0631 - val_accuracy: 0.1905 - val_loss: 2.0071 - learning_rate: 9.0914e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3177 - loss: 1.9721  \n",
      "Epoch 12: val_accuracy did not improve from 0.19643\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.3158 - loss: 1.9721 - val_accuracy: 0.1905 - val_loss: 2.8823 - learning_rate: 8.8227e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3096 - loss: 1.9078\n",
      "Epoch 13: val_accuracy did not improve from 0.19643\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.3100 - loss: 1.9092 - val_accuracy: 0.1964 - val_loss: 3.0507 - learning_rate: 8.5129e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1000ms/step - accuracy: 0.4001 - loss: 1.7094\n",
      "Epoch 14: val_accuracy did not improve from 0.19643\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.3972 - loss: 1.7149 - val_accuracy: 0.1964 - val_loss: 2.7739 - learning_rate: 8.1628e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.2976 - loss: 1.8377 \n",
      "Epoch 15: val_accuracy did not improve from 0.19643\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.2993 - loss: 1.8368 - val_accuracy: 0.1964 - val_loss: 2.5733 - learning_rate: 7.7744e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3138 - loss: 1.8986\n",
      "Epoch 16: val_accuracy did not improve from 0.19643\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.3153 - loss: 1.8968 - val_accuracy: 0.1905 - val_loss: 3.0423 - learning_rate: 7.3507e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3872 - loss: 1.7551 \n",
      "Epoch 17: val_accuracy improved from 0.19643 to 0.21429, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_imagenet_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.3884 - loss: 1.7539 - val_accuracy: 0.2143 - val_loss: 3.1942 - learning_rate: 6.8961e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3650 - loss: 1.7264  \n",
      "Epoch 18: val_accuracy did not improve from 0.21429\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.3655 - loss: 1.7298 - val_accuracy: 0.1964 - val_loss: 2.6740 - learning_rate: 6.4159e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3527 - loss: 1.7832\n",
      "Epoch 19: val_accuracy improved from 0.21429 to 0.23214, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_imagenet_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.3534 - loss: 1.7828 - val_accuracy: 0.2321 - val_loss: 2.3737 - learning_rate: 5.9165e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3961 - loss: 1.8123 \n",
      "Epoch 20: val_accuracy improved from 0.23214 to 0.27976, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_imagenet_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.3959 - loss: 1.8112 - val_accuracy: 0.2798 - val_loss: 1.9767 - learning_rate: 5.4050e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4191 - loss: 1.7107  \n",
      "Epoch 21: val_accuracy did not improve from 0.27976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4199 - loss: 1.7086 - val_accuracy: 0.1964 - val_loss: 4.0864 - learning_rate: 4.8889e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4336 - loss: 1.6240\n",
      "Epoch 22: val_accuracy did not improve from 0.27976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4327 - loss: 1.6258 - val_accuracy: 0.1964 - val_loss: 5.4590 - learning_rate: 4.3759e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3548 - loss: 1.7883\n",
      "Epoch 23: val_accuracy did not improve from 0.27976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.3562 - loss: 1.7867 - val_accuracy: 0.1964 - val_loss: 3.2210 - learning_rate: 3.8738e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4402 - loss: 1.6648\n",
      "Epoch 24: val_accuracy did not improve from 0.27976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4395 - loss: 1.6656 - val_accuracy: 0.1964 - val_loss: 2.8514 - learning_rate: 3.3898e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4132 - loss: 1.6919\n",
      "Epoch 25: val_accuracy did not improve from 0.27976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4135 - loss: 1.6903 - val_accuracy: 0.1964 - val_loss: 2.7460 - learning_rate: 2.9304e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4577 - loss: 1.5420\n",
      "Epoch 26: val_accuracy did not improve from 0.27976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4562 - loss: 1.5467 - val_accuracy: 0.1964 - val_loss: 2.9359 - learning_rate: 2.5013e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4112 - loss: 1.6634\n",
      "Epoch 27: val_accuracy did not improve from 0.27976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4100 - loss: 1.6660 - val_accuracy: 0.1964 - val_loss: 4.3169 - learning_rate: 2.1068e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4194 - loss: 1.6077 \n",
      "Epoch 28: val_accuracy did not improve from 0.27976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4188 - loss: 1.6108 - val_accuracy: 0.2679 - val_loss: 2.9747 - learning_rate: 1.7500e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3715 - loss: 1.6413\n",
      "Epoch 29: val_accuracy did not improve from 0.27976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.3719 - loss: 1.6425 - val_accuracy: 0.2619 - val_loss: 3.0924 - learning_rate: 1.4327e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4549 - loss: 1.5493\n",
      "Epoch 30: val_accuracy did not improve from 0.27976\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4538 - loss: 1.5515 - val_accuracy: 0.2798 - val_loss: 2.6482 - learning_rate: 1.1554e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4673 - loss: 1.5758\n",
      "Epoch 31: val_accuracy improved from 0.27976 to 0.30357, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_imagenet_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4670 - loss: 1.5771 - val_accuracy: 0.3036 - val_loss: 2.1892 - learning_rate: 9.1729e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4335 - loss: 1.6209 \n",
      "Epoch 32: val_accuracy did not improve from 0.30357\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4340 - loss: 1.6203 - val_accuracy: 0.2917 - val_loss: 2.2983 - learning_rate: 7.1644e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4267 - loss: 1.6264\n",
      "Epoch 33: val_accuracy did not improve from 0.30357\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4273 - loss: 1.6262 - val_accuracy: 0.2857 - val_loss: 2.2163 - learning_rate: 5.5017e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4585 - loss: 1.5788\n",
      "Epoch 34: val_accuracy improved from 0.30357 to 0.37500, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_imagenet_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4575 - loss: 1.5784 - val_accuracy: 0.3750 - val_loss: 1.8861 - learning_rate: 4.1511e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5036 - loss: 1.5246\n",
      "Epoch 35: val_accuracy did not improve from 0.37500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.5024 - loss: 1.5281 - val_accuracy: 0.3690 - val_loss: 1.8412 - learning_rate: 3.0755e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4385 - loss: 1.6214 \n",
      "Epoch 36: val_accuracy did not improve from 0.37500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4401 - loss: 1.6195 - val_accuracy: 0.3631 - val_loss: 1.7666 - learning_rate: 2.2359e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4476 - loss: 1.6215\n",
      "Epoch 37: val_accuracy did not improve from 0.37500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4480 - loss: 1.6215 - val_accuracy: 0.3631 - val_loss: 1.7296 - learning_rate: 1.5939e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4147 - loss: 1.7086\n",
      "Epoch 38: val_accuracy did not improve from 0.37500\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4156 - loss: 1.7050 - val_accuracy: 0.3690 - val_loss: 1.6752 - learning_rate: 1.1135e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4886 - loss: 1.5020\n",
      "Epoch 39: val_accuracy improved from 0.37500 to 0.41071, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_imagenet_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4878 - loss: 1.5047 - val_accuracy: 0.4107 - val_loss: 1.6781 - learning_rate: 7.6168e-06\n",
      "Epoch 40/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4609 - loss: 1.5808\n",
      "Epoch 40: val_accuracy did not improve from 0.41071\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4616 - loss: 1.5811 - val_accuracy: 0.4107 - val_loss: 1.6565 - learning_rate: 5.0985e-06\n",
      "Epoch 41/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4343 - loss: 1.6790\n",
      "Epoch 41: val_accuracy improved from 0.41071 to 0.44048, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_imagenet_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4363 - loss: 1.6742 - val_accuracy: 0.4405 - val_loss: 1.6469 - learning_rate: 3.3370e-06\n",
      "Epoch 42/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4725 - loss: 1.5569\n",
      "Epoch 42: val_accuracy did not improve from 0.44048\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4704 - loss: 1.5593 - val_accuracy: 0.4286 - val_loss: 1.6206 - learning_rate: 2.1340e-06\n",
      "Epoch 43/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3642 - loss: 1.6569\n",
      "Epoch 43: val_accuracy did not improve from 0.44048\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.3662 - loss: 1.6551 - val_accuracy: 0.4286 - val_loss: 1.6103 - learning_rate: 1.3323e-06\n",
      "Epoch 44/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4824 - loss: 1.5590\n",
      "Epoch 44: val_accuracy did not improve from 0.44048\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4807 - loss: 1.5618 - val_accuracy: 0.4405 - val_loss: 1.6004 - learning_rate: 8.1149e-07\n",
      "Epoch 45/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4283 - loss: 1.5921\n",
      "Epoch 45: val_accuracy improved from 0.44048 to 0.45238, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_imagenet_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4292 - loss: 1.5910 - val_accuracy: 0.4524 - val_loss: 1.5940 - learning_rate: 4.8178e-07\n",
      "Epoch 46/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4164 - loss: 1.6088   \n",
      "Epoch 46: val_accuracy improved from 0.45238 to 0.46429, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_imagenet_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4173 - loss: 1.6081 - val_accuracy: 0.4643 - val_loss: 1.5884 - learning_rate: 2.7857e-07\n",
      "Epoch 47/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4978 - loss: 1.5268\n",
      "Epoch 47: val_accuracy improved from 0.46429 to 0.47024, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_imagenet_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4963 - loss: 1.5284 - val_accuracy: 0.4702 - val_loss: 1.5850 - learning_rate: 1.5674e-07\n",
      "Epoch 48/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4544 - loss: 1.6136\n",
      "Epoch 48: val_accuracy improved from 0.47024 to 0.47619, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_imagenet_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.4549 - loss: 1.6116 - val_accuracy: 0.4762 - val_loss: 1.5796 - learning_rate: 8.5747e-08\n",
      "Epoch 49/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4353 - loss: 1.5651\n",
      "Epoch 49: val_accuracy improved from 0.47619 to 0.50000, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_imagenet_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4352 - loss: 1.5663 - val_accuracy: 0.5000 - val_loss: 1.5803 - learning_rate: 4.5565e-08\n",
      "Epoch 50/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4644 - loss: 1.6057\n",
      "Epoch 50: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4630 - loss: 1.6062 - val_accuracy: 0.4940 - val_loss: 1.5801 - learning_rate: 2.3498e-08\n",
      "Epoch 51/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4730 - loss: 1.6093\n",
      "Epoch 51: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4727 - loss: 1.6080 - val_accuracy: 0.4940 - val_loss: 1.5786 - learning_rate: 1.1749e-08\n",
      "Epoch 52/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4328 - loss: 1.6363\n",
      "Epoch 52: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4337 - loss: 1.6346 - val_accuracy: 0.5000 - val_loss: 1.5802 - learning_rate: 5.6901e-09\n",
      "Epoch 53/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4313 - loss: 1.5943\n",
      "Epoch 53: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4317 - loss: 1.5960 - val_accuracy: 0.4881 - val_loss: 1.5790 - learning_rate: 2.6664e-09\n",
      "Epoch 54/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3658 - loss: 1.6719\n",
      "Epoch 54: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.3668 - loss: 1.6698 - val_accuracy: 0.4881 - val_loss: 1.5788 - learning_rate: 1.2077e-09\n",
      "Epoch 55/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4486 - loss: 1.5501\n",
      "Epoch 55: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4485 - loss: 1.5506 - val_accuracy: 0.4940 - val_loss: 1.5777 - learning_rate: 5.2818e-10\n",
      "Epoch 56/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4347 - loss: 1.5557\n",
      "Epoch 56: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4347 - loss: 1.5570 - val_accuracy: 0.4940 - val_loss: 1.5793 - learning_rate: 2.2278e-10\n",
      "Epoch 57/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4835 - loss: 1.5291\n",
      "Epoch 57: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4829 - loss: 1.5301 - val_accuracy: 0.4940 - val_loss: 1.5768 - learning_rate: 9.0516e-11\n",
      "Epoch 58/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4245 - loss: 1.6053\n",
      "Epoch 58: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4245 - loss: 1.6057 - val_accuracy: 0.4940 - val_loss: 1.5786 - learning_rate: 3.5385e-11\n",
      "Epoch 59/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4479 - loss: 1.6199\n",
      "Epoch 59: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4484 - loss: 1.6171 - val_accuracy: 0.5000 - val_loss: 1.5784 - learning_rate: 1.3293e-11\n",
      "Epoch 60/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4509 - loss: 1.5398\n",
      "Epoch 60: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4515 - loss: 1.5393 - val_accuracy: 0.5000 - val_loss: 1.5768 - learning_rate: 4.7921e-12\n",
      "Epoch 61/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4131 - loss: 1.6309\n",
      "Epoch 61: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4134 - loss: 1.6310 - val_accuracy: 0.5000 - val_loss: 1.5791 - learning_rate: 1.6556e-12\n",
      "Epoch 62/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.5066 - loss: 1.5331\n",
      "Epoch 62: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.5044 - loss: 1.5363 - val_accuracy: 0.5000 - val_loss: 1.5831 - learning_rate: 5.4740e-13\n",
      "Epoch 63/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4225 - loss: 1.6754\n",
      "Epoch 63: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 1s/step - accuracy: 0.4237 - loss: 1.6727 - val_accuracy: 0.5000 - val_loss: 1.5853 - learning_rate: 1.7294e-13\n",
      "Epoch 64/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4784 - loss: 1.5230\n",
      "Epoch 64: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4776 - loss: 1.5264 - val_accuracy: 0.5000 - val_loss: 1.5842 - learning_rate: 5.2130e-14\n",
      "Epoch 65/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.4763 - loss: 1.6044\n",
      "Epoch 65: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 5s/step - accuracy: 0.4758 - loss: 1.6023 - val_accuracy: 0.5000 - val_loss: 1.5859 - learning_rate: 1.4967e-14\n",
      "Epoch 66/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4259 - loss: 1.6270\n",
      "Epoch 66: val_accuracy improved from 0.50000 to 0.50595, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_imagenet_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.4254 - loss: 1.6285 - val_accuracy: 0.5060 - val_loss: 1.5842 - learning_rate: 4.0861e-15\n",
      "Epoch 67/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25s/step - accuracy: 0.3983 - loss: 1.5849 \n",
      "Epoch 67: val_accuracy did not improve from 0.50595\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 25s/step - accuracy: 0.3986 - loss: 1.5862 - val_accuracy: 0.5060 - val_loss: 1.5832 - learning_rate: 1.0588e-15\n",
      "Epoch 68/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4569 - loss: 1.6357\n",
      "Epoch 68: val_accuracy did not improve from 0.50595\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - accuracy: 0.4573 - loss: 1.6363 - val_accuracy: 0.5060 - val_loss: 1.5834 - learning_rate: 2.5991e-16\n",
      "Epoch 69/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15s/step - accuracy: 0.4429 - loss: 1.6128 \n",
      "Epoch 69: val_accuracy did not improve from 0.50595\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 16s/step - accuracy: 0.4437 - loss: 1.6098 - val_accuracy: 0.5060 - val_loss: 1.5838 - learning_rate: 6.0322e-17\n",
      "Epoch 70/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4365 - loss: 1.5888\n",
      "Epoch 70: val_accuracy did not improve from 0.50595\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - accuracy: 0.4372 - loss: 1.5905 - val_accuracy: 0.5060 - val_loss: 1.5853 - learning_rate: 1.3208e-17\n",
      "Epoch 71/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4785 - loss: 1.5461\n",
      "Epoch 71: val_accuracy did not improve from 0.50595\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 1s/step - accuracy: 0.4777 - loss: 1.5479 - val_accuracy: 0.5000 - val_loss: 1.5845 - learning_rate: 2.7223e-18\n",
      "Epoch 72/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4708 - loss: 1.5564\n",
      "Epoch 72: val_accuracy did not improve from 0.50595\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - accuracy: 0.4709 - loss: 1.5580 - val_accuracy: 0.4940 - val_loss: 1.5816 - learning_rate: 5.2689e-19\n",
      "Epoch 73/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4365 - loss: 1.6225\n",
      "Epoch 73: val_accuracy did not improve from 0.50595\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.4359 - loss: 1.6243 - val_accuracy: 0.5000 - val_loss: 1.5814 - learning_rate: 9.5519e-20\n",
      "Epoch 74/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4824 - loss: 1.5224\n",
      "Epoch 74: val_accuracy improved from 0.50595 to 0.51190, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_imagenet_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3s/step - accuracy: 0.4818 - loss: 1.5242 - val_accuracy: 0.5119 - val_loss: 1.5797 - learning_rate: 1.6175e-20\n",
      "Epoch 75/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4814 - loss: 1.5160\n",
      "Epoch 75: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m513s\u001b[0m 1s/step - accuracy: 0.4813 - loss: 1.5162 - val_accuracy: 0.5060 - val_loss: 1.5774 - learning_rate: 2.5513e-21\n",
      "Epoch 76/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4777 - loss: 1.5920\n",
      "Epoch 76: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m950s\u001b[0m 59s/step - accuracy: 0.4769 - loss: 1.5919 - val_accuracy: 0.5119 - val_loss: 1.5762 - learning_rate: 3.7363e-22\n",
      "Epoch 77/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3910 - loss: 1.7283\n",
      "Epoch 77: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.3928 - loss: 1.7222 - val_accuracy: 0.5119 - val_loss: 1.5764 - learning_rate: 5.0633e-23\n",
      "Epoch 78/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4566 - loss: 1.5667\n",
      "Epoch 78: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.4570 - loss: 1.5656 - val_accuracy: 0.5119 - val_loss: 1.5776 - learning_rate: 6.3263e-24\n",
      "Epoch 79/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4730 - loss: 1.5742\n",
      "Epoch 79: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - accuracy: 0.4723 - loss: 1.5741 - val_accuracy: 0.5119 - val_loss: 1.5772 - learning_rate: 7.2590e-25\n",
      "Epoch 80/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4362 - loss: 1.6154\n",
      "Epoch 80: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - accuracy: 0.4360 - loss: 1.6138 - val_accuracy: 0.5060 - val_loss: 1.5808 - learning_rate: 7.6163e-26\n",
      "Epoch 81/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.4950 - loss: 1.5096\n",
      "Epoch 81: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2s/step - accuracy: 0.4929 - loss: 1.5122 - val_accuracy: 0.5060 - val_loss: 1.5836 - learning_rate: 7.2729e-27\n",
      "Epoch 82/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.4942 - loss: 1.4987\n",
      "Epoch 82: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - accuracy: 0.4919 - loss: 1.5044 - val_accuracy: 0.5000 - val_loss: 1.5827 - learning_rate: 6.2881e-28\n",
      "Epoch 83/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4398 - loss: 1.6013\n",
      "Epoch 83: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.4396 - loss: 1.6015 - val_accuracy: 0.5119 - val_loss: 1.5814 - learning_rate: 4.8944e-29\n",
      "Epoch 84/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4359 - loss: 1.6065\n",
      "Epoch 84: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4363 - loss: 1.6061 - val_accuracy: 0.5000 - val_loss: 1.5835 - learning_rate: 3.4080e-30\n",
      "Epoch 85/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51s/step - accuracy: 0.4499 - loss: 1.5814 \n",
      "Epoch 85: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m823s\u001b[0m 51s/step - accuracy: 0.4499 - loss: 1.5804 - val_accuracy: 0.5000 - val_loss: 1.5840 - learning_rate: 2.1077e-31\n",
      "Epoch 86/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4313 - loss: 1.5515\n",
      "Epoch 86: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4326 - loss: 1.5507 - val_accuracy: 0.5000 - val_loss: 1.5875 - learning_rate: 1.1486e-32\n",
      "Epoch 87/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4700 - loss: 1.5265\n",
      "Epoch 87: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4692 - loss: 1.5285 - val_accuracy: 0.5000 - val_loss: 1.5888 - learning_rate: 5.4659e-34\n",
      "Epoch 88/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4760 - loss: 1.5166 \n",
      "Epoch 88: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4757 - loss: 1.5196 - val_accuracy: 0.5000 - val_loss: 1.5898 - learning_rate: 2.2477e-35\n",
      "Epoch 89/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3887 - loss: 1.6564\n",
      "Epoch 89: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.3912 - loss: 1.6530 - val_accuracy: 0.5060 - val_loss: 1.5898 - learning_rate: 7.8922e-37\n",
      "Epoch 90/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4464 - loss: 1.5840\n",
      "Epoch 90: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4458 - loss: 1.5845 - val_accuracy: 0.5060 - val_loss: 1.5891 - learning_rate: 2.3329e-38\n",
      "Epoch 91/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4381 - loss: 1.6418\n",
      "Epoch 91: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4387 - loss: 1.6412 - val_accuracy: 0.5060 - val_loss: 1.5860 - learning_rate: 5.7090e-40\n",
      "Epoch 92/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4426 - loss: 1.6033\n",
      "Epoch 92: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4421 - loss: 1.6040 - val_accuracy: 0.5060 - val_loss: 1.5873 - learning_rate: 1.1334e-41\n",
      "Epoch 93/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4476 - loss: 1.5871\n",
      "Epoch 93: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.4480 - loss: 1.5869 - val_accuracy: 0.5060 - val_loss: 1.5870 - learning_rate: 1.7796e-43\n",
      "Epoch 94/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4331 - loss: 1.5647\n",
      "Epoch 94: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.4326 - loss: 1.5658 - val_accuracy: 0.5060 - val_loss: 1.5848 - learning_rate: 2.8026e-45\n",
      "Epoch 95/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59s/step - accuracy: 0.4789 - loss: 1.5752  \n",
      "Epoch 95: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m940s\u001b[0m 59s/step - accuracy: 0.4767 - loss: 1.5768 - val_accuracy: 0.5000 - val_loss: 1.5889 - learning_rate: 0.0000e+00\n",
      "Epoch 96/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.4750 - loss: 1.5488\n",
      "Epoch 96: val_accuracy did not improve from 0.51190\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.4741 - loss: 1.5498 - val_accuracy: 0.5000 - val_loss: 1.5913 - learning_rate: 0.0000e+00\n",
      "Epoch 96: early stopping\n",
      "Restoring model weights from the end of the best epoch: 76.\n",
      "\n",
      "âœ“ Training complete in 105.3 minutes\n",
      "\n",
      "ğŸ“Š Evaluating...\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ImageNet PRE-TRAINING RESULTS\n",
      "================================================================================\n",
      "\n",
      "Final Metrics:\n",
      "  Training Accuracy:   45.86%\n",
      "  Validation Accuracy: 51.19%\n",
      "  Best Val Accuracy:   51.19%\n",
      "  Validation Loss:     1.5762\n",
      "  Train-Val Gap:       -5.33pp\n",
      "  Epochs Trained:      96/100\n",
      "  Training Time:       105.3 minutes\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ˆ COMPLETE EXPERIMENTAL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "          Experiment          Pre-training Frozen     LR Val Acc (%) Improvement\n",
      "Baseline (Fruit-360)    Fruit-360 (studio)    60% 0.0001       55.36           â€”\n",
      "   Exp 1 (Fruit-360)    Fruit-360 (studio)     0% 0.0001       58.93     +3.57pp\n",
      "      Exp 2 Enhanced    Fruit-360 (studio)     0%  0.001       64.29     +8.93pp\n",
      "        From Scratch                  None    N/A 0.0005       64.29     +8.93pp\n",
      "  ImageNet Pre-train ImageNet (real-world)    70%  0.001       51.19    +-4.17pp\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ HYPOTHESIS EVALUATION\n",
      "================================================================================\n",
      "âš ï¸  NO IMPROVEMENT: Similar to scratch\n",
      "   ImageNet: 51.19% vs Scratch: 64.29%\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¡ KEY INSIGHTS\n",
      "================================================================================\n",
      "\n",
      "1. PRE-TRAINING DOMAIN MATTERS:\n",
      "   âš ï¸  ImageNet similar to scratch: -13.10pp\n",
      "   âš ï¸  800 images may be limiting factor\n",
      "\n",
      "2. COMPARISON TO FRUIT-360:\n",
      "   Fruit-360 â†’ Fine-tune: 64.29%\n",
      "   ImageNet â†’ Fine-tune:  51.19%\n",
      "   Difference: -13.10pp\n",
      "   âš ï¸  Both approaches similar\n",
      "\n",
      "3. MODEL CAPACITY:\n",
      "   âš ï¸  Model still struggling (45.86%)\n",
      "\n",
      "4. GENERALIZATION:\n",
      "   âœ… Excellent generalization: -5.3pp gap\n",
      "\n",
      "ğŸ’¾ Saving results...\n",
      "\n",
      "ğŸ“Š Creating comprehensive visualizations...\n",
      "âœ“ Visualizations saved\n",
      "\n",
      "================================================================================\n",
      "âœ… ImageNet PRE-TRAINING EXPERIMENT COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ FINAL RESULTS:\n",
      "  Status: SIMILAR\n",
      "  Validation Accuracy: 51.19%\n",
      "  Improvement from Baseline: -4.17pp\n",
      "  Improvement from Scratch: -13.10pp\n",
      "  Training Time: 105.3 minutes\n",
      "\n",
      "ğŸ“ Results saved to: /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_imagenet_results/\n",
      "  - experiment_summary.csv\n",
      "  - training_history.csv\n",
      "  - all_experiments_comparison.csv\n",
      "  - imagenet_comprehensive_analysis.png\n",
      "\n",
      "ğŸ’¾ Best model saved:\n",
      "  /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_imagenet_model.keras\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š COMPLETE EXPERIMENTAL JOURNEY\n",
      "================================================================================\n",
      "\n",
      "Phase 1: Fruit-360 Pre-training\n",
      "  Baseline (60% frozen):     55.36%\n",
      "  Exp 1 (0% frozen):         58.93%\n",
      "  Exp 2 Enhanced:            64.29%\n",
      "  Conclusion: Negative transfer (studio â†’ real-world)\n",
      "\n",
      "Phase 2: From Scratch\n",
      "  PyramidNet-18:             64.29%\n",
      "  Conclusion: Same as optimized Fruit-360\n",
      "\n",
      "Phase 3: ImageNet Pre-training\n",
      "  ResNet50 + Fine-tune:      51.19%\n",
      "  Conclusion: âš ï¸  Similar to scratch/Fruit-360\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¡ KEY TAKEAWAY\n",
      "================================================================================\n",
      "\n",
      "âš ï¸  ImageNet similar to scratch\n",
      "   ImageNet: 51.19% vs Scratch: 64.29%\n",
      "\n",
      "ğŸ¯ RECOMMENDATION:\n",
      "   Dataset size (800 images) may be the limiting factor\n",
      "   Consider:\n",
      "   1. Collecting more data (target: 1500-2000 images)\n",
      "   2. Using current best model ({exp2_enh_val:.2f}%)\n",
      "   3. Documenting the complete experimental journey\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ NEXT STEPS\n",
      "================================================================================\n",
      "\n",
      "1. ğŸ“Š Document complete experimental process\n",
      "   - Thorough comparison of pre-training strategies\n",
      "   - Best result: {imagenet_val:.2f}%\n",
      "\n",
      "2. ğŸ¤– Build multimodal system with best model\n",
      "\n",
      "3. ğŸ’¡ Discuss limitations (dataset size) in report\n",
      "\n",
      "================================================================================\n",
      "âœ¨ EXPERIMENT COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EXPERIMENT: ImageNet Pre-training + Fine-tuning\n",
    "\n",
    "Key Difference from Previous Experiments:\n",
    "- Previous: Fruit-360 (studio photos) â†’ Your data (real-world) = NEGATIVE TRANSFER\n",
    "- This: ImageNet (real-world diverse) â†’ Your data (real-world) = POSITIVE TRANSFER!\n",
    "\n",
    "Strategy:\n",
    "1. Use PyramidNet-18 pre-trained on ImageNet (1.2M diverse real-world images)\n",
    "2. Fine-tune on your 800 curated apple images\n",
    "3. Apply all successful optimizations from Exp 2 Enhanced\n",
    "\n",
    "Expected Result: 70-75% validation accuracy (vs 64.29% from scratch)\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, applications\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "PROJECT_ROOT = '/Users/vishal/Desktop/ML study/project/CNN type/first'\n",
    "\n",
    "CURATED_DATA = os.path.join(PROJECT_ROOT, 'data set fine tune')\n",
    "\n",
    "# ImageNet experiment outputs\n",
    "IMAGENET_MODEL_PATH = os.path.join(PROJECT_ROOT, 'experiment_imagenet_model.keras')\n",
    "IMAGENET_RESULTS_PATH = os.path.join(PROJECT_ROOT, 'experiment_imagenet_results')\n",
    "\n",
    "os.makedirs(IMAGENET_RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "# Training parameters (same as Exp 2 Enhanced for fair comparison)\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "INITIAL_LEARNING_RATE = 0.001\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \"*15 + \"EXPERIMENT: ImageNet Pre-training\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“Š PREVIOUS RESULTS:\")\n",
    "print(\"  Fruit-360 pre-train â†’ Fine-tune: 55.36% â†’ 64.29% (negative transfer)\")\n",
    "print(\"  From scratch:                    64.29%\")\n",
    "print(\"  Issue: Studio photos â‰  Real-world photos\")\n",
    "\n",
    "print(\"\\nğŸ¯ THIS EXPERIMENT:\")\n",
    "print(\"  ImageNet pre-train â†’ Fine-tune: ??%\")\n",
    "print(\"  Hypothesis: Real-world pre-training will give POSITIVE transfer\")\n",
    "print(\"  Expected: 70-75% validation accuracy\")\n",
    "\n",
    "print(\"\\nğŸ’¡ WHY ImageNet Should Work Better:\")\n",
    "print(\"  âœ“ 1.2M+ diverse real-world images\")\n",
    "print(\"  âœ“ Natural lighting, varied backgrounds\")\n",
    "print(\"  âœ“ Same domain as your curated data\")\n",
    "print(\"  âœ“ Standard practice in computer vision\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================\n",
    "# STRONGER DATA AUGMENTATION\n",
    "# ============================================\n",
    "print(\"\\nğŸ“ Loading data with enhanced augmentation...\")\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.6, 1.4],\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.25\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.25\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    CURATED_DATA,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    CURATED_DATA,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded:\")\n",
    "print(f\"  Training:   {train_generator.samples} images\")\n",
    "print(f\"  Validation: {val_generator.samples} images\")\n",
    "print(f\"  Classes:    {num_classes}\")\n",
    "\n",
    "# ============================================\n",
    "# BUILD MODEL WITH IMAGENET PRE-TRAINING\n",
    "# ============================================\n",
    "print(\"\\nğŸ”§ Building PyramidNet-18 with ImageNet pre-training...\")\n",
    "\n",
    "# We'll use ResNet18 as PyramidNet backbone (closest architecture with ImageNet weights)\n",
    "# Note: True PyramidNet doesn't have official ImageNet weights, so we use ResNet18\n",
    "# which has similar architecture and parameter count (~11M)\n",
    "\n",
    "print(\"\\nâš ï¸  Note: Using ResNet18 with ImageNet weights\")\n",
    "print(\"   (PyramidNet-18 doesn't have official ImageNet pre-trained weights)\")\n",
    "print(\"   ResNet18 has similar architecture and performance\")\n",
    "\n",
    "# Load ResNet18 with ImageNet weights\n",
    "base_model = applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Loaded ResNet50 with ImageNet weights\")\n",
    "print(f\"âœ“ Pre-trained on 1.2M+ diverse images\")\n",
    "\n",
    "# Build classification head\n",
    "inputs = base_model.input\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=outputs, name='ResNet50_ImageNet')\n",
    "\n",
    "print(f\"âœ“ Model architecture built\")\n",
    "print(f\"âœ“ Base: ResNet50 (ImageNet pre-trained)\")\n",
    "print(f\"âœ“ Head: Custom classification layers\")\n",
    "\n",
    "# ============================================\n",
    "# LAYER FREEZING STRATEGY\n",
    "# ============================================\n",
    "print(\"\\nğŸ”“ Setting layer freezing strategy...\")\n",
    "\n",
    "# Strategy: Freeze early layers, fine-tune later layers + head\n",
    "# Early layers learn generic features (edges, textures)\n",
    "# Later layers + head learn task-specific features\n",
    "\n",
    "total_base_layers = len(base_model.layers)\n",
    "freeze_until = int(total_base_layers * 0.7)  # Freeze first 70% of base model\n",
    "\n",
    "print(f\"\\nâœ“ Base model layers: {total_base_layers}\")\n",
    "print(f\"âœ“ Freezing first: {freeze_until} layers (70%)\")\n",
    "print(f\"âœ“ Fine-tuning last: {total_base_layers - freeze_until} layers (30%)\")\n",
    "\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    if i < freeze_until:\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = True\n",
    "\n",
    "# Classification head is always trainable\n",
    "trainable_layers = sum([1 for layer in model.layers if layer.trainable])\n",
    "frozen_layers = sum([1 for layer in model.layers if not layer.trainable])\n",
    "\n",
    "print(f\"\\nâœ“ Total model layers: {len(model.layers)}\")\n",
    "print(f\"âœ“ Frozen: {frozen_layers}\")\n",
    "print(f\"âœ“ Trainable: {trainable_layers}\")\n",
    "\n",
    "# ============================================\n",
    "# COSINE ANNEALING LR SCHEDULE\n",
    "# ============================================\n",
    "\n",
    "def cosine_annealing_schedule(epoch, initial_lr=INITIAL_LEARNING_RATE, epochs=EPOCHS):\n",
    "    \"\"\"Cosine annealing learning rate schedule\"\"\"\n",
    "    lr = initial_lr * 0.5 * (1 + np.cos(np.pi * epoch / epochs))\n",
    "    return lr\n",
    "\n",
    "# ============================================\n",
    "# COMPILE MODEL\n",
    "# ============================================\n",
    "print(\"\\nâš™ï¸  Compiling model...\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=INITIAL_LEARNING_RATE),\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Optimizer: Adam (LR={INITIAL_LEARNING_RATE})\")\n",
    "print(f\"âœ“ Loss: Categorical Crossentropy (Label Smoothing={LABEL_SMOOTHING})\")\n",
    "print(f\"âœ“ LR Schedule: Cosine Annealing\")\n",
    "\n",
    "# ============================================\n",
    "# CALLBACKS\n",
    "# ============================================\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    LearningRateScheduler(cosine_annealing_schedule, verbose=0),\n",
    "    ModelCheckpoint(\n",
    "        IMAGENET_MODEL_PATH,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# ============================================\n",
    "# TRAIN\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸš€ STARTING ImageNet PRE-TRAINING EXPERIMENT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"\\nKey Settings:\")\n",
    "print(f\"  âœ“ Pre-training: ImageNet (real-world diverse images)\")\n",
    "print(f\"  âœ“ Frozen: 70% of base model (keep generic features)\")\n",
    "print(f\"  âœ“ Fine-tune: 30% of base + classification head\")\n",
    "print(f\"  âœ“ Learning Rate: 0.001 with cosine annealing\")\n",
    "print(f\"  âœ“ Label Smoothing: 0.1\")\n",
    "print(f\"  âœ“ Strong augmentation\")\n",
    "print(f\"  âœ“ Max epochs: {EPOCHS}\")\n",
    "print(f\"\\nExpected time: 30-45 minutes\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ“ Training complete in {training_time/60:.1f} minutes\")\n",
    "\n",
    "# ============================================\n",
    "# EVALUATE\n",
    "# ============================================\n",
    "print(\"\\nğŸ“Š Evaluating...\")\n",
    "\n",
    "val_loss, val_accuracy = model.evaluate(val_generator, verbose=0)\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "epochs_trained = len(history.history['accuracy'])\n",
    "train_val_gap = final_train_acc - val_accuracy\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š ImageNet PRE-TRAINING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "print(f\"  Training Accuracy:   {final_train_acc*100:.2f}%\")\n",
    "print(f\"  Validation Accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"  Best Val Accuracy:   {best_val_acc*100:.2f}%\")\n",
    "print(f\"  Validation Loss:     {val_loss:.4f}\")\n",
    "print(f\"  Train-Val Gap:       {train_val_gap*100:.2f}pp\")\n",
    "print(f\"  Epochs Trained:      {epochs_trained}/{EPOCHS}\")\n",
    "print(f\"  Training Time:       {training_time/60:.1f} minutes\")\n",
    "\n",
    "# ============================================\n",
    "# COMPREHENSIVE COMPARISON\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“ˆ COMPLETE EXPERIMENTAL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# All experiments\n",
    "baseline_val = 55.36\n",
    "exp1_val = 58.93\n",
    "exp2_enh_val = 64.29\n",
    "imagenet_val = val_accuracy * 100\n",
    "\n",
    "improvement_from_baseline = imagenet_val - baseline_val\n",
    "improvement_from_scratch = imagenet_val - exp2_enh_val\n",
    "\n",
    "experiments_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Experiment': 'Baseline (Fruit-360)',\n",
    "        'Pre-training': 'Fruit-360 (studio)',\n",
    "        'Frozen': '60%',\n",
    "        'LR': '0.0001',\n",
    "        'Val Acc (%)': baseline_val,\n",
    "        'Improvement': 'â€”'\n",
    "    },\n",
    "    {\n",
    "        'Experiment': 'Exp 1 (Fruit-360)',\n",
    "        'Pre-training': 'Fruit-360 (studio)',\n",
    "        'Frozen': '0%',\n",
    "        'LR': '0.0001',\n",
    "        'Val Acc (%)': exp1_val,\n",
    "        'Improvement': f'+{exp1_val - baseline_val:.2f}pp'\n",
    "    },\n",
    "    {\n",
    "        'Experiment': 'Exp 2 Enhanced',\n",
    "        'Pre-training': 'Fruit-360 (studio)',\n",
    "        'Frozen': '0%',\n",
    "        'LR': '0.001',\n",
    "        'Val Acc (%)': exp2_enh_val,\n",
    "        'Improvement': f'+{exp2_enh_val - baseline_val:.2f}pp'\n",
    "    },\n",
    "    {\n",
    "        'Experiment': 'From Scratch',\n",
    "        'Pre-training': 'None',\n",
    "        'Frozen': 'N/A',\n",
    "        'LR': '0.0005',\n",
    "        'Val Acc (%)': exp2_enh_val,\n",
    "        'Improvement': f'+{exp2_enh_val - baseline_val:.2f}pp'\n",
    "    },\n",
    "    {\n",
    "        'Experiment': 'ImageNet Pre-train',\n",
    "        'Pre-training': 'ImageNet (real-world)',\n",
    "        'Frozen': '70%',\n",
    "        'LR': '0.001',\n",
    "        'Val Acc (%)': f'{imagenet_val:.2f}',\n",
    "        'Improvement': f'+{improvement_from_baseline:.2f}pp'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + experiments_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¯ HYPOTHESIS EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if imagenet_val >= 80:\n",
    "    print(\"ğŸ‰ OUTSTANDING: Exceeded stretch goal (80%+)!\")\n",
    "    print(\"   ImageNet pre-training was HIGHLY effective!\")\n",
    "    status = \"OUTSTANDING\"\n",
    "    success = True\n",
    "elif imagenet_val >= 75:\n",
    "    print(\"ğŸ‰ EXCELLENT: High performance (75-80%)!\")\n",
    "    print(\"   ImageNet pre-training worked very well!\")\n",
    "    status = \"EXCELLENT\"\n",
    "    success = True\n",
    "elif imagenet_val >= 70:\n",
    "    print(\"âœ… HYPOTHESIS CONFIRMED: Achieved target (70-75%)!\")\n",
    "    print(\"   ImageNet pre-training gave POSITIVE transfer!\")\n",
    "    status = \"SUCCESS\"\n",
    "    success = True\n",
    "elif imagenet_val >= 65:\n",
    "    print(\"âœ“ GOOD: Better than scratch/Fruit-360 (65-70%)\")\n",
    "    print(\"   ImageNet helped but modest improvement\")\n",
    "    status = \"GOOD\"\n",
    "    success = True\n",
    "elif imagenet_val > exp2_enh_val:\n",
    "    print(\"âœ“ IMPROVEMENT: Better than previous approaches\")\n",
    "    print(f\"   ImageNet: {imagenet_val:.2f}% vs Scratch: {exp2_enh_val:.2f}%\")\n",
    "    status = \"IMPROVEMENT\"\n",
    "    success = True\n",
    "else:\n",
    "    print(\"âš ï¸  NO IMPROVEMENT: Similar to scratch\")\n",
    "    print(f\"   ImageNet: {imagenet_val:.2f}% vs Scratch: {exp2_enh_val:.2f}%\")\n",
    "    status = \"SIMILAR\"\n",
    "    success = False\n",
    "\n",
    "# ============================================\n",
    "# KEY INSIGHTS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ’¡ KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. PRE-TRAINING DOMAIN MATTERS:\")\n",
    "if imagenet_val > exp2_enh_val + 3:\n",
    "    print(f\"   âœ… ImageNet (real-world) >> Fruit-360 (studio)\")\n",
    "    print(f\"   âœ… Improvement: {imagenet_val - exp2_enh_val:.2f}pp\")\n",
    "    print(f\"   âœ… Positive transfer achieved!\")\n",
    "elif imagenet_val > exp2_enh_val:\n",
    "    print(f\"   âœ“ ImageNet slightly better: {imagenet_val - exp2_enh_val:.2f}pp\")\n",
    "    print(f\"   âœ“ Real-world pre-training helps\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  ImageNet similar to scratch: {imagenet_val - exp2_enh_val:.2f}pp\")\n",
    "    print(f\"   âš ï¸  800 images may be limiting factor\")\n",
    "\n",
    "print(f\"\\n2. COMPARISON TO FRUIT-360:\")\n",
    "print(f\"   Fruit-360 â†’ Fine-tune: {exp2_enh_val:.2f}%\")\n",
    "print(f\"   ImageNet â†’ Fine-tune:  {imagenet_val:.2f}%\")\n",
    "print(f\"   Difference: {imagenet_val - exp2_enh_val:+.2f}pp\")\n",
    "if imagenet_val > exp2_enh_val + 3:\n",
    "    print(f\"   âœ… ImageNet pre-training is CLEARLY better!\")\n",
    "elif imagenet_val > exp2_enh_val:\n",
    "    print(f\"   âœ“ ImageNet pre-training helps\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  Both approaches similar\")\n",
    "\n",
    "print(f\"\\n3. MODEL CAPACITY:\")\n",
    "if final_train_acc >= 0.80:\n",
    "    print(f\"   âœ… Model fits training data well ({final_train_acc*100:.2f}%)\")\n",
    "    print(f\"   âœ… Much better than Fruit-360 approach (62.81%)\")\n",
    "elif final_train_acc >= 0.70:\n",
    "    print(f\"   âœ“ Model learning training data ({final_train_acc*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  Model still struggling ({final_train_acc*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n4. GENERALIZATION:\")\n",
    "if train_val_gap > 0.15:\n",
    "    print(f\"   âš ï¸  Some overfitting: {train_val_gap*100:.1f}pp gap\")\n",
    "elif train_val_gap > 0.05:\n",
    "    print(f\"   âœ“ Healthy gap: {train_val_gap*100:.1f}pp\")\n",
    "else:\n",
    "    print(f\"   âœ… Excellent generalization: {train_val_gap*100:.1f}pp gap\")\n",
    "\n",
    "# ============================================\n",
    "# SAVE RESULTS\n",
    "# ============================================\n",
    "print(\"\\nğŸ’¾ Saving results...\")\n",
    "\n",
    "# Save history\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df['learning_rate'] = [cosine_annealing_schedule(e) for e in range(len(history_df))]\n",
    "history_df.to_csv(os.path.join(IMAGENET_RESULTS_PATH, 'training_history.csv'), index=False)\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'experiment': 'ImageNet Pre-training',\n",
    "    'status': status,\n",
    "    'success': success,\n",
    "    'pretrain_source': 'ImageNet (1.2M real-world images)',\n",
    "    'baseline_val_acc': baseline_val,\n",
    "    'fruit360_best': exp2_enh_val,\n",
    "    'scratch_best': exp2_enh_val,\n",
    "    'imagenet_val_acc': imagenet_val,\n",
    "    'improvement_from_baseline': improvement_from_baseline,\n",
    "    'improvement_from_scratch': improvement_from_scratch,\n",
    "    'frozen_layers': '70%',\n",
    "    'initial_learning_rate': INITIAL_LEARNING_RATE,\n",
    "    'lr_schedule': 'Cosine Annealing',\n",
    "    'label_smoothing': LABEL_SMOOTHING,\n",
    "    'max_epochs': EPOCHS,\n",
    "    'epochs_trained': epochs_trained,\n",
    "    'training_time_min': training_time/60,\n",
    "    'final_train_acc': final_train_acc * 100,\n",
    "    'final_val_acc': imagenet_val,\n",
    "    'best_val_acc': best_val_acc * 100,\n",
    "    'train_val_gap': train_val_gap * 100\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df.to_csv(os.path.join(IMAGENET_RESULTS_PATH, 'experiment_summary.csv'), index=False)\n",
    "\n",
    "# Save complete comparison\n",
    "experiments_summary.to_csv(os.path.join(IMAGENET_RESULTS_PATH, 'all_experiments_comparison.csv'), index=False)\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================\n",
    "print(\"\\nğŸ“Š Creating comprehensive visualizations...\")\n",
    "\n",
    "fig = plt.figure(figsize=(20, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
    "\n",
    "# 1. Complete experiment progression\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "experiments = ['Baseline\\n(Fruit-360\\n60% frz)', \n",
    "               'Exp 1\\n(Fruit-360\\n0% frz)', \n",
    "               'Exp 2 Enh\\n(Fruit-360\\noptimized)',\n",
    "               'From\\nScratch',\n",
    "               'ImageNet\\nPre-train']\n",
    "accuracies = [baseline_val, exp1_val, exp2_enh_val, exp2_enh_val, imagenet_val]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#2ECC71', '#F39C12', '#9B59B6']\n",
    "\n",
    "bars = ax1.bar(experiments, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax1.axhline(y=70, color='green', linestyle='--', label='Target (70%)', linewidth=2)\n",
    "ax1.axhline(y=64.29, color='orange', linestyle='--', label='Scratch baseline', linewidth=2)\n",
    "ax1.set_ylabel('Validation Accuracy (%)', fontweight='bold', fontsize=13)\n",
    "ax1.set_title('Complete Experimental Journey: Pre-training Domain Impact', fontsize=18, fontweight='bold')\n",
    "ax1.set_ylim([0, 100])\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.legend(fontsize=11)\n",
    "\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, height + 2,\n",
    "             f'{acc:.1f}%', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# 2. Training curves\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.plot(history.history['accuracy'], label='Train', linewidth=2.5, color='#3498DB')\n",
    "ax2.plot(history.history['val_accuracy'], label='Val', linewidth=2.5, color='#E74C3C')\n",
    "ax2.set_title('ImageNet: Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Loss curves\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "ax3.plot(history.history['loss'], label='Train', linewidth=2.5, color='#3498DB')\n",
    "ax3.plot(history.history['val_loss'], label='Val', linewidth=2.5, color='#E74C3C')\n",
    "ax3.set_title('ImageNet: Loss', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Learning rate\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "lrs = [cosine_annealing_schedule(e) for e in range(epochs_trained)]\n",
    "ax4.plot(lrs, linewidth=2.5, color='#9B59B6')\n",
    "ax4.set_title('Cosine Annealing Schedule', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Learning Rate')\n",
    "ax4.set_yscale('log')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Pre-training comparison\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "pretrain_types = ['Fruit-360\\n(Studio)', 'From\\nScratch', 'ImageNet\\n(Real-world)']\n",
    "pretrain_accs = [exp2_enh_val, exp2_enh_val, imagenet_val]\n",
    "pretrain_colors = ['#E74C3C', '#F39C12', '#9B59B6']\n",
    "\n",
    "bars = ax5.bar(pretrain_types, pretrain_accs, color=pretrain_colors, alpha=0.8, \n",
    "               edgecolor='black', linewidth=2)\n",
    "ax5.set_ylabel('Validation Accuracy (%)', fontweight='bold')\n",
    "ax5.set_title('Pre-training Strategy Comparison', fontsize=14, fontweight='bold')\n",
    "ax5.set_ylim([50, 85])\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, acc in zip(bars, pretrain_accs):\n",
    "    height = bar.get_height()\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2, height + 1,\n",
    "             f'{acc:.1f}%', ha='center', fontweight='bold', fontsize=11)\n",
    "\n",
    "# 6. Train-Val gap\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "train_val_gaps = [t - v for t, v in zip(history.history['accuracy'], history.history['val_accuracy'])]\n",
    "ax6.plot(train_val_gaps, linewidth=2.5, color='#E67E22')\n",
    "ax6.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "ax6.axhline(y=0.15, color='red', linestyle='--', label='High overfit', linewidth=2)\n",
    "ax6.set_title('Overfitting Monitor', fontsize=14, fontweight='bold')\n",
    "ax6.set_xlabel('Epoch')\n",
    "ax6.set_ylabel('Train-Val Gap')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Final comparison table\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "ax7.axis('tight')\n",
    "ax7.axis('off')\n",
    "\n",
    "comparison_data = [\n",
    "    ['Pre-training', 'Val Acc', 'vs Baseline'],\n",
    "    ['Fruit-360', f'{exp2_enh_val:.1f}%', f'+{exp2_enh_val-baseline_val:.1f}pp'],\n",
    "    ['None (Scratch)', f'{exp2_enh_val:.1f}%', f'+{exp2_enh_val-baseline_val:.1f}pp'],\n",
    "    ['ImageNet', f'{imagenet_val:.1f}%', f'+{improvement_from_baseline:.1f}pp'],\n",
    "]\n",
    "\n",
    "table = ax7.table(cellText=comparison_data, cellLoc='center', loc='center',\n",
    "                  colWidths=[0.4, 0.3, 0.3])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# Style header\n",
    "for i in range(3):\n",
    "    table[(0, i)].set_facecolor('#2C3E50')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "# Highlight ImageNet row\n",
    "for i in range(3):\n",
    "    if imagenet_val >= 70:\n",
    "        table[(3, i)].set_facecolor('#D5F4E6')  # Light green\n",
    "    else:\n",
    "        table[(3, i)].set_facecolor('#FFF3CD')  # Light yellow\n",
    "\n",
    "ax7.set_title('Pre-training Impact Summary', fontsize=12, fontweight='bold', pad=10)\n",
    "\n",
    "plt.savefig(os.path.join(IMAGENET_RESULTS_PATH, 'imagenet_comprehensive_analysis.png'), \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"âœ“ Visualizations saved\")\n",
    "\n",
    "# ============================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ImageNet PRE-TRAINING EXPERIMENT COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ¯ FINAL RESULTS:\")\n",
    "print(f\"  Status: {status}\")\n",
    "print(f\"  Validation Accuracy: {imagenet_val:.2f}%\")\n",
    "print(f\"  Improvement from Baseline: {improvement_from_baseline:+.2f}pp\")\n",
    "print(f\"  Improvement from Scratch: {improvement_from_scratch:+.2f}pp\")\n",
    "print(f\"  Training Time: {training_time/60:.1f} minutes\")\n",
    "\n",
    "print(f\"\\nğŸ“ Results saved to: {IMAGENET_RESULTS_PATH}/\")\n",
    "print(f\"  - experiment_summary.csv\")\n",
    "print(f\"  - training_history.csv\")\n",
    "print(f\"  - all_experiments_comparison.csv\")\n",
    "print(f\"  - imagenet_comprehensive_analysis.png\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Best model saved:\")\n",
    "print(f\"  {IMAGENET_MODEL_PATH}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š COMPLETE EXPERIMENTAL JOURNEY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nPhase 1: Fruit-360 Pre-training\")\n",
    "print(f\"  Baseline (60% frozen):     {baseline_val:.2f}%\")\n",
    "print(f\"  Exp 1 (0% frozen):         {exp1_val:.2f}%\")\n",
    "print(f\"  Exp 2 Enhanced:            {exp2_enh_val:.2f}%\")\n",
    "print(f\"  Conclusion: Negative transfer (studio â†’ real-world)\")\n",
    "\n",
    "print(f\"\\nPhase 2: From Scratch\")\n",
    "print(f\"  PyramidNet-18:             {exp2_enh_val:.2f}%\")\n",
    "print(f\"  Conclusion: Same as optimized Fruit-360\")\n",
    "\n",
    "print(f\"\\nPhase 3: ImageNet Pre-training\")\n",
    "print(f\"  ResNet50 + Fine-tune:      {imagenet_val:.2f}%\")\n",
    "if imagenet_val >= 70:\n",
    "    print(f\"  Conclusion: âœ… POSITIVE TRANSFER - Best approach!\")\n",
    "elif imagenet_val > exp2_enh_val + 2:\n",
    "    print(f\"  Conclusion: âœ“ Better than previous approaches\")\n",
    "else:\n",
    "    print(f\"  Conclusion: âš ï¸  Similar to scratch/Fruit-360\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ’¡ KEY TAKEAWAY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if imagenet_val >= 70:\n",
    "    print(\"\\nâœ… SUCCESS: ImageNet pre-training works!\")\n",
    "    print(f\"   Real-world pre-training â†’ Real-world data = {imagenet_val:.2f}%\")\n",
    "    print(f\"   Studio pre-training â†’ Real-world data = {exp2_enh_val:.2f}%\")\n",
    "    print(f\"   Improvement: {imagenet_val - exp2_enh_val:+.2f}pp\")\n",
    "    print(\"\\nğŸ¯ RECOMMENDATION:\")\n",
    "    print(\"   Use this ImageNet pre-trained model for:\")\n",
    "    print(\"   1. Final documentation\")\n",
    "    print(\"   2. Multimodal system\")\n",
    "    print(\"   3. Project showcase\")\n",
    "elif imagenet_val > exp2_enh_val + 2:\n",
    "    print(\"\\nâœ“ ImageNet pre-training helps modestly\")\n",
    "    print(f\"   Improvement over scratch: {imagenet_val - exp2_enh_val:+.2f}pp\")\n",
    "    print(\"\\nğŸ¯ RECOMMENDATION:\")\n",
    "    print(\"   Use this model if you want:\")\n",
    "    print(\"   - Best possible accuracy\")\n",
    "    print(\"   - Standard CV approach (ImageNet pre-training)\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  ImageNet similar to scratch\")\n",
    "    print(f\"   ImageNet: {imagenet_val:.2f}% vs Scratch: {exp2_enh_val:.2f}%\")\n",
    "    print(\"\\nğŸ¯ RECOMMENDATION:\")\n",
    "    print(\"   Dataset size (800 images) may be the limiting factor\")\n",
    "    print(\"   Consider:\")\n",
    "    print(\"   1. Collecting more data (target: 1500-2000 images)\")\n",
    "    print(\"   2. Using current best model ({exp2_enh_val:.2f}%)\")\n",
    "    print(\"   3. Documenting the complete experimental journey\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸš€ NEXT STEPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if success and imagenet_val >= 70:\n",
    "    print(\"\\n1. âœ… Document this SUCCESS story:\")\n",
    "    print(\"   - Negative transfer discovery (Fruit-360)\")\n",
    "    print(\"   - Positive transfer success (ImageNet)\")\n",
    "    print(\"   - Final accuracy: {imagenet_val:.2f}%\")\n",
    "    print(\"\\n2. ğŸ¤– Build multimodal system with this model\")\n",
    "    print(\"\\n3. ğŸ“Š Create final project report\")\n",
    "elif success:\n",
    "    print(\"\\n1. âœ“ Document experimental journey\")\n",
    "    print(\"   - All three pre-training strategies tested\")\n",
    "    print(\"   - ImageNet performed best: {imagenet_val:.2f}%\")\n",
    "    print(\"\\n2. ğŸ¤– Build multimodal system\")\n",
    "    print(\"\\n3. ğŸ“Š Optional: Try collecting more data\")\n",
    "else:\n",
    "    print(\"\\n1. ğŸ“Š Document complete experimental process\")\n",
    "    print(\"   - Thorough comparison of pre-training strategies\")\n",
    "    print(\"   - Best result: {imagenet_val:.2f}%\")\n",
    "    print(\"\\n2. ğŸ¤– Build multimodal system with best model\")\n",
    "    print(\"\\n3. ğŸ’¡ Discuss limitations (dataset size) in report\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ¨ EXPERIMENT COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a562e6fd-b6f0-40a5-bf0c-ebb4258add3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a7a8f9-9cdb-4d17-bcc4-ea4ddb1b5889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "          ULTIMATE EXPERIMENT: From Scratch + ALL Optimizations\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š COMPARISON:\n",
      "\n",
      "  Exp 2 Enhanced (Fruit-360 pre-trained):\n",
      "    Settings: LR=0.001, Cosine, Label Smooth, Enhanced Aug, 100ep\n",
      "    Result: 64.29%\n",
      "\n",
      "  From Scratch (Previous):\n",
      "    Settings: LR=0.0005, ReduceLR, NO label smooth, Basic Aug, 75ep\n",
      "    Result: 64.29%\n",
      "\n",
      "ğŸ¯ THIS EXPERIMENT (From Scratch + Full Optimization):\n",
      "    Settings: LR=0.001, Cosine, Label Smooth, Enhanced Aug, 100ep\n",
      "    Hypothesis: Should BEAT 64.29% since it got same result with weaker settings!\n",
      "    Expected: 66-70% validation accuracy\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Loading data with ENHANCED augmentation...\n",
      "Found 519 images belonging to 8 classes.\n",
      "Found 168 images belonging to 8 classes.\n",
      "\n",
      "âœ“ Data loaded:\n",
      "  Training:   519 images\n",
      "  Validation: 168 images\n",
      "  Classes:    8\n",
      "\n",
      "âœ“ ENHANCED Augmentation (same as Exp 2):\n",
      "  Rotation: Â±30Â° (was Â±20Â°)\n",
      "  Shift: Â±20% (was Â±15%)\n",
      "  Shear: Â±15Â° (NEW)\n",
      "  Zoom: Â±30% (was Â±20%)\n",
      "  Brightness: 0.6-1.4 (wider range)\n",
      "\n",
      "ğŸ”§ Building PyramidNet-18 from scratch...\n",
      "âœ“ Model built\n",
      "âœ“ Architecture: PyramidNet-18\n",
      "âœ“ Parameters: 294,888\n",
      "âœ“ Training: FROM SCRATCH (random initialization)\n",
      "\n",
      "âš™ï¸  Compiling model with ALL optimizations...\n",
      "\n",
      "âœ“ Optimizer: Adam\n",
      "âœ“ Learning Rate: 0.001 (2x from-scratch baseline)\n",
      "âœ“ LR Schedule: Cosine Annealing (smooth decay)\n",
      "âœ“ Label Smoothing: 0.1 (NEW!)\n",
      "âœ“ Dropout: 0.5 (in model)\n",
      "âœ“ Data Augmentation: Enhanced\n",
      "âœ“ Max Epochs: 100\n",
      "\n",
      "================================================================================\n",
      "ALL OPTIMIZATIONS APPLIED:\n",
      "================================================================================\n",
      "1. âœ… Right-sized architecture (PyramidNet-18, 11M params)\n",
      "2. âœ… Optimal learning rate (0.001)\n",
      "3. âœ… Cosine annealing schedule\n",
      "4. âœ… Enhanced data augmentation\n",
      "5. âœ… Label smoothing (0.1)\n",
      "6. âœ… Dropout (0.5)\n",
      "7. âœ… Batch normalization\n",
      "8. âœ… Early stopping\n",
      "9. âœ… More epochs (100)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ STARTING ULTIMATE EXPERIMENT\n",
      "================================================================================\n",
      "Started at: 18:04:49\n",
      "\n",
      "This combines:\n",
      "  âœ“ Best architecture (PyramidNet-18)\n",
      "  âœ“ Training from scratch (no pre-training)\n",
      "  âœ“ ALL optimizations (Exp 2 Enhanced settings)\n",
      "\n",
      "Expected time: 4-6 minutes\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.1854 - loss: 2.1102"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/PIL/Image.py:1056: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.25000, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_ultimate_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 278ms/step - accuracy: 0.1872 - loss: 2.1049 - val_accuracy: 0.2500 - val_loss: 2.1864 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.3786 - loss: 1.7639\n",
      "Epoch 2: val_accuracy improved from 0.25000 to 0.35714, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_ultimate_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 269ms/step - accuracy: 0.3768 - loss: 1.7655 - val_accuracy: 0.3571 - val_loss: 1.7790 - learning_rate: 9.9975e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.3908 - loss: 1.6402\n",
      "Epoch 3: val_accuracy did not improve from 0.35714\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 270ms/step - accuracy: 0.3914 - loss: 1.6400 - val_accuracy: 0.3512 - val_loss: 1.8601 - learning_rate: 9.9877e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.4221 - loss: 1.7051\n",
      "Epoch 4: val_accuracy did not improve from 0.35714\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 259ms/step - accuracy: 0.4224 - loss: 1.7025 - val_accuracy: 0.1369 - val_loss: 3.6103 - learning_rate: 9.9655e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - accuracy: 0.4217 - loss: 1.5706\n",
      "Epoch 5: val_accuracy did not improve from 0.35714\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 262ms/step - accuracy: 0.4228 - loss: 1.5708 - val_accuracy: 0.1607 - val_loss: 5.6225 - learning_rate: 9.9262e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.4527 - loss: 1.5940\n",
      "Epoch 6: val_accuracy did not improve from 0.35714\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 265ms/step - accuracy: 0.4526 - loss: 1.5953 - val_accuracy: 0.1607 - val_loss: 2.9278 - learning_rate: 9.8651e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.4602 - loss: 1.5775\n",
      "Epoch 7: val_accuracy did not improve from 0.35714\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 264ms/step - accuracy: 0.4603 - loss: 1.5770 - val_accuracy: 0.3274 - val_loss: 2.1721 - learning_rate: 9.7777e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.4517 - loss: 1.5692\n",
      "Epoch 8: val_accuracy did not improve from 0.35714\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.4527 - loss: 1.5687 - val_accuracy: 0.2857 - val_loss: 2.8506 - learning_rate: 9.6600e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.4700 - loss: 1.5500\n",
      "Epoch 9: val_accuracy did not improve from 0.35714\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 249ms/step - accuracy: 0.4705 - loss: 1.5485 - val_accuracy: 0.1964 - val_loss: 3.9315 - learning_rate: 9.5083e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.5222 - loss: 1.4192\n",
      "Epoch 10: val_accuracy did not improve from 0.35714\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 255ms/step - accuracy: 0.5219 - loss: 1.4205 - val_accuracy: 0.1250 - val_loss: 7.8262 - learning_rate: 9.3195e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.5092 - loss: 1.4959\n",
      "Epoch 11: val_accuracy did not improve from 0.35714\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 247ms/step - accuracy: 0.5097 - loss: 1.4950 - val_accuracy: 0.1964 - val_loss: 3.5716 - learning_rate: 9.0914e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.5311 - loss: 1.4675\n",
      "Epoch 12: val_accuracy did not improve from 0.35714\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 258ms/step - accuracy: 0.5295 - loss: 1.4692 - val_accuracy: 0.2500 - val_loss: 2.9229 - learning_rate: 8.8227e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.5137 - loss: 1.4266\n",
      "Epoch 13: val_accuracy did not improve from 0.35714\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 255ms/step - accuracy: 0.5137 - loss: 1.4268 - val_accuracy: 0.2738 - val_loss: 2.1737 - learning_rate: 8.5129e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.5528 - loss: 1.4427\n",
      "Epoch 14: val_accuracy did not improve from 0.35714\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 254ms/step - accuracy: 0.5522 - loss: 1.4424 - val_accuracy: 0.2560 - val_loss: 1.9804 - learning_rate: 8.1628e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 249ms/step - accuracy: 0.5499 - loss: 1.4415\n",
      "Epoch 15: val_accuracy improved from 0.35714 to 0.37500, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_ultimate_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 276ms/step - accuracy: 0.5501 - loss: 1.4388 - val_accuracy: 0.3750 - val_loss: 2.1552 - learning_rate: 7.7744e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.5671 - loss: 1.3598\n",
      "Epoch 16: val_accuracy improved from 0.37500 to 0.50000, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_ultimate_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 259ms/step - accuracy: 0.5661 - loss: 1.3609 - val_accuracy: 0.5000 - val_loss: 1.4657 - learning_rate: 7.3507e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.5705 - loss: 1.3698\n",
      "Epoch 17: val_accuracy did not improve from 0.50000\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 261ms/step - accuracy: 0.5698 - loss: 1.3708 - val_accuracy: 0.2857 - val_loss: 2.0193 - learning_rate: 6.8961e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.5440 - loss: 1.4086\n",
      "Epoch 18: val_accuracy improved from 0.50000 to 0.51190, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_ultimate_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 257ms/step - accuracy: 0.5430 - loss: 1.4100 - val_accuracy: 0.5119 - val_loss: 1.4642 - learning_rate: 6.4159e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.5590 - loss: 1.3534\n",
      "Epoch 19: val_accuracy improved from 0.51190 to 0.58929, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_ultimate_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 263ms/step - accuracy: 0.5587 - loss: 1.3538 - val_accuracy: 0.5893 - val_loss: 1.3854 - learning_rate: 5.9165e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.5908 - loss: 1.3461\n",
      "Epoch 20: val_accuracy did not improve from 0.58929\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.5898 - loss: 1.3472 - val_accuracy: 0.5536 - val_loss: 1.3755 - learning_rate: 5.4050e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.5795 - loss: 1.3908\n",
      "Epoch 21: val_accuracy did not improve from 0.58929\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.5791 - loss: 1.3893 - val_accuracy: 0.5119 - val_loss: 1.4180 - learning_rate: 4.8889e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.6215 - loss: 1.3136\n",
      "Epoch 22: val_accuracy did not improve from 0.58929\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 254ms/step - accuracy: 0.6201 - loss: 1.3147 - val_accuracy: 0.5774 - val_loss: 1.3602 - learning_rate: 4.3759e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 241ms/step - accuracy: 0.6030 - loss: 1.3152\n",
      "Epoch 23: val_accuracy improved from 0.58929 to 0.59524, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_ultimate_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 267ms/step - accuracy: 0.6024 - loss: 1.3161 - val_accuracy: 0.5952 - val_loss: 1.3100 - learning_rate: 3.8738e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.5929 - loss: 1.3352\n",
      "Epoch 24: val_accuracy did not improve from 0.59524\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 258ms/step - accuracy: 0.5930 - loss: 1.3350 - val_accuracy: 0.5417 - val_loss: 1.3490 - learning_rate: 3.3898e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 240ms/step - accuracy: 0.5623 - loss: 1.3685\n",
      "Epoch 25: val_accuracy did not improve from 0.59524\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 262ms/step - accuracy: 0.5631 - loss: 1.3672 - val_accuracy: 0.5595 - val_loss: 1.3499 - learning_rate: 2.9304e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.6447 - loss: 1.2576\n",
      "Epoch 26: val_accuracy improved from 0.59524 to 0.61905, saving model to /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_ultimate_model.keras\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 259ms/step - accuracy: 0.6436 - loss: 1.2597 - val_accuracy: 0.6190 - val_loss: 1.3598 - learning_rate: 2.5013e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.6136 - loss: 1.3038\n",
      "Epoch 27: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 257ms/step - accuracy: 0.6136 - loss: 1.3026 - val_accuracy: 0.5952 - val_loss: 1.3196 - learning_rate: 2.1068e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.6244 - loss: 1.2475\n",
      "Epoch 28: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 254ms/step - accuracy: 0.6233 - loss: 1.2482 - val_accuracy: 0.5952 - val_loss: 1.3556 - learning_rate: 1.7500e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 242ms/step - accuracy: 0.5920 - loss: 1.2711\n",
      "Epoch 29: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 264ms/step - accuracy: 0.5933 - loss: 1.2699 - val_accuracy: 0.5595 - val_loss: 1.3124 - learning_rate: 1.4327e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.6347 - loss: 1.2348\n",
      "Epoch 30: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 261ms/step - accuracy: 0.6337 - loss: 1.2361 - val_accuracy: 0.5714 - val_loss: 1.3012 - learning_rate: 1.1554e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.6783 - loss: 1.1749\n",
      "Epoch 31: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 254ms/step - accuracy: 0.6752 - loss: 1.1799 - val_accuracy: 0.6071 - val_loss: 1.2557 - learning_rate: 9.1729e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.6308 - loss: 1.2397\n",
      "Epoch 32: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 252ms/step - accuracy: 0.6308 - loss: 1.2397 - val_accuracy: 0.5893 - val_loss: 1.2970 - learning_rate: 7.1644e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.6690 - loss: 1.1721\n",
      "Epoch 33: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 254ms/step - accuracy: 0.6684 - loss: 1.1742 - val_accuracy: 0.5833 - val_loss: 1.2871 - learning_rate: 5.5017e-05\n",
      "Epoch 34/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.6772 - loss: 1.1891\n",
      "Epoch 34: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 251ms/step - accuracy: 0.6762 - loss: 1.1911 - val_accuracy: 0.5714 - val_loss: 1.2885 - learning_rate: 4.1511e-05\n",
      "Epoch 35/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.6218 - loss: 1.2596\n",
      "Epoch 35: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 255ms/step - accuracy: 0.6229 - loss: 1.2580 - val_accuracy: 0.5952 - val_loss: 1.2741 - learning_rate: 3.0755e-05\n",
      "Epoch 36/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.6357 - loss: 1.2402\n",
      "Epoch 36: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 256ms/step - accuracy: 0.6359 - loss: 1.2396 - val_accuracy: 0.6131 - val_loss: 1.2632 - learning_rate: 2.2359e-05\n",
      "Epoch 37/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.5766 - loss: 1.2915\n",
      "Epoch 37: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 261ms/step - accuracy: 0.5789 - loss: 1.2894 - val_accuracy: 0.6071 - val_loss: 1.2577 - learning_rate: 1.5939e-05\n",
      "Epoch 38/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.6723 - loss: 1.1713\n",
      "Epoch 38: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 259ms/step - accuracy: 0.6712 - loss: 1.1736 - val_accuracy: 0.6131 - val_loss: 1.2585 - learning_rate: 1.1135e-05\n",
      "Epoch 39/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.6714 - loss: 1.1731\n",
      "Epoch 39: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 262ms/step - accuracy: 0.6703 - loss: 1.1754 - val_accuracy: 0.6131 - val_loss: 1.2579 - learning_rate: 7.6168e-06\n",
      "Epoch 40/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.6206 - loss: 1.3020\n",
      "Epoch 40: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 251ms/step - accuracy: 0.6212 - loss: 1.2992 - val_accuracy: 0.6071 - val_loss: 1.2613 - learning_rate: 5.0985e-06\n",
      "Epoch 41/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.6372 - loss: 1.2314\n",
      "Epoch 41: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 257ms/step - accuracy: 0.6369 - loss: 1.2313 - val_accuracy: 0.6131 - val_loss: 1.2630 - learning_rate: 3.3370e-06\n",
      "Epoch 42/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.6686 - loss: 1.1643\n",
      "Epoch 42: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 256ms/step - accuracy: 0.6677 - loss: 1.1664 - val_accuracy: 0.6131 - val_loss: 1.2644 - learning_rate: 2.1340e-06\n",
      "Epoch 43/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.6235 - loss: 1.2642\n",
      "Epoch 43: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 257ms/step - accuracy: 0.6249 - loss: 1.2618 - val_accuracy: 0.6131 - val_loss: 1.2635 - learning_rate: 1.3323e-06\n",
      "Epoch 44/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.6631 - loss: 1.1899\n",
      "Epoch 44: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 252ms/step - accuracy: 0.6614 - loss: 1.1927 - val_accuracy: 0.6131 - val_loss: 1.2634 - learning_rate: 8.1149e-07\n",
      "Epoch 45/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 247ms/step - accuracy: 0.6687 - loss: 1.2116\n",
      "Epoch 45: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 270ms/step - accuracy: 0.6684 - loss: 1.2117 - val_accuracy: 0.6131 - val_loss: 1.2626 - learning_rate: 4.8178e-07\n",
      "Epoch 46/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step - accuracy: 0.6056 - loss: 1.2969\n",
      "Epoch 46: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 269ms/step - accuracy: 0.6077 - loss: 1.2935 - val_accuracy: 0.6131 - val_loss: 1.2614 - learning_rate: 2.7857e-07\n",
      "Epoch 47/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.6261 - loss: 1.2499\n",
      "Epoch 47: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 250ms/step - accuracy: 0.6265 - loss: 1.2494 - val_accuracy: 0.6131 - val_loss: 1.2614 - learning_rate: 1.5674e-07\n",
      "Epoch 48/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.6228 - loss: 1.1905\n",
      "Epoch 48: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 246ms/step - accuracy: 0.6226 - loss: 1.1923 - val_accuracy: 0.6131 - val_loss: 1.2626 - learning_rate: 8.5747e-08\n",
      "Epoch 49/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.6270 - loss: 1.2542\n",
      "Epoch 49: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 246ms/step - accuracy: 0.6276 - loss: 1.2538 - val_accuracy: 0.6131 - val_loss: 1.2623 - learning_rate: 4.5565e-08\n",
      "Epoch 50/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 237ms/step - accuracy: 0.6626 - loss: 1.2219\n",
      "Epoch 50: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 260ms/step - accuracy: 0.6616 - loss: 1.2229 - val_accuracy: 0.6190 - val_loss: 1.2633 - learning_rate: 2.3498e-08\n",
      "Epoch 51/100\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.6750 - loss: 1.2016\n",
      "Epoch 51: val_accuracy did not improve from 0.61905\n",
      "\u001b[1m17/17\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 247ms/step - accuracy: 0.6754 - loss: 1.2013 - val_accuracy: 0.6190 - val_loss: 1.2630 - learning_rate: 1.1749e-08\n",
      "Epoch 51: early stopping\n",
      "Restoring model weights from the end of the best epoch: 31.\n",
      "\n",
      "âœ“ Training complete in 3.9 minutes\n",
      "\n",
      "ğŸ“Š Evaluating...\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ULTIMATE EXPERIMENT RESULTS\n",
      "================================================================================\n",
      "\n",
      "Final Metrics:\n",
      "  Training Accuracy:   68.21%\n",
      "  Validation Accuracy: 60.71%\n",
      "  Best Val Accuracy:   61.90%\n",
      "  Validation Loss:     1.2557\n",
      "  Train-Val Gap:       7.49pp\n",
      "  Epochs Trained:      51/100\n",
      "  Training Time:       3.9 minutes\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ˆ COMPLETE EXPERIMENTAL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "               Experiment Pre-train Optimizations Val Acc (%)\n",
      "                 Baseline Fruit-360          Poor       55.36\n",
      "                    Exp 1 Fruit-360       Partial       58.93\n",
      "           Exp 2 Enhanced Fruit-360          Full       64.29\n",
      "     From Scratch (Basic)      None       Partial       64.29\n",
      "ULTIMATE (Scratch + Full)      None          Full       60.71\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ HYPOTHESIS EVALUATION\n",
      "================================================================================\n",
      "âš ï¸  Similar to previous from-scratch\n",
      "   Ultimate: 60.71% vs Basic: 64.29%\n",
      "   Optimizations had limited impact\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¡ KEY INSIGHTS\n",
      "================================================================================\n",
      "\n",
      "1. OPTIMIZATION IMPACT:\n",
      "   âš ï¸  Similar result: 60.71% vs 64.29%\n",
      "   âš ï¸  Basic settings were already near-optimal\n",
      "\n",
      "2. PRE-TRAINING COMPARISON:\n",
      "   Fruit-360 + Full Opt: 64.29%\n",
      "   From Scratch + Full Opt: 60.71%\n",
      "   Difference: -3.58pp\n",
      "   âš ï¸  Pre-training was slightly better\n",
      "\n",
      "3. TRAINING CAPACITY:\n",
      "   âš ï¸  Model struggles with training data (68.21%)\n",
      "\n",
      "4. GENERALIZATION:\n",
      "   âœ… Excellent generalization: 7.5pp gap\n",
      "\n",
      "ğŸ’¾ Saving results...\n",
      "\n",
      "ğŸ“Š Creating visualizations...\n",
      "âœ“ Visualizations saved\n",
      "\n",
      "================================================================================\n",
      "âœ… ULTIMATE EXPERIMENT COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ FINAL RESULTS:\n",
      "  Status: SIMILAR\n",
      "  Validation Accuracy: 60.71%\n",
      "  Improvement from Scratch-Basic: -3.58pp\n",
      "  Training Time: 3.9 minutes\n",
      "\n",
      "ğŸ“ Results saved to: /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_ultimate_results/\n",
      "\n",
      "ğŸ’¾ Best model saved:\n",
      "  /Users/vishal/Desktop/ML study/project/CNN type/first/experiment_ultimate_model.keras\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š FINAL EXPERIMENTAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "All Experiments Tested:\n",
      "  1. Baseline (Fruit-360, poor settings): 55.36%\n",
      "  2. Exp 1 (Fruit-360, unfroze): 58.93%\n",
      "  3. Exp 2 Enhanced (Fruit-360, full opt): 64.29%\n",
      "  4. From Scratch (basic settings): 64.29%\n",
      "  5. ULTIMATE (scratch + full opt): 60.71%\n",
      "\n",
      "ğŸ† BEST MODEL:\n",
      "  Both are similar:\n",
      "  - From Scratch (Basic): 64.29%\n",
      "  - Ultimate (Full Opt): 60.71%\n",
      "  Conclusion: 800 images is the limiting factor\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¡ FINAL CONCLUSIONS\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š INSIGHT: 64-65% appears to be the limit for 800 images\n",
      "  To improve further:\n",
      "  1. Need more data (1500-2000 images)\n",
      "  2. Or advanced techniques (ensembles, self-supervised)\n",
      "  3. Current result is strong for dataset size\n",
      "\n",
      "================================================================================\n",
      "âœ¨ ALL EXPERIMENTS COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ NEXT STEPS:\n",
      "  1. âœ… Complete experimental validation done\n",
      "  2. â†’ Use best model for multimodal system\n",
      "  3. â†’ Document complete findings\n",
      "  4. â†’ Build final application\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EXPERIMENT: PyramidNet-18 From Scratch + ALL OPTIMIZATIONS\n",
    "\n",
    "This is the experiment we should have run!\n",
    "\n",
    "Key Insight from Analysis:\n",
    "- Exp 2 Enhanced (Fruit-360 pre-training): 64.29% with FULL optimizations\n",
    "- From Scratch: 64.29% with WEAKER settings\n",
    "- Question: What if we use FULL optimizations from scratch?\n",
    "\n",
    "Hypothesis: PyramidNet-18 from scratch with ALL optimizations will exceed 64.29%\n",
    "Expected Result: 66-70% validation accuracy\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "PROJECT_ROOT = '/Users/vishal/Desktop/ML study/project/CNN type/first'\n",
    "\n",
    "CURATED_DATA = os.path.join(PROJECT_ROOT, 'data set fine tune')\n",
    "\n",
    "# Ultimate experiment outputs\n",
    "ULTIMATE_MODEL_PATH = os.path.join(PROJECT_ROOT, 'experiment_ultimate_model.keras')\n",
    "ULTIMATE_RESULTS_PATH = os.path.join(PROJECT_ROOT, 'experiment_ultimate_results')\n",
    "\n",
    "os.makedirs(ULTIMATE_RESULTS_PATH, exist_ok=True)\n",
    "\n",
    "# ALL OPTIMIZATIONS (from Exp 2 Enhanced)\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100  # More than from-scratch (was 75)\n",
    "INITIAL_LEARNING_RATE = 0.001  # Higher than from-scratch (was 0.0005)\n",
    "LABEL_SMOOTHING = 0.1  # NEW for from-scratch\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \"*10 + \"ULTIMATE EXPERIMENT: From Scratch + ALL Optimizations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ“Š COMPARISON:\")\n",
    "print(\"\\n  Exp 2 Enhanced (Fruit-360 pre-trained):\")\n",
    "print(\"    Settings: LR=0.001, Cosine, Label Smooth, Enhanced Aug, 100ep\")\n",
    "print(\"    Result: 64.29%\")\n",
    "\n",
    "print(\"\\n  From Scratch (Previous):\")\n",
    "print(\"    Settings: LR=0.0005, ReduceLR, NO label smooth, Basic Aug, 75ep\")\n",
    "print(\"    Result: 64.29%\")\n",
    "\n",
    "print(\"\\nğŸ¯ THIS EXPERIMENT (From Scratch + Full Optimization):\")\n",
    "print(\"    Settings: LR=0.001, Cosine, Label Smooth, Enhanced Aug, 100ep\")\n",
    "print(\"    Hypothesis: Should BEAT 64.29% since it got same result with weaker settings!\")\n",
    "print(\"    Expected: 66-70% validation accuracy\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================\n",
    "# ENHANCED DATA AUGMENTATION (from Exp 2)\n",
    "# ============================================\n",
    "print(\"\\nğŸ“ Loading data with ENHANCED augmentation...\")\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,  # Enhanced (was 20 in from-scratch)\n",
    "    width_shift_range=0.2,  # Enhanced (was 0.15)\n",
    "    height_shift_range=0.2,  # Enhanced (was 0.15)\n",
    "    shear_range=0.15,  # NEW (not in from-scratch)\n",
    "    zoom_range=0.3,  # Enhanced (was 0.2)\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.6, 1.4],  # Enhanced (was 0.7-1.3)\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.25\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.25\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    CURATED_DATA,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    CURATED_DATA,\n",
    "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "num_classes = train_generator.num_classes\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded:\")\n",
    "print(f\"  Training:   {train_generator.samples} images\")\n",
    "print(f\"  Validation: {val_generator.samples} images\")\n",
    "print(f\"  Classes:    {num_classes}\")\n",
    "\n",
    "print(f\"\\nâœ“ ENHANCED Augmentation (same as Exp 2):\")\n",
    "print(f\"  Rotation: Â±30Â° (was Â±20Â°)\")\n",
    "print(f\"  Shift: Â±20% (was Â±15%)\")\n",
    "print(f\"  Shear: Â±15Â° (NEW)\")\n",
    "print(f\"  Zoom: Â±30% (was Â±20%)\")\n",
    "print(f\"  Brightness: 0.6-1.4 (wider range)\")\n",
    "\n",
    "# ============================================\n",
    "# PYRAMIDNET-18 ARCHITECTURE (FROM SCRATCH)\n",
    "# ============================================\n",
    "\n",
    "def pyramidnet_basicblock(x, in_filters, out_filters, stride=1):\n",
    "    \"\"\"PyramidNet Basic Block\"\"\"\n",
    "    shortcut = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(out_filters, 3, strides=stride, padding='same', use_bias=False)(x)\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(out_filters, 3, strides=1, padding='same', use_bias=False)(x)\n",
    "    \n",
    "    if in_filters != out_filters or stride != 1:\n",
    "        shortcut = layers.Conv2D(out_filters, 1, strides=stride, use_bias=False)(shortcut)\n",
    "    \n",
    "    x = layers.Add()([x, shortcut])\n",
    "    return x\n",
    "\n",
    "def build_pyramidnet18(num_classes, alpha=48, input_shape=(224, 224, 3)):\n",
    "    \"\"\"Build PyramidNet-18 from scratch\"\"\"\n",
    "    num_blocks = 8\n",
    "    add_channels_per_block = alpha / num_blocks\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    start_filters = 16\n",
    "    x = layers.Conv2D(start_filters, 7, strides=2, padding='same', use_bias=False)(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "    \n",
    "    current_filters = start_filters\n",
    "    \n",
    "    for stage in range(4):\n",
    "        for block in range(2):\n",
    "            block_idx = stage * 2 + block\n",
    "            out_filters = int(start_filters + add_channels_per_block * (block_idx + 1))\n",
    "            stride = 2 if (block == 0 and stage > 0) else 1\n",
    "            x = pyramidnet_basicblock(x, int(current_filters), out_filters, stride=stride)\n",
    "            current_filters = out_filters\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)  # Dropout regularization\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name='PyramidNet18')\n",
    "    return model\n",
    "\n",
    "print(\"\\nğŸ”§ Building PyramidNet-18 from scratch...\")\n",
    "\n",
    "model = build_pyramidnet18(num_classes=num_classes, alpha=48)\n",
    "\n",
    "print(f\"âœ“ Model built\")\n",
    "print(f\"âœ“ Architecture: PyramidNet-18\")\n",
    "print(f\"âœ“ Parameters: {model.count_params():,}\")\n",
    "print(f\"âœ“ Training: FROM SCRATCH (random initialization)\")\n",
    "\n",
    "# ============================================\n",
    "# COSINE ANNEALING LR SCHEDULE\n",
    "# ============================================\n",
    "\n",
    "def cosine_annealing_schedule(epoch, initial_lr=INITIAL_LEARNING_RATE, epochs=EPOCHS):\n",
    "    \"\"\"Cosine annealing learning rate schedule\"\"\"\n",
    "    lr = initial_lr * 0.5 * (1 + np.cos(np.pi * epoch / epochs))\n",
    "    return lr\n",
    "\n",
    "print(\"\\nâš™ï¸  Compiling model with ALL optimizations...\")\n",
    "\n",
    "# Compile with LABEL SMOOTHING (NEW for from-scratch!)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=INITIAL_LEARNING_RATE),\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=LABEL_SMOOTHING),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Optimizer: Adam\")\n",
    "print(f\"âœ“ Learning Rate: {INITIAL_LEARNING_RATE} (2x from-scratch baseline)\")\n",
    "print(f\"âœ“ LR Schedule: Cosine Annealing (smooth decay)\")\n",
    "print(f\"âœ“ Label Smoothing: {LABEL_SMOOTHING} (NEW!)\")\n",
    "print(f\"âœ“ Dropout: 0.5 (in model)\")\n",
    "print(f\"âœ“ Data Augmentation: Enhanced\")\n",
    "print(f\"âœ“ Max Epochs: {EPOCHS}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL OPTIMIZATIONS APPLIED:\")\n",
    "print(\"=\"*80)\n",
    "print(\"1. âœ… Right-sized architecture (PyramidNet-18, 11M params)\")\n",
    "print(\"2. âœ… Optimal learning rate (0.001)\")\n",
    "print(\"3. âœ… Cosine annealing schedule\")\n",
    "print(\"4. âœ… Enhanced data augmentation\")\n",
    "print(\"5. âœ… Label smoothing (0.1)\")\n",
    "print(\"6. âœ… Dropout (0.5)\")\n",
    "print(\"7. âœ… Batch normalization\")\n",
    "print(\"8. âœ… Early stopping\")\n",
    "print(\"9. âœ… More epochs (100)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ============================================\n",
    "# CALLBACKS\n",
    "# ============================================\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    LearningRateScheduler(cosine_annealing_schedule, verbose=0),\n",
    "    ModelCheckpoint(\n",
    "        ULTIMATE_MODEL_PATH,\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# ============================================\n",
    "# TRAIN\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸš€ STARTING ULTIMATE EXPERIMENT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"\\nThis combines:\")\n",
    "print(f\"  âœ“ Best architecture (PyramidNet-18)\")\n",
    "print(f\"  âœ“ Training from scratch (no pre-training)\")\n",
    "print(f\"  âœ“ ALL optimizations (Exp 2 Enhanced settings)\")\n",
    "print(f\"\\nExpected time: 4-6 minutes\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nâœ“ Training complete in {training_time/60:.1f} minutes\")\n",
    "\n",
    "# ============================================\n",
    "# EVALUATE\n",
    "# ============================================\n",
    "print(\"\\nğŸ“Š Evaluating...\")\n",
    "\n",
    "val_loss, val_accuracy = model.evaluate(val_generator, verbose=0)\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "epochs_trained = len(history.history['accuracy'])\n",
    "train_val_gap = final_train_acc - val_accuracy\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š ULTIMATE EXPERIMENT RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nFinal Metrics:\")\n",
    "print(f\"  Training Accuracy:   {final_train_acc*100:.2f}%\")\n",
    "print(f\"  Validation Accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"  Best Val Accuracy:   {best_val_acc*100:.2f}%\")\n",
    "print(f\"  Validation Loss:     {val_loss:.4f}\")\n",
    "print(f\"  Train-Val Gap:       {train_val_gap*100:.2f}pp\")\n",
    "print(f\"  Epochs Trained:      {epochs_trained}/{EPOCHS}\")\n",
    "print(f\"  Training Time:       {training_time/60:.1f} minutes\")\n",
    "\n",
    "# ============================================\n",
    "# COMPLETE COMPARISON\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“ˆ COMPLETE EXPERIMENTAL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_val = 55.36\n",
    "exp1_val = 58.93\n",
    "exp2_enh_val = 64.29\n",
    "scratch_basic_val = 64.29\n",
    "ultimate_val = val_accuracy * 100\n",
    "\n",
    "improvement_from_baseline = ultimate_val - baseline_val\n",
    "improvement_from_scratch = ultimate_val - scratch_basic_val\n",
    "\n",
    "experiments_summary = pd.DataFrame([\n",
    "    {\n",
    "        'Experiment': 'Baseline',\n",
    "        'Pre-train': 'Fruit-360',\n",
    "        'Optimizations': 'Poor',\n",
    "        'Val Acc (%)': baseline_val\n",
    "    },\n",
    "    {\n",
    "        'Experiment': 'Exp 1',\n",
    "        'Pre-train': 'Fruit-360',\n",
    "        'Optimizations': 'Partial',\n",
    "        'Val Acc (%)': exp1_val\n",
    "    },\n",
    "    {\n",
    "        'Experiment': 'Exp 2 Enhanced',\n",
    "        'Pre-train': 'Fruit-360',\n",
    "        'Optimizations': 'Full',\n",
    "        'Val Acc (%)': exp2_enh_val\n",
    "    },\n",
    "    {\n",
    "        'Experiment': 'From Scratch (Basic)',\n",
    "        'Pre-train': 'None',\n",
    "        'Optimizations': 'Partial',\n",
    "        'Val Acc (%)': scratch_basic_val\n",
    "    },\n",
    "    {\n",
    "        'Experiment': 'ULTIMATE (Scratch + Full)',\n",
    "        'Pre-train': 'None',\n",
    "        'Optimizations': 'Full',\n",
    "        'Val Acc (%)': f'{ultimate_val:.2f}'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\" + experiments_summary.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ¯ HYPOTHESIS EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if ultimate_val >= 70:\n",
    "    print(\"ğŸ‰ OUTSTANDING: Exceeded 70% target!\")\n",
    "    print(f\"   Ultimate optimization achieved {ultimate_val:.2f}%\")\n",
    "    print(f\"   Improvement from scratch-basic: {improvement_from_scratch:+.2f}pp\")\n",
    "    status = \"OUTSTANDING\"\n",
    "elif ultimate_val >= 67:\n",
    "    print(\"âœ… EXCELLENT: Significant improvement!\")\n",
    "    print(f\"   From scratch-basic: 64.29%\")\n",
    "    print(f\"   Ultimate: {ultimate_val:.2f}%\")\n",
    "    print(f\"   Improvement: {improvement_from_scratch:+.2f}pp\")\n",
    "    status = \"EXCELLENT\"\n",
    "elif ultimate_val >= 65:\n",
    "    print(\"âœ“ GOOD: Modest improvement\")\n",
    "    print(f\"   Full optimizations helped: {improvement_from_scratch:+.2f}pp\")\n",
    "    status = \"GOOD\"\n",
    "else:\n",
    "    print(\"âš ï¸  Similar to previous from-scratch\")\n",
    "    print(f\"   Ultimate: {ultimate_val:.2f}% vs Basic: {scratch_basic_val:.2f}%\")\n",
    "    print(f\"   Optimizations had limited impact\")\n",
    "    status = \"SIMILAR\"\n",
    "\n",
    "# ============================================\n",
    "# KEY INSIGHTS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ’¡ KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. OPTIMIZATION IMPACT:\")\n",
    "if ultimate_val > scratch_basic_val + 2:\n",
    "    print(f\"   âœ… Full optimizations made significant difference!\")\n",
    "    print(f\"   âœ… Improvement: {improvement_from_scratch:+.2f}pp\")\n",
    "    print(f\"   âœ… Validates optimization importance\")\n",
    "elif ultimate_val > scratch_basic_val:\n",
    "    print(f\"   âœ“ Some improvement: {improvement_from_scratch:+.2f}pp\")\n",
    "    print(f\"   âœ“ Optimizations help but modest impact\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  Similar result: {ultimate_val:.2f}% vs {scratch_basic_val:.2f}%\")\n",
    "    print(f\"   âš ï¸  Basic settings were already near-optimal\")\n",
    "\n",
    "print(f\"\\n2. PRE-TRAINING COMPARISON:\")\n",
    "print(f\"   Fruit-360 + Full Opt: {exp2_enh_val:.2f}%\")\n",
    "print(f\"   From Scratch + Full Opt: {ultimate_val:.2f}%\")\n",
    "print(f\"   Difference: {ultimate_val - exp2_enh_val:+.2f}pp\")\n",
    "if ultimate_val > exp2_enh_val + 1:\n",
    "    print(f\"   âœ… From scratch is better!\")\n",
    "elif ultimate_val < exp2_enh_val - 1:\n",
    "    print(f\"   âš ï¸  Pre-training was slightly better\")\n",
    "else:\n",
    "    print(f\"   âœ“ Both approaches equivalent with full optimization\")\n",
    "\n",
    "print(f\"\\n3. TRAINING CAPACITY:\")\n",
    "if final_train_acc >= 0.85:\n",
    "    print(f\"   âœ… Excellent: Model fits training data well ({final_train_acc*100:.2f}%)\")\n",
    "elif final_train_acc >= 0.75:\n",
    "    print(f\"   âœ“ Good: Model learning well ({final_train_acc*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  Model struggles with training data ({final_train_acc*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n4. GENERALIZATION:\")\n",
    "if train_val_gap > 0.20:\n",
    "    print(f\"   âš ï¸  High overfitting: {train_val_gap*100:.1f}pp gap\")\n",
    "elif train_val_gap > 0.10:\n",
    "    print(f\"   âœ“ Acceptable gap: {train_val_gap*100:.1f}pp\")\n",
    "else:\n",
    "    print(f\"   âœ… Excellent generalization: {train_val_gap*100:.1f}pp gap\")\n",
    "\n",
    "# ============================================\n",
    "# SAVE RESULTS\n",
    "# ============================================\n",
    "print(\"\\nğŸ’¾ Saving results...\")\n",
    "\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df['learning_rate'] = [cosine_annealing_schedule(e) for e in range(len(history_df))]\n",
    "history_df.to_csv(os.path.join(ULTIMATE_RESULTS_PATH, 'training_history.csv'), index=False)\n",
    "\n",
    "summary = {\n",
    "    'experiment': 'Ultimate: From Scratch + Full Optimization',\n",
    "    'status': status,\n",
    "    'architecture': 'PyramidNet-18',\n",
    "    'pretrain': 'None (from scratch)',\n",
    "    'baseline_val_acc': baseline_val,\n",
    "    'scratch_basic_val_acc': scratch_basic_val,\n",
    "    'ultimate_val_acc': ultimate_val,\n",
    "    'improvement_from_baseline': improvement_from_baseline,\n",
    "    'improvement_from_scratch_basic': improvement_from_scratch,\n",
    "    'learning_rate': INITIAL_LEARNING_RATE,\n",
    "    'lr_schedule': 'Cosine Annealing',\n",
    "    'label_smoothing': LABEL_SMOOTHING,\n",
    "    'augmentation': 'Enhanced',\n",
    "    'max_epochs': EPOCHS,\n",
    "    'epochs_trained': epochs_trained,\n",
    "    'training_time_min': training_time/60,\n",
    "    'final_train_acc': final_train_acc * 100,\n",
    "    'final_val_acc': ultimate_val,\n",
    "    'best_val_acc': best_val_acc * 100,\n",
    "    'train_val_gap': train_val_gap * 100\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df.to_csv(os.path.join(ULTIMATE_RESULTS_PATH, 'experiment_summary.csv'), index=False)\n",
    "\n",
    "experiments_summary.to_csv(os.path.join(ULTIMATE_RESULTS_PATH, 'all_experiments_final.csv'), index=False)\n",
    "\n",
    "# ============================================\n",
    "# VISUALIZATIONS\n",
    "# ============================================\n",
    "print(\"\\nğŸ“Š Creating visualizations...\")\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
    "\n",
    "# 1. Complete progression\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "experiments = ['Baseline', 'Exp 1', 'Exp 2 Enh\\n(Fruit-360)', \n",
    "               'Scratch\\n(Basic)', 'ULTIMATE\\n(Scratch+Full)']\n",
    "accuracies = [baseline_val, exp1_val, exp2_enh_val, scratch_basic_val, ultimate_val]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#2ECC71', '#F39C12', '#9B59B6']\n",
    "\n",
    "bars = ax1.bar(experiments, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax1.axhline(y=64.29, color='orange', linestyle='--', linewidth=2, label='Previous best')\n",
    "ax1.axhline(y=70, color='green', linestyle='--', linewidth=2, label='Target (70%)')\n",
    "ax1.set_ylabel('Validation Accuracy (%)', fontweight='bold', fontsize=13)\n",
    "ax1.set_title('Complete Journey: Testing ALL Optimizations', fontsize=18, fontweight='bold')\n",
    "ax1.set_ylim([0, 100])\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.legend(fontsize=11)\n",
    "\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, height + 2,\n",
    "             f'{acc:.1f}%', ha='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "# 2. Training curves\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.plot(history.history['accuracy'], label='Train', linewidth=2.5, color='#3498DB')\n",
    "ax2.plot(history.history['val_accuracy'], label='Val', linewidth=2.5, color='#E74C3C')\n",
    "ax2.set_title('Ultimate: Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Loss curves\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "ax3.plot(history.history['loss'], label='Train', linewidth=2.5, color='#3498DB')\n",
    "ax3.plot(history.history['val_loss'], label='Val', linewidth=2.5, color='#E74C3C')\n",
    "ax3.set_title('Ultimate: Loss', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Loss')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. LR schedule\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "lrs = [cosine_annealing_schedule(e) for e in range(epochs_trained)]\n",
    "ax4.plot(lrs, linewidth=2.5, color='#9B59B6')\n",
    "ax4.set_title('Cosine Annealing', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Learning Rate')\n",
    "ax4.set_yscale('log')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Optimization impact\n",
    "ax5 = fig.add_subplot(gs[2, 0])\n",
    "opt_types = ['Baseline\\n(Poor)', 'Partial\\n(Exp 1)', 'Full\\n(Exp 2)', \n",
    "             'Partial\\n(Scratch)', 'Full\\n(Ultimate)']\n",
    "opt_accs = [baseline_val, exp1_val, exp2_enh_val, scratch_basic_val, ultimate_val]\n",
    "opt_colors = ['#FF6B6B', '#FFA07A', '#2ECC71', '#F39C12', '#9B59B6']\n",
    "\n",
    "bars = ax5.bar(opt_types, opt_accs, color=opt_colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "ax5.set_ylabel('Validation Accuracy (%)', fontweight='bold')\n",
    "ax5.set_title('Optimization Level Impact', fontsize=14, fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 6. Train-Val gap\n",
    "ax6 = fig.add_subplot(gs[2, 1])\n",
    "train_val_gaps = [t - v for t, v in zip(history.history['accuracy'], history.history['val_accuracy'])]\n",
    "ax6.plot(train_val_gaps, linewidth=2.5, color='#E67E22')\n",
    "ax6.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "ax6.axhline(y=0.15, color='red', linestyle='--', label='High overfit')\n",
    "ax6.set_title('Overfitting Monitor', fontsize=14, fontweight='bold')\n",
    "ax6.set_xlabel('Epoch')\n",
    "ax6.set_ylabel('Train-Val Gap')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Summary table\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "ax7.axis('tight')\n",
    "ax7.axis('off')\n",
    "\n",
    "table_data = [\n",
    "    ['Metric', 'Value'],\n",
    "    ['Final Val Acc', f'{ultimate_val:.2f}%'],\n",
    "    ['Best Val Acc', f'{best_val_acc*100:.2f}%'],\n",
    "    ['Train Acc', f'{final_train_acc*100:.2f}%'],\n",
    "    ['Train-Val Gap', f'{train_val_gap*100:.2f}pp'],\n",
    "    ['vs Baseline', f'{improvement_from_baseline:+.2f}pp'],\n",
    "    ['vs Scratch-Basic', f'{improvement_from_scratch:+.2f}pp'],\n",
    "    ['Epochs', f'{epochs_trained}/{EPOCHS}'],\n",
    "    ['Time', f'{training_time/60:.1f} min']\n",
    "]\n",
    "\n",
    "table = ax7.table(cellText=table_data, cellLoc='left', loc='center',\n",
    "                  colWidths=[0.5, 0.5])\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(11)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "for i in range(2):\n",
    "    table[(0, i)].set_facecolor('#2C3E50')\n",
    "    table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "for i in range(1, len(table_data)):\n",
    "    for j in range(2):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#ECF0F1')\n",
    "\n",
    "ax7.set_title('Ultimate Results', fontsize=14, fontweight='bold', pad=10)\n",
    "\n",
    "plt.savefig(os.path.join(ULTIMATE_RESULTS_PATH, 'ultimate_comprehensive_analysis.png'), \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"âœ“ Visualizations saved\")\n",
    "\n",
    "# ============================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ULTIMATE EXPERIMENT COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nğŸ¯ FINAL RESULTS:\")\n",
    "print(f\"  Status: {status}\")\n",
    "print(f\"  Validation Accuracy: {ultimate_val:.2f}%\")\n",
    "print(f\"  Improvement from Scratch-Basic: {improvement_from_scratch:+.2f}pp\")\n",
    "print(f\"  Training Time: {training_time/60:.1f} minutes\")\n",
    "\n",
    "print(f\"\\nğŸ“ Results saved to: {ULTIMATE_RESULTS_PATH}/\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Best model saved:\")\n",
    "print(f\"  {ULTIMATE_MODEL_PATH}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š FINAL EXPERIMENTAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nAll Experiments Tested:\")\n",
    "print(f\"  1. Baseline (Fruit-360, poor settings): {baseline_val:.2f}%\")\n",
    "print(f\"  2. Exp 1 (Fruit-360, unfroze): {exp1_val:.2f}%\")\n",
    "print(f\"  3. Exp 2 Enhanced (Fruit-360, full opt): {exp2_enh_val:.2f}%\")\n",
    "print(f\"  4. From Scratch (basic settings): {scratch_basic_val:.2f}%\")\n",
    "print(f\"  5. ULTIMATE (scratch + full opt): {ultimate_val:.2f}%\")\n",
    "\n",
    "print(f\"\\nğŸ† BEST MODEL:\")\n",
    "if ultimate_val > scratch_basic_val + 1:\n",
    "    print(f\"  âœ… Ultimate (From Scratch + Full Opt): {ultimate_val:.2f}%\")\n",
    "    print(f\"  âœ… Full optimizations made a difference!\")\n",
    "else:\n",
    "    print(f\"  Both are similar:\")\n",
    "    print(f\"  - From Scratch (Basic): {scratch_basic_val:.2f}%\")\n",
    "    print(f\"  - Ultimate (Full Opt): {ultimate_val:.2f}%\")\n",
    "    print(f\"  Conclusion: 800 images is the limiting factor\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ’¡ FINAL CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if ultimate_val >= 70:\n",
    "    print(\"\\nğŸ‰ SUCCESS: Broke through 70% barrier!\")\n",
    "    print(\"  All optimizations working together achieved target!\")\n",
    "elif ultimate_val >= 67:\n",
    "    print(\"\\nâœ… EXCELLENT: Significant improvement achieved!\")\n",
    "    print(\"  Full optimizations from scratch made a difference!\")\n",
    "elif ultimate_val > scratch_basic_val + 1:\n",
    "    print(\"\\nâœ“ GOOD: Optimizations helped measurably\")\n",
    "    print(\"  Every percentage point counts with limited data!\")\n",
    "else:\n",
    "    print(\"\\nğŸ“Š INSIGHT: 64-65% appears to be the limit for 800 images\")\n",
    "    print(\"  To improve further:\")\n",
    "    print(\"  1. Need more data (1500-2000 images)\")\n",
    "    print(\"  2. Or advanced techniques (ensembles, self-supervised)\")\n",
    "    print(\"  3. Current result is strong for dataset size\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ¨ ALL EXPERIMENTS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nğŸ¯ NEXT STEPS:\")\n",
    "print(\"  1. âœ… Complete experimental validation done\")\n",
    "print(\"  2. â†’ Use best model for multimodal system\")\n",
    "print(\"  3. â†’ Document complete findings\")\n",
    "print(\"  4. â†’ Build final application\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a7b80b-dbe0-4179-8389-ab7aedc6c616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b4438e-83b5-4eee-9e46-37ca46d7bf48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f1cd5-bd2b-4840-916c-ada17a8aee4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
