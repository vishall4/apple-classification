# ğŸ Apple Variety Classification: When Transfer Learning Fails

A systematic computer vision study exploring CNN architectures and transfer learning strategies for image classification with limited data. **Key finding: training from scratch with the right architecture outperforms transfer learning when domain mismatch exists.**

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

<p align="center">
  <img src="results/figures/COMPLETE_EXPERIMENTAL_SUMMARY.png" alt="Experimental Results" width="800"/>
</p>

---

## ğŸ¯ Key Results

| Metric | Value |
|--------|-------|
| **Best Model** | PyramidNet-18 (trained from scratch) |
| **Validation Accuracy** | 64.29% |
| **Dataset Size** | 800 hand-curated real-world images |
| **Experiments Conducted** | 9 |
| **Architectures Tested** | 5 |
| **Inference Time** | 20.4ms per image |

---

## ğŸ’¡ Key Findings

### 1. Transfer Learning Can Hurt Performance
Pre-training on mismatched domains (studio photos â†’ real-world photos) causes **negative transfer**.

| Pre-training Strategy | Accuracy | Verdict |
|----------------------|----------|---------|
| Fruit-360 (studio images) | 64.29% | âš ï¸ Required heavy optimization |
| ImageNet | 51.19% | âŒ Model too large |
| **None (from scratch)** | **64.29%** | âœ… Best approach |

### 2. Architecture Selection > Pre-training Strategy

| Architecture | Parameters | Accuracy | Status |
|-------------|------------|----------|--------|
| **PyramidNet-18** | **11M** | **64.29%** | âœ… Winner |
| EfficientNet-B0 | 5.3M | 19.64% | âŒ Failed |
| ResNet-18 | 11M | 17.26% | âŒ Failed |
| MobileNetV2 | 3.5M | 11.90% | âŒ Failed |
| DenseNet-121 | 8M | 11.90% | âŒ Failed |

### 3. Optimization Drove +16% Improvement

```
Baseline (poor settings):     55.36%
    â†“ Unfroze all layers
Experiment 1:                 58.93% (+3.6pp)
    â†“ 10x learning rate + enhanced augmentation  
Experiment 2 (optimized):     64.29% (+8.9pp)
```

---

## ğŸ”¬ Experimental Journey

### Phase 1: Fruit-360 Pre-training
- **Hypothesis:** Pre-training on fruit images will help
- **Result:** Negative transfer discovered â€” studio â‰  real-world
- **Solution:** Heavy optimization recovered performance to 64.29%

### Phase 2: Architecture Comparison (From Scratch)
- **Tested:** 5 architectures with identical training settings
- **Result:** Only PyramidNet-18 converged
- **Insight:** Gradual channel increase > abrupt jumps (ResNet)

### Phase 3: ImageNet Pre-training
- **Hypothesis:** Standard ImageNet pre-training will help
- **Result:** 51.19% â€” worse than from scratch!
- **Insight:** 25M parameter model too large for 800 images

---

## ğŸ› ï¸ Technical Details

### Final Model Configuration
```python
Architecture:      PyramidNet-18
Parameters:        ~11M
Training:          From scratch (random initialization)
Learning Rate:     0.001 with cosine annealing
Regularization:    Label smoothing (0.1), Dropout
Data Augmentation: Rotation (Â±30Â°), Shift (Â±20%), Shear (Â±15Â°), Zoom (Â±30%)
Inference Time:    20.4ms/image
```

### Dataset
- **Size:** 800 images (hand-curated from real-world sources)
- **Split:** 600 training / 200 validation
- **Characteristics:** Natural lighting, varied backgrounds, multiple orientations
- **Key difference from Fruit-360:** Real-world conditions vs controlled studio

---

## ğŸ“ Project Structure

```
apple-classification/
â”œâ”€â”€ README.md                              # This file
â”œâ”€â”€ requirements.txt                       # Dependencies
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Model_Comparison.ipynb            # Main experiments notebook
â”‚   â””â”€â”€ instructions_and_things_to_keep_in_mind__.ipynb
â”œâ”€â”€ results/
â”‚   â””â”€â”€ figures/
â”‚       â””â”€â”€ COMPLETE_EXPERIMENTAL_SUMMARY.png
â””â”€â”€ docs/
    â”œâ”€â”€ COMPLETE_PROJECT_STORY.md         # Full experimental narrative
    â”œâ”€â”€ EXECUTIVE_SUMMARY.md              # High-level summary
    â””â”€â”€ QUICK_REFERENCE_SUMMARY.md        # Quick reference table
```

---

## ğŸš€ Quick Start

### Installation
```bash
git clone https://github.com/vishall4/apple-classification.git
cd apple-classification
pip install -r requirements.txt
```

### Run Experiments
Open `notebooks/Model_Comparison.ipynb` in Jupyter or Google Colab.

---

## ğŸ“Š Complete Results Table

| # | Experiment | Pre-training | Architecture | Val Acc | Status |
|---|------------|--------------|--------------|---------|--------|
| 1 | Baseline | Fruit-360 | PyramidNet-18 | 55.36% | âŒ Poor |
| 2 | Exp 1 | Fruit-360 | PyramidNet-18 | 58.93% | âš ï¸ Better |
| 3 | Exp 2 Enhanced | Fruit-360 | PyramidNet-18 | 64.29% | âœ“ Good |
| 4 | **From Scratch** | **None** | **PyramidNet-18** | **64.29%** | **âœ… Best** |
| 5 | From Scratch | None | ResNet-18 | 17.26% | âŒ Failed |
| 6 | From Scratch | None | MobileNetV2 | 11.90% | âŒ Failed |
| 7 | From Scratch | None | EfficientNet-B0 | 19.64% | âŒ Failed |
| 8 | From Scratch | None | DenseNet-121 | 11.90% | âŒ Failed |
| 9 | ImageNet | ImageNet | ResNet50 | 51.19% | âŒ Worse |

---

## ğŸ“š Lessons Learned

### When to Use Transfer Learning
| âœ… DO use when | âŒ DON'T use when |
|---------------|------------------|
| Source & target domains match | Domain mismatch (studio â†’ real-world) |
| Large target dataset (1000+ images) | Small dataset (<1000 images) |
| Computational constraints | Model too large for data |

### For Small Dataset Learning
1. âœ… Match model size to data (~11M params for 800 images)
2. âœ… Consider training from scratch
3. âœ… Focus on optimization (LR, augmentation, schedule)
4. âœ… Try PyramidNet over ResNet

---

## ğŸ”® Future Work

- [ ] Collect more data (target: 2000 images â†’ 70-75% expected)
- [ ] Implement MixUp / CutMix augmentation
- [ ] Add Grad-CAM visualization for interpretability
- [ ] Build ensemble of PyramidNet models
- [ ] Deploy as web application with Streamlit

---

## ğŸ“„ License

MIT License - feel free to use this code for your own projects.

---

## ğŸ‘¤ Author

**Vishal Lohiya**  
- GitHub: [@vishall4](https://github.com/vishall4)
- LinkedIn: [vishal-lohiya](https://www.linkedin.com/in/vishal-lohiya-750a3b36a/)

---

*This project demonstrates that standard practices (like transfer learning) don't always apply. Systematic experimentation revealed that training from scratch with the right architecture outperforms pre-training for limited data scenarios.*
